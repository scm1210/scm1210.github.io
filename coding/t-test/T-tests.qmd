---
title: "T-tests"
author: "Steven"
date: "`r Sys.Date()`"
format: 
  html:
    code-folding: hide
    toc: true
    toc-depth: 2
    theme: united
    css: styles.css   # <-- add your custom CSS here
---

```{r setup, include=F, message=FALSE}
# set chunk options for the document
# include=FALSE means that this chunk will not show up in the report

knitr::opts_chunk$set(
	echo = TRUE,
	fig.path = "figs/t-test/",
	message = FALSE,
	warning = FALSE,
	cache = FALSE,
	dpi = 150
)
 
# echo = TRUE means that the source code will be displayed
# message = FALSE suppresses messages
# warning = FALSE suppresses warnings
# cache = FALSE recompiles from scratch each time you knit 
# dpi = 150 sets the figure resolution
# fig.path specifies a directory where figures will be output

options(digits = 4) 
set.seed(123) #set seed for random number generation
```

# Getting everything set up {.tabset}

```{r}
setwd("~/Desktop/Coding-Boot-Camp/Data-viz-basics")
```

## Set Working Directory

**Change to your own working directory (WD) to save things like plots**. You can do that by modifying the file path or go session (on the upper bar) --\> set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it's important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the `getwd` command in the console (usually the lower left hand pane) to get your WD.

## Load Packages

```{r}
if (!require("pacman")) install.packages("pacman") #run this if you don't have pacman 
library(pacman)
pacman::p_load(tidyverse, ggpubr, rstatix, caret, broom, kableExtra, reactable, Hmisc, datarium, car, DT,install = T) 
#use pacman to load packages quickly 
```

One of the great things about R is its ability to be super flexible. This comes from R's ability to use different `packages`. You can load packages into your current `work environment` by using the `library(PACKAGE)` function. It is important to note that in order to `library` a package you must first have it installed. To install a package you can use the `install.packages("PACKAGE")` command. You can learn more about the different types of packages hosted on the Comprehensive R Archive Network (CRAN) [here](https://cran.r-project.org/)! One other important thing is that some packages often have similar commands (e.g., `plyr` and `hmisc` both use `summarize`) that are masked meaning that you will call a function and may not get the function you expect. To get around this you can use `PACKAGE::FUNCTION` to call package-specific function.

For this script, and here forward, We use `pacman` to load in all of our packages rather than using the iterative `if (!require("PACKAGE")) install.packages("PACKAGE")` set-up. There's still some merit to using that if loading in packages in a certain order creates issues (e.g.,`tidyverse` and `brms` in a certain fashion).

## Get our plot aesthetics set-up

This is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!

```{r, include=T}
palette_map = c("#3B9AB2", "#EBCC2A", "#F21A00")
palette_condition = c("#ee9b00", "#bb3e03", "#005f73")

plot_aes = theme_classic() + # 
  theme(legend.position = "top",
        legend.text = element_text(size = 12),
        text = element_text(size = 16, family = "Futura Medium"),
        axis.text = element_text(color = "black"),
        axis.line = element_line(colour = "black"),
        axis.ticks.y = element_blank())
```

## Build Relevant Functions

Using stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.

```{r}
mystats <- function(x, na.omit=FALSE){
  if (na.omit)
    x <- x[!is.na(x)]
  m <- mean(x)
  n <- length(x)
  s <- sd(x)
  skew <- sum((x-m)^3/s^3)/n
  kurt <- sum((x-m)^4/s^4)/n - 3
  return(c(n=n, mean=m, stdev=s, skew=skew, kurtosis=kurt))
}

```

## Load data

Since we are using an existing dataset in R, we don't need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you'll see people use nomenclature similar to `df`, `data`, `dataframe`, etc. to denote a dataframe. If you are working with multiple datasets, it's advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe `atlantic` and the NYT Best-sellers `NYT` for simplicity and so I don't accidentally write over files.

For example, if your WD is already set and the data exists within said directory you can use: `df <- read_csv(MY_CSV.csv)`

If the data is on something like `Github` you can use: `df <- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data`.

If you are working in one directory and need to call something for another directory you can do something like: `Atlantic_FK <- read_csv("~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv")`

There are also other packages/functions that allow you to read in files with different extensions such as `haven::read_sav()` to read in a file from SPSS or `rjson:: fromJSON(file="data.json")`to read in a json file. If you want to learn more about how to reading in different files you can take a peek at [this site](https://www.datafiles.samhsa.gov/get-help/format-specific-issues/how-do-i-read-data-r).

```{r}
# Load the data
data("genderweight", package = "datarium")
genderweight <- as.data.frame(genderweight)

# Show a sample of the data by group
```

# Brief Description

Now that we got everything set up, we are going to get into our analyses. For the first part we are going to be looking at gender differences between males and females in weight using the `genderweight` dataset from the `Datarium` package in R! To this quantify this, we'll use an **Independent T-test**. An Independent T-test is when we have one IV with **only** two levels or categories (e.g., gender: Male and Female) and one DV that is continuous (e.g., weight ranging from 0 to 250). If the IV has more than two levels we need to use another type of test called ANOVA; more on that later :).

Some other things:

-   Two means calculated from two samples Null hypothesis: Means from two samples are similar

-   Alternative hypothesis: Means from two samples are different

-   The bigger the observed difference between sample means = More likely sample means differ (capture this with effect size).

## Statistical Assumptions {.tabset}

Assumption are also important. That is, data need to possess certain qualities for us to be able to use this type of test. For a t-test these are:

-   The DV data are continuous (not ordinal or nominal).

-   The sample data have been randomly sampled from a population.

-   There is homogeneity of variance (i.e., the variability of the data in each group is similar).

-   The distribution is approximately normal.

Click through the tabs to see how to check each assumption.

### Continuous

We can check this by looking at the structure of our data using the `class` function (for one variable) or `str` function (for all the variables in our dataset). We can see that weight is `numeric` and therefore continuous! Therefore, we can move forward with our analyses.

```{r}
class(genderweight$weight)
```

```{r}
str(genderweight)
```

### Randomly Sampled

This is something you do when you design the study--we can't do anything in R to check this.

### Homogeneity of Variance

We need to make sure the variability of the data in each group is similar. We can use something called Levene's Test for equality of error variances to do this. If we violate this assumption (p \<. 05 in our test) we will have to use a Welch's T-test. We violate this assumption so if we were actually doing a meaningful project we would need to use a different statistical test. For the sake of brevity we'll pretend we are ok for now.

```{r}
leveneTest(genderweight$weight ~ genderweight$group)
```

### The distribution is approximately normal.

To check the distribution of the data we can use density plots in the `ggplot` within `tidyverse` to visualize this. It's also important to get some statistics behind this, and to do that we can look at skewness and kurtosis via the `mystats` function that we wrote earlier. You can also use `psych::describe` to get similar information. For skewness and kurtosis, we want values of skewness fall between − 3 and + 3, and kurtosis is appropriate from a range of − 10 to + 10

```{r}
# Basic density
p <- ggplot(genderweight, aes(x=weight)) + 
  geom_density(color="dodgerblue4", fill="dodgerblue3", alpha=0.2) + plot_aes +
  geom_vline(aes(xintercept=mean(weight)),
            color="dodgerblue3", linetype="dashed", size=1) 
annotate_figure(p,
                top = text_grob("Density Plots for both genders",  color = "black", face = "bold", size = 20),
                bottom = text_grob("Verical line represents mean value."
                                   , color = "Black",
                                   hjust = 1.1, x = 1, face = "italic", size = 12))
```

We can also have the densities by gender. Looks like we should have some interesting results!

```{r}
p<-ggplot(genderweight, aes(x=weight, color=group, fill=group, alpha=0.1)) +
  geom_density()+geom_vline(aes(xintercept=mean(weight)),
            color="blue", linetype="dashed", size=1) + plot_aes 

annotate_figure(p,
                top = text_grob("Density Plots for both genders",  color = "black", face = "bold", size = 20),
                bottom = text_grob("Verical line represents mean value."
                                   , color = "Black",
                                   hjust = 1.1, x = 1, face = "italic", size = 12))
```

```{r}
mystats(genderweight$weight)
```

# Summary Statistics {.tabset}

It's also important for us to get some summary statistics for our data (e.g., N's, Means, SDs).

## Sample Size, Means, and Standard Deviations

```{r}
genderweight %>%
  group_by(group) %>%
  get_summary_stats(weight, type = "mean_sd") %>% 
  reactable::reactable(striped = TRUE)
```

## Build a simple graph to visualize

We also visualize our data with a box plot, while overlaying the scatter plots!

```{r}
 # Create a box plot with jittered data points
ggplot(genderweight, aes(x = group, y = weight,color = group)) +
  geom_boxplot() +
  geom_jitter(width = 0.2, size = 2,alpha=0.2) +
  # Add axis labels
  xlab("Groups") +
  ylab("Weight") +
  plot_aes +
  # Add plot title
  ggtitle("Weight by Groups") + theme(plot.title = element_text(hjust = 0.5))
```

# Main Analyses {.tabset}

Now, we'll conduct our independent t-test to see if there are gender differences in weight, as well as getting the proportion of variance accounted for/Magnitude of the effect (Cohen's D).

## Independent T-test

We can see that we have a significant effect of gender on weight. That is, when conducting our independent t-test we observed a large effect \[*t*(26.87) =-20.79, *p* \< .001, d = -6.575\], such that Men (M = 85.826, SD = 4.354) possessed significantly greater body weight compared to their female counterparts (M = 63.499, SD = 2.028).

```{r}
stat.test <- genderweight %>% 
  t_test(weight ~ group) %>%
  add_significance()
stat.test
```

## Cohen's D

From inspecting our output, we can see we have a large effect of gender

```{r}
genderweight %>%  cohens_d(weight ~ group, var.equal = TRUE)
```

## Visualize our results using GG-Plot with stats

```{r}
plot <- ggplot(genderweight, aes(x = group, y = weight, color = group)) +
  geom_boxplot() +
  geom_jitter(width = 0.2, size = 2) +
  # Add axis labels
  xlab("Gender") +
  ylab("Weight") +
  plot_aes +
  # Add plot title
  ggtitle("Weight by Gender") + theme(plot.title = element_text(hjust = 0.5))

stat.test <- stat.test %>% add_xy_position(x = "group")
plot <- plot +  stat_pvalue_manual(stat.test, tip.length = 0) +
  labs(subtitle = get_test_label(stat.test, detailed = TRUE)) 

annotate_figure(plot,
                bottom = text_grob("D = -6.575"
                                   , color = "Black",
                                   hjust = 1.1, x = 1, face = "italic", size = 16))
  

```

# High Level Example

Next we will see what this looks like at the highest level.

This part of the tutorial uses data and reproduces a subset of analyses reported in the following manuscript:

[Mesquiti & Seraj. (Preprint) The Psychological Impacts of the COVID-19 Pandemic on Corporate Leadership](https://psyarxiv.com/kvar9/)

You can find this project's github [here](https://github.com/scm1210/Language_Lab_Repro).

## Abstract

The COVID-19 pandemic sent shockwaves across the fabric of our society. Examining the impact of the pandemic on business leadership is particularly important to understanding how this event affected their decision-making. The present study documents the psychological effects of the COVID-19 pandemic on chief executive officers (CEOs). This was accomplished by analyzing CEOs' language from quarterly earnings calls (N = 19,536) for a year before and after lockdown. CEOs had large shifts in language in the months immediately following the start of the pandemic lockdowns. Analytic thinking plummeted after the world went into lockdown, with CEOs' language becoming less technical and more personal and intuitive. In parallel, CEOs' language showed signs of increased cognitive load, as they were processing the effect of the pandemic on their business practices. Business leaders' use of collective-focused language (we-usage) dropped substantially after the pandemic began, perhaps suggesting CEOs felt disconnected from their companies. Self-focused (I-usage) language increased, showing the increased preoccupation of business leaders. The size of the observed shifts in language during the pandemic also dwarfed responses to other events that occurred dating back to 2010, with the effect lasting around seven months.

# Prep data {.tabset}

## Load necessary packages and set Working Directory

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse,zoo,lubridate,plotrix,ggpubr, caret, broom, kableExtra, reactable, effsize, install = T)
```

## Define aesthetics

```{r}
palette_map = c("#3B9AB2", "#EBCC2A", "#F21A00")
palette_condition = c("#ee9b00", "#bb3e03", "#005f73")

plot_aes = theme_classic() +
  theme(text = element_text(size = 16, family = "Futura Medium")) + 
  theme(axis.text.x=element_text(angle=45, hjust=1)) +
  theme(plot.title.position = 'plot', 
        plot.title = element_text(hjust = 0.5, face = "bold", size = 16)) + 
  theme(axis.text=element_text(size=16),
        axis.title=element_text(size=20,face="bold"))+
  theme(plot.title.position = 'plot', 
        plot.title = element_text(hjust = 0.5, face = "bold", size = 20)) +
  theme(axis.text=element_text(size = 14),
        axis.title=element_text(size = 20,face="bold"))
```

## Write our Table Funcions

```{r}
baseline_ttest <- function(ttest_list) {
  # Extract relevant information from each test and store in a data frame
  ttest_df <- data.frame(
    Group1 = seq(0,0,1),
    Group2 = seq(1,24,1),
    t = sapply(ttest_list, function(x) x$statistic),
    df = sapply(ttest_list, function(x) x$parameter),
    p_value = sapply(ttest_list, function(x) x$p.value)
  )
  
  # Format p-values as scientific notation
  ttest_df$p_value <- format(ttest_df$p_value, scientific = T)
  
  # Rename columns
  colnames(ttest_df) <- c("t", "t + 1 ", "t-value", "Degrees of Freedom", "p-value")
  
  # Create table using kableExtra
  kable(ttest_df, caption = "Summary of Welch's t-Tests", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

post_pandemic_summary <- function(ttest_list) {
  # Extract relevant information from each test and store in a data frame
  ttest_df <- data.frame(
    Group1 = seq(12,23,1),
    Group2 = seq(13,24,1),
    t = sapply(ttest_list, function(x) x$statistic),
    df = sapply(ttest_list, function(x) x$parameter),
    p_value = sapply(ttest_list, function(x) x$p.value)
  )
  
  # Format p-values as scientific notation
  ttest_df$p_value <- format(ttest_df$p_value, scientific = T)
  
  # Rename columns
  colnames(ttest_df) <- c("t", "t + 1 ", "t-value", "Degrees of Freedom", "p-value")
  
  # Create table using kableExtra
  kable(ttest_df, caption = "Summary of Welch's t-Tests", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}



baseline_cohen_d <- function(cohen_d_list) {
  # Extract relevant information from each test and store in a data frame
  cohen_d_df <- data.frame(
    Group1 = seq(0,0,1),
    Group2 = seq(1,24,1),
    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)
  )
  
  # Rename columns
  colnames(cohen_d_df) <- c("t", "t + 1", "Cohen's d")
  
  # Create table using kableExtra
  kable(cohen_d_df, caption = "Summary of Cohen's D", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

post_cohen_d <- function(cohen_d_list) {
  # Extract relevant information from each test and store in a data frame
  cohen_d_df <- data.frame(
    Group1 = seq(12,23,1),
    Group2 = seq(13,24,1),
    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)
  )
  
  # Rename columns
  colnames(cohen_d_df) <- c("t", "t+1", "Cohen's d")
  
  # Create table using kableExtra
  kable(cohen_d_df, caption = "Summary of Cohen's D", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

baseline_mean_diff <- function(mean_diff_list) {
  # Extract relevant information from each mean difference calculation and store in a data frame
  mean_diff_df <- data.frame(
    Group1 = seq(0,0,1),
    Group2 = seq(1,24,1),
    mean_diff = mean_diff_list
  )
  
  # Rename columns
  colnames(mean_diff_df) <- c("t", "t+1", "Mean Difference")
  
  # Create table using kableExtra
  kable(mean_diff_df, caption = "Summary of Mean Differences", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}


post_mean_diff <- function(mean_diff_list) {
  # Extract relevant information from each mean difference calculation and store in a data frame
  mean_diff_df <- data.frame(
    Group1 = seq(12,23,1),
    Group2 = seq(13,24,1),
    mean_diff = mean_diff_list
  )
  
  # Rename columns
  colnames(mean_diff_df) <- c("t", "t+1", "Mean Difference")
  
  # Create table using kableExtra
  kable(mean_diff_df, caption = "Summary of Mean Differences", booktabs = TRUE) %>%
   kableExtra::kable_styling()
}

```

## Load in the Data

```{r}
data  <-  read_csv("https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Big_CEO.csv") #read in the data from github 

data <- data["2019-03-01"<= data$Date & data$Date <= "2021-04-01",] #subsetting covid dates 

data <- data %>% filter(WC<=5400) %>% #filter out based on our exclusion criteria
  filter(WC>=25)

data$month_year <- format(as.Date(data$Date), "%Y-%m") #reformat 

data_tidy <- data %>% dplyr::select(Date, Speaker, Analytic, cogproc,allnone,we,i,emo_anx) %>%
  mutate(Date = lubridate::ymd(Date),
         time_month = as.numeric(Date - ymd("2019-03-01")) / 30, #centering at start of march
         time_month_quad = time_month * time_month) #making our quadratic term

data_tidy$Date_off <- floor(data_tidy$time_month) #rounding off dates to whole months using ceiling function (0 = 2019-03, 24 = 2021-04)
data_tidy$Date_covid <- as.factor(data_tidy$Date_off) #factorize

```

## Create Tidy Data for Graphs

```{r,}
df <- read_csv("https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Big_CEO.csv")#put code here to read in Big CEO data
df <- df %>% filter(WC<=5400)   %>% 
  filter(WC>=25)

df$month_year <- format(as.Date(df$Date), "%Y-%m") ###extracting month and year to build fiscal quarter graphs, need a new variable bc if not it'll give us issues

df2 <- df %>%#converting our dates to quarterly dates 
  group_by(month_year) %>% ###grouping by the Top100 tag and date 
  summarise_at(vars("Date","WC","Analytic","cogproc",'we','i'),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest

df2 <- df2["2019-01"<= df2$month_year & df2$month_year <= "2021-03",] #covid dates 
```

# Write our Stats Functions {.tabset}

We were interested in how language changed relative to baseline one year pre-pandemic, as well as how language changed after the Pandemic.

As a result we ran two separate set of analyses comparing t(time zero) to t\[i\] and t(12 months after our centered data point) to t + 1. The groups you see will be centered on 03/2019. That is, 12 = 03/2020, 13 = 04/2020, etc. etc.

## Analytic Thinking

```{r}
analytic_my.t = function(fac1, fac2){
  t.test(data_tidy$Analytic[data_tidy$Date_covid==fac1], 
         data_tidy$Analytic[data_tidy$Date_covid==fac2])
} #writing our t-test function to compare t to t[i] 

analytic_my.d = function(fac1, fac2){
  cohen.d(data_tidy$Analytic[data_tidy$Date_covid==fac1], 
          data_tidy$Analytic[data_tidy$Date_covid==fac2])
} #function for cohen's d

analytic_mean <-  function(fac1, fac2){
  mean(data_tidy$Analytic[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$Analytic[data_tidy$Date_covid==fac2])
} #function to do mean differences

```

## Cognitive Processing

```{r}
cogproc_my.t = function(fac1, fac2){
  t.test(data_tidy$cogproc[data_tidy$Date_covid==fac1], 
         data_tidy$cogproc[data_tidy$Date_covid==fac2])
} #writing our t-test function to compare t to t[i] 


cogproc_my.d = function(fac1, fac2){
  cohen.d(data_tidy$cogproc[data_tidy$Date_covid==fac1], 
          data_tidy$cogproc[data_tidy$Date_covid==fac2])
} #function for cohen's d

cogproc_mean <-  function(fac1, fac2){
  mean(data_tidy$cogproc[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$cogproc[data_tidy$Date_covid==fac2])
} #function to do mean differences
```

## I-words

```{r}
i_my.t = function(fac1, fac2){
  t.test(data_tidy$i[data_tidy$Date_covid==fac1], 
         data_tidy$i[data_tidy$Date_covid==fac2])
} #writing our t-test function to compare t to t + 1 

i_my.d = function(fac1, fac2){
  cohen.d(data_tidy$i[data_tidy$Date_covid==fac1], 
          data_tidy$i[data_tidy$Date_covid==fac2])
} #function for cohen's d


i_mean <-  function(fac1, fac2){
  mean(data_tidy$i[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$i[data_tidy$Date_covid==fac2])
} #function to do mean differences

```

## We-words

```{r}
we_my.t = function(fac1, fac2){
  t.test(data_tidy$we[data_tidy$Date_covid==fac1], 
         data_tidy$we[data_tidy$Date_covid==fac2])
} 

we_my.d = function(fac1, fac2){
  cohen.d(data_tidy$we[data_tidy$Date_covid==fac1], 
          data_tidy$we[data_tidy$Date_covid==fac2])
} #function for cohen's d

we_mean <-  function(fac1, fac2){
  mean(data_tidy$we[data_tidy$Date_covid==fac1])- 
    mean(data_tidy$we[data_tidy$Date_covid==fac2])
} #function to do mean differences
```

## Tidy data

Data transformations

-   None

Exclusions

-   Excluded texts that were shorter than \*\* 25 words \*\* and greater than \*\* 5,400 words \*\*!

# Summary of the Data {.tabset}

## Range of Dates

```{r}
range(data$Date)
```

## Number of Speakers

```{r}
speakers <- data %>%
  select(Speaker) %>%
  unique() %>%
  dplyr::summarize(n = n()) %>%
  DT::datatable()
speakers
```

## Number of Transcripts

```{r}
transcripts <- data %>%
  select(1) %>%
  dplyr::summarize(n = n()) %>%
  DT::datatable()

transcripts
```

## Mean Word Count

```{r}
word_count <- data %>%
  select(WC) %>%
  dplyr::summarize(mean = mean(WC)) %>%
  DT::datatable()
word_count
```

# How did language change after the Pandemic?

## Analytic Thinking {.tabset}

### T-test

```{r}
analytic_ttest<- mapply(analytic_my.t,seq(12,23,1), seq(13,24,1),SIMPLIFY=F) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1
post_pandemic_summary(analytic_ttest)
```

### Cohen's D

```{r}
analytic_d <- mapply(analytic_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) 
post_cohen_d(analytic_d)
```

### Mean Differences

```{r}
analytic_meandiff <- mapply(analytic_mean, seq(12,23,1), seq(13,24,1)) #across all of the months comparing to time zero
post_mean_diff(analytic_meandiff)
```

## Cogproc {.tabset}

### T-test

```{r}
cogproc_ttest <-mapply(cogproc_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
post_pandemic_summary(cogproc_ttest)
```

### Cohen's D

```{r}
cogproc_d <-mapply(cogproc_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)
post_cohen_d(cogproc_d)
```

### Mean Differences

```{r}
cogproc_meandiff <- mapply(cogproc_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2019]across all of the months
post_mean_diff(cogproc_meandiff)
```

## I-words {.tabset}

### T-test

```{r}
i_ttest <- mapply(i_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first paratheses) to t[i] (second parentheses) increasing by 1
post_pandemic_summary(i_ttest)
```

### Cohen's D

```{r}
i_d <- mapply(i_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)
post_cohen_d(i_d)
```

### Mean Differences

```{r}
i_meandiff <- mapply(i_mean,seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months
post_mean_diff(i_meandiff)
```

## We-words {.tabset}

### T-test

```{r}
we_ttest <- mapply(we_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
post_pandemic_summary(we_ttest)
```

### Cohen's D

```{r}
we_d <- mapply(we_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)
post_cohen_d(we_d)
```

### Mean Differences

```{r}
we_meandiff <- mapply(we_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months
post_mean_diff(we_meandiff)
```

# How did language change relative to baseline (one year before the pandemic; 03/2019)?

## Analytic Thining {.tabset}

### T-test

```{r}
analytic_ttest_baseline <-mapply(analytic_my.t,0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1
baseline_ttest(analytic_ttest_baseline)
```

### Cohen's D

```{r}
analytic_D_baseline <- mapply(analytic_my.d,0, seq(1,24,1),SIMPLIFY=FALSE) 
baseline_cohen_d(analytic_D_baseline)
```

### Mean Differences

```{r}
analytic_mean_baseline <- mapply(analytic_mean, 0, seq(1,24,1)) #across all of the months comparing to time zero
baseline_mean_diff(analytic_mean_baseline)
```

## Cogproc {.tabset}

### T-test

```{r}
cogproc_ttest_baseline <- mapply(cogproc_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
baseline_ttest(cogproc_ttest_baseline)
```

### Cohen's D

```{r}
cogproc_D_baseline <- mapply(cogproc_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)
baseline_cohen_d(cogproc_D_baseline)
```

### Mean Differences

```{r}
cogproc_mean_baseline <- mapply(cogproc_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months
baseline_mean_diff(cogproc_meandiff)
```

## I-words {.tabset}

### T-test

```{r}
i_ttest_baseline <- mapply(i_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first paratheseses) to t[i] (second parentheses) increasing by 1
baseline_ttest(i_ttest_baseline)
```

### Cohen's D

```{r}
i_D_baseline <- mapply(i_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)
baseline_cohen_d(i_D_baseline)
```

### Mean Differences

```{r}
i_mean_baseline <- mapply(i_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months
baseline_mean_diff(i_mean_baseline)
```

## We-words {.tabset}

### T-test

```{r}
we_ttest_baseline <- mapply(we_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1
baseline_ttest(we_ttest_baseline)
```

### Cohen's D

```{r}
we_D_baseline <- mapply(we_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)
baseline_cohen_d(we_D_baseline)
```

### Mean Differences

```{r}
we_mean_baseline <- mapply(we_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months
baseline_mean_diff(we_mean_baseline)
```

# Build our Graphs {.tabset}

## Analytic Thinking

```{r fig.height=6, fig.width=8}

Analytic <- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) +
  ggtitle("Analytic Thinking") +
  labs(x = "Month", y = 'Standardized score') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
Analytic <- Analytic + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=43,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=43,label="Winter 2020 surge", size = 3)
Analytic
```

## Cogproc

```{r fig.height=6, fig.width=8}
Cogproc <- ggplot(data=df2, aes(x=Date_mean, y=cogproc_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=cogproc_mean-cogproc_std.error, ymax=cogproc_mean+cogproc_std.error), alpha=0.2) +
  ggtitle("Cognitive Processing") +
  labs(x = "Month", y = '% Total Words') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
Cogproc <- Cogproc + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=12.5,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=12.5,label="Winter 2020 surge", size = 3)
Cogproc
```

## I-words

```{r fig.height=6, fig.width=8}
i <- ggplot(data=df2, aes(x=Date_mean, y=i_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=i_mean-i_std.error, ymax=i_mean+i_std.error), alpha=0.2) +
  ggtitle("I-usage") +
  labs(x = "Month", y = '% Total Words') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
i <- i + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=1.95,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=1.95,label="Winter 2020 surge", size = 3)
i
```

## We-words

```{r fig.height=6, fig.width=8}
we <- ggplot(data=df2, aes(x=Date_mean, y=we_mean, group=1)) +
  geom_line(colour = "dodgerblue3") +
  scale_x_date(date_breaks = "3 month", date_labels = "%Y-%m") +
  geom_ribbon(aes(ymin=we_mean-we_std.error, ymax=we_mean+we_std.error), alpha=0.2) +
  ggtitle("We-usage") +
  labs(x = "Month", y = '% Total Words') +
  plot_aes + #here's our plot aes object
  geom_vline(xintercept = as.numeric(as.Date("2020-03-01")), linetype = 1) +
  geom_rect(data = df2, #summer surge
            aes(xmin = as.Date("2020-06-15", "%Y-%m-%d"), 
                xmax = as.Date("2020-07-20",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009) +
  geom_rect(data = df2, #winter surge
            aes(xmin = as.Date("2020-11-15", "%Y-%m-%d"), 
                xmax = as.Date("2021-01-01",  "%Y-%m-%d"),
                ymin = -Inf, 
                ymax = Inf),
            fill = "gray", 
            alpha = 0.009)
we <- we + annotate(geom="text",x=as.Date("2020-07-01"),
                                y=6.5,label="Summer 2020 surge", size = 3) + 
  annotate(geom="text",x=as.Date("2020-12-03"),
           y=6.5,label="Winter 2020 surge", size = 3)
we
```

## Tie them all together

```{r,fig.height=14, fig.width=14}
graphs <- ggpubr::ggarrange(Analytic,Cogproc,i,we,ncol=2, nrow=2, common.legend = TRUE, legend = "bottom")
annotate_figure(graphs,
                top = text_grob("CEOs' Language Change",  color = "black", face = "bold", size = 20),
                bottom = text_grob("Note. Vertical Line Represents the onset of the pandemic. \n\ Horizontal shading represents Standard Error. Vertical bars represent virus surges."
                                   , color = "Black",
                                   hjust = 1.1, x = 1, face = "italic", size = 16))
```

# Package Citations

```{r}
report::cite_packages()
```

All credit goes to the great Dani Cosme for teaching me how to make these! You can find her [github](https://github.com/dcosme) here!
