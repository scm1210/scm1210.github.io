[
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html",
    "href": "coding/Data-viz-basics/Data-viz.html",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\n\n\n\nDatasets can either be built-in or can be loaded from external sources in R.\nBuilt-in datasets refer to the datasets already provided within R. For the first part, we will be using a dataset called the air quality dataset, which pertains to the daily air quality measurements in New York from May to September 1973. This dataset consists of more than 100 observations for 6 variables\n\nOzone(mean parts per billion)\nSolar.R(Solar Radiation)\nWind(Average wind speed)\nTemp(maximum daily temperature in Fahrenheit)\nMonth(month of observation)\nDay(Day of the month)\n\n\ndata(airquality)\n\nIn case of an External data source (CSV, Excel, text, HTML file etc.), simply set the folder containing the data as the working directory with the setwd() command. Alternatively, you can set the path to the file if you don‚Äôt want to change your directory, but this is not recommended.\n\nsetwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n\nNow, load the file with the help of the read command. In this case, data is in the form of a CSV file named airquality.csv which can be downloaded from here.\nairquality &lt;- read.csv('airquality.csv',header=TRUE, sep=\",\")\nOne small (but important) thing to note is that you can name objects in R using both &lt;- and = which basically tells the computer ‚Äúsave the csv under this name‚Äù. In our case save ‚Äòairquality.csv‚Äô as airquality\nThe above code reads the file airquality.csv into a data frame airquality. Header=TRUE specifies that the data includes a header and sep=‚Äù,‚Äù specifies that the values in data are separated by commas.\n\n\n\nOnce the data has been loaded into the global environment (workspace), we need to explore it to get an idea about its structure and what we have to work with.\nTo do so, we can use several different functions within R\n\nstr displays the internal structure of an R object and gives a quick overview of the rows and columns of the dataset.\n\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nhead(data,n) and tail(data,n) The head outputs the top n elements in the dataset while the tail method outputs the bottom n.¬†The default value for n in R is 10 but you can obviously specify it to be something else if needed :).\n\n\nhead(airquality) \n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\ntail(airquality) \n\n    Ozone Solar.R Wind Temp Month Day\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n150    NA     145 13.2   77     9  27\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\n\n\nsummary(airquality)The summary method displays descriptive statistics for every variable in the dataset, depending upon the type of the variable. We can see at a glance the mean, median, max and the quartile values of the variables, as well as an missing observations which is especially valuable.\n\n\nsummary(airquality)\n\n     Ozone          Solar.R         Wind            Temp          Month     \n Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0   Min.   :5.00  \n 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0   1st Qu.:6.00  \n Median : 31.5   Median :205   Median : 9.70   Median :79.0   Median :7.00  \n Mean   : 42.1   Mean   :186   Mean   : 9.96   Mean   :77.9   Mean   :6.99  \n 3rd Qu.: 63.2   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0   3rd Qu.:8.00  \n Max.   :168.0   Max.   :334   Max.   :20.70   Max.   :97.0   Max.   :9.00  \n NA's   :37      NA's   :7                                                  \n      Day      \n Min.   : 1.0  \n 1st Qu.: 8.0  \n Median :16.0  \n Mean   :15.8  \n 3rd Qu.:23.0  \n Max.   :31.0"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#set-working-directory",
    "href": "coding/Data-viz-basics/Data-viz.html#set-working-directory",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#loading-datasets",
    "href": "coding/Data-viz-basics/Data-viz.html#loading-datasets",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "Datasets can either be built-in or can be loaded from external sources in R.\nBuilt-in datasets refer to the datasets already provided within R. For the first part, we will be using a dataset called the air quality dataset, which pertains to the daily air quality measurements in New York from May to September 1973. This dataset consists of more than 100 observations for 6 variables\n\nOzone(mean parts per billion)\nSolar.R(Solar Radiation)\nWind(Average wind speed)\nTemp(maximum daily temperature in Fahrenheit)\nMonth(month of observation)\nDay(Day of the month)\n\n\ndata(airquality)\n\nIn case of an External data source (CSV, Excel, text, HTML file etc.), simply set the folder containing the data as the working directory with the setwd() command. Alternatively, you can set the path to the file if you don‚Äôt want to change your directory, but this is not recommended.\n\nsetwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n\nNow, load the file with the help of the read command. In this case, data is in the form of a CSV file named airquality.csv which can be downloaded from here.\nairquality &lt;- read.csv('airquality.csv',header=TRUE, sep=\",\")\nOne small (but important) thing to note is that you can name objects in R using both &lt;- and = which basically tells the computer ‚Äúsave the csv under this name‚Äù. In our case save ‚Äòairquality.csv‚Äô as airquality\nThe above code reads the file airquality.csv into a data frame airquality. Header=TRUE specifies that the data includes a header and sep=‚Äù,‚Äù specifies that the values in data are separated by commas."
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#data-exploration",
    "href": "coding/Data-viz-basics/Data-viz.html#data-exploration",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "Once the data has been loaded into the global environment (workspace), we need to explore it to get an idea about its structure and what we have to work with.\nTo do so, we can use several different functions within R\n\nstr displays the internal structure of an R object and gives a quick overview of the rows and columns of the dataset.\n\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nhead(data,n) and tail(data,n) The head outputs the top n elements in the dataset while the tail method outputs the bottom n.¬†The default value for n in R is 10 but you can obviously specify it to be something else if needed :).\n\n\nhead(airquality) \n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\ntail(airquality) \n\n    Ozone Solar.R Wind Temp Month Day\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n150    NA     145 13.2   77     9  27\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\n\n\nsummary(airquality)The summary method displays descriptive statistics for every variable in the dataset, depending upon the type of the variable. We can see at a glance the mean, median, max and the quartile values of the variables, as well as an missing observations which is especially valuable.\n\n\nsummary(airquality)\n\n     Ozone          Solar.R         Wind            Temp          Month     \n Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0   Min.   :5.00  \n 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0   1st Qu.:6.00  \n Median : 31.5   Median :205   Median : 9.70   Median :79.0   Median :7.00  \n Mean   : 42.1   Mean   :186   Mean   : 9.96   Mean   :77.9   Mean   :6.99  \n 3rd Qu.: 63.2   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0   3rd Qu.:8.00  \n Max.   :168.0   Max.   :334   Max.   :20.70   Max.   :97.0   Max.   :9.00  \n NA's   :37      NA's   :7                                                  \n      Day      \n Min.   : 1.0  \n 1st Qu.: 8.0  \n Median :16.0  \n Mean   :15.8  \n 3rd Qu.:23.0  \n Max.   :31.0"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#the-plot-function",
    "href": "coding/Data-viz-basics/Data-viz.html#the-plot-function",
    "title": "üé® R Data Visualization Adventure",
    "section": "The Plot function",
    "text": "The Plot function\nThe plot() function is a generic function for plotting of R objects. When we run the code below, we get a scatter/dot plot here wherein each dot represents the value of the Ozone in mean parts per billion.\n\nplot(airquality$Ozone)\n\n\n\n\n\n\n\n\nLet‚Äôs now advance this some and plot a graph between the Ozone and Wind values to study the relationship between the two. The plot shows that Wind and Ozone values have a somewhat negative correlation.\n\nplot(airquality$Ozone, airquality$Wind)\n\n\n\n\n\n\n\n\nWhat happens when we use plot command with the entire dataset without selecting any particular columns?\nWe get a matrix of scatterplots which is a correlation matrix of all the columns. The plot above instantly shows that:\n\nThe level of Ozone and Temperature is correlated positively.\nWind speed is negatively correlated to both Temperature and Ozone level.\n\nWe can quickly discover the relationship between variables by merely looking at the plots drawn between them.\n\nplot(airquality)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#using-arguments-with-the-plot-function",
    "href": "coding/Data-viz-basics/Data-viz.html#using-arguments-with-the-plot-function",
    "title": "üé® R Data Visualization Adventure",
    "section": "Using arguments with the plot() function",
    "text": "Using arguments with the plot() function\nWe can easily style our charts by playing with the arguments of the plot() function.\nThe plot function has an argument called type which can take in values like p: points, l: lines, b: both etc. This decides the shape of the output graph.\n\n# points and lines \n plot(airquality$Ozone, type= \"b\")\n\n\n\n\n\n\n\n\n\n# high density vertical lines.\n plot(airquality$Ozone, type= \"h\")\n\n\n\n\n\n\n\n\nLabels and Titles\nWe can also label the X and the Y axis and give a title to our plot. Additionally, we also have the option of giving color to the plot.\n\nplot(airquality$Ozone, xlab = 'ozone Concentration', ylab = 'No of Instances', main = 'Ozone levels in NY city', col = 'green')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#barplot",
    "href": "coding/Data-viz-basics/Data-viz.html#barplot",
    "title": "üé® R Data Visualization Adventure",
    "section": "Barplot",
    "text": "Barplot\nIn a bar plot, data is represented in the form of rectangular bars and the length of the bar is proportional to the value of the variable or column in the dataset. Both horizontal, as well as a vertical bar chart, can be generated by tweaking the horiz parameter.\n\n# Horizontal bar plot\n barplot(airquality$Ozone, main = 'Ozone Concenteration in air',xlab = 'ozone levels', col= 'green',horiz = TRUE)\n\n\n\n\n\n\n\n\nVertical Barplot\n\n# Horizontal bar plot\n barplot(airquality$Ozone, main = 'Ozone Concenteration in air',xlab = 'ozone levels', col= 'green',horiz = F)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#histogram",
    "href": "coding/Data-viz-basics/Data-viz.html#histogram",
    "title": "üé® R Data Visualization Adventure",
    "section": "Histogram",
    "text": "Histogram\nA histogram is quite similar to a bar chart except that it groups values into continuous ranges. A histogram represents the frequencies of values of a variable bucketed into ranges. We get a histogram of the Solar.R values with hist(airquality$Solar.R).\n\nhist(airquality$Solar.R)\n\n\n\n\n\n\n\n\nBy giving an appropriate value for the color argument (e.g., col='red'), we can obtain a colored histogram as well.\n\nhist(airquality$Solar.R, main = 'Solar Radiation values in air',xlab = 'Solar rad.', col='red')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#boxplot",
    "href": "coding/Data-viz-basics/Data-viz.html#boxplot",
    "title": "üé® R Data Visualization Adventure",
    "section": "Boxplot",
    "text": "Boxplot\nWe have seen how the summary() command in R can display the descriptive statistics for every variable in the dataset. Boxplot does the same albeit graphically in the form of quartiles (e.g.,lowest 25% of the data, the middle 50% of the data, and the highest 25% of the data). It is again very straightforward to plot a boxplot in R.\nMaking a single box plot\n\n#Single box plot\nboxplot(airquality$Solar.R)\n\n\n\n\n\n\n\n\nMaking multiple box plots\n\n# Multiple box plots\nboxplot(airquality[,0:4], main='Multiple Box plots')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#grid-of-charts",
    "href": "coding/Data-viz-basics/Data-viz.html#grid-of-charts",
    "title": "üé® R Data Visualization Adventure",
    "section": "Grid of Charts",
    "text": "Grid of Charts\nThere is a very interesting feature in R which enables us to plot multiple charts at once. This comes in very handy during the EDA since the need to plot multiple graphs one by one is eliminated. For drawing a grid, the first argument should specify certain attributes like the margin of the grid(mar), no of rows and columns(mfrow), whether a border is to be included(bty) and position of the labels(las: 1 for horizontal, las: 0 for vertical).\n\npar(mfrow=c(3,3), mar=c(2,5,2,1), las=1, bty=\"n\")\nplot(airquality$Ozone)\nplot(airquality$Ozone, airquality$Wind)\nplot(airquality$Ozone, type= \"c\")\nplot(airquality$Ozone, type= \"s\")\nplot(airquality$Ozone, type= \"h\")\nbarplot(airquality$Ozone, main = 'Ozone Concenteration in air',xlab = 'ozone levels', col='green',horiz = TRUE)\nhist(airquality$Solar.R)\nboxplot(airquality$Solar.R)\nboxplot(airquality[,0:4], main='Multiple Box plots')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#ggplot2",
    "href": "coding/Data-viz-basics/Data-viz.html#ggplot2",
    "title": "üé® R Data Visualization Adventure",
    "section": "Ggplot2",
    "text": "Ggplot2\nThe ggplot2 package is one of the most widely used visualization packages in R. It enables the users to create sophisticated visualizations with little code The popularity of ggplot2 has increased tremendously in recent years since it makes it possible to create graphs that contain both univariate and multivariate data in a very simple manner.\nInstall and Load Package and Data\n\n#Installing & Loading the package \n   \n#install.packages(\"ggplot2\") uncomment this to install\nlibrary(ggplot2)\n   \n#Loading the dataset\nattach(mtcars)\n# create factors with value labels \n\n\nScatterplots\ngeom_point() is used to create scatterplots and geom can have many variations like geom_jitter(), geom_count(), etc. Here, we use it to create a scatterplot for weight and mpg of cars. Notice how we specify what variables we want on our X and Y axes.\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nStyling Scatterplots\nWe can also style our scatterplots. For example, we can introduce an aesthetic that colors the points on the graph by some type of factor (e.g.¬†number of cylinders)\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg, color = as.factor(cyl))) + geom_point()\n\n\n\n\n\n\n\n\nThe color parameter is used to differentiate between different factor level of the cyl variable.\nAdditionally, we can introduce things like size\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg, size = qsec)) + geom_point()\n\n\n\n\n\n\n\n\nIn the above example, the value of qsec indicates the acceleration which decides the size of the points.\nWe can also use different symbols to specify different things, as well.\n\np  &lt;-  ggplot(mtcars,aes(mpg, wt, shape  =  factor(cyl)))\n  p + geom_point(aes(colour  =  factor(cyl)), size  =  4) + geom_point(colour  =  \"grey90\", size  =  1.5)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#load-packages",
    "href": "coding/Data-viz-basics/Data-viz.html#load-packages",
    "title": "üé® R Data Visualization Adventure",
    "section": "Load Packages",
    "text": "Load Packages\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix,plotrix, caret, broom, kableExtra, reactable, Hmisc, datarium, car,install = T) \n#use pacman to load packages quickly \n\nOne of the great things about R is its ability to be super flexible. This comes from R‚Äôs ability to use different packages. You can load packages into your current work environment by using the library(PACKAGE) function. It is important to note that in order to library a package you must first have it installed. To install a package you can use the install.packages(\"PACKAGE\") command. You can learn more about the different types of packages hosted on the Comprehensive R Archive Network (CRAN) here! One other important thing is that some packages often have similar commands (e.g., plyr and hmisc both use summarize) that are masked meaning that you will call a function and may not get the function you expect. To get around this you can use PACKAGE::FUNCTION to call package-specific function.\nFor this part of the script, and here forward, we use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion; I‚Äôve had issues with this in the past -_-)."
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#get-our-plot-aesthetics-set-up",
    "href": "coding/Data-viz-basics/Data-viz.html#get-our-plot-aesthetics-set-up",
    "title": "üé® R Data Visualization Adventure",
    "section": "Get our plot aesthetics set-up",
    "text": "Get our plot aesthetics set-up\nThis is a super quick and easy way to style our plots without introducing a vile amount of code lines to each chunk! Let‚Äôs break down what we are working with:\n\ntheme_classic() let‚Äôs us style our plot with a transparent back drop, rather than the grey, and use some other styling features.\ntheme() allows us to specify other parameters which are discussed below\nlegend.position we can specify where we want our graph‚Äôs legend to be. We can set it to: left, right, bottom, or top\ntext let‚Äôs us style our text. We can specify things like the size, color, adjustment, margins, etc.\naxis.text allows us to style the axis text similar to above\naxis.line permits for the styling of the axis lines (e.g., color, etc.)\naxis.ticks.x or y allows us to style the x and y axes\n\n\nplot_aes = theme_classic() +\n  theme(text = element_text(size = 16, family = \"Futura Medium\")) + \n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)) + \n  theme(axis.text=element_text(size=16),\n        axis.title=element_text(size=20,face=\"bold\"))+\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 20)) +\n  theme(axis.text=element_text(size = 14),\n        axis.title=element_text(size = 20,face=\"bold\"))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#load-up-the-data",
    "href": "coding/Data-viz-basics/Data-viz.html#load-up-the-data",
    "title": "üé® R Data Visualization Adventure",
    "section": "Load up the data",
    "text": "Load up the data\nSince we are using existing datasets in R and on github, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts, you‚Äôll see people use nomenclature similar to: df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over my files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to read in different files you can take a peek at this site.\n\n# Load the data\ndata(\"genderweight\", package = \"datarium\")\ngenderweight &lt;- as.data.frame(genderweight)\n\n# Show a sample of the data by group"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#basic-density",
    "href": "coding/Data-viz-basics/Data-viz.html#basic-density",
    "title": "üé® R Data Visualization Adventure",
    "section": "Basic Density",
    "text": "Basic Density\nTo check the distribution of the data we can use density plots in the ggplot within tidyverse to visualize this. In the first part of the code, we tell ggplot to get our data from the genderweight dataset and use the weight variable. Using geom_denisty we then tell it we want to color our plot dodgerblue (my favorite R palette :) ), as well as fill it with that color using color=\"dodgerblue4\", fill=\"dodgerblue3\". We then add our plot_aes object (usually doesn‚Äôt matter too much where we add this). Next we add a vertical line to our denisty plot using geom_vline, tell it that we want it to be at the mean value of the weight variable (which must be a continuous variable) and then style it color=\"dodgerblue3\", linetype=\"dashed\", size=1, telling R we want a blue, dashed line.\nLastly, we annotate the graph using annotate_figure. We feed in the object we want to annotate first (in this case p) and then tell R what we want for the top and bottom annotations!\n\np &lt;- ggplot(genderweight, aes(x=weight)) + \n  geom_density(color=\"dodgerblue4\", fill=\"dodgerblue3\", alpha=0.2) + plot_aes +\n  geom_vline(aes(xintercept=mean(weight)),\n            color=\"dodgerblue3\", linetype=\"dashed\", size=1) \nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Vertical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#styling-basic-density",
    "href": "coding/Data-viz-basics/Data-viz.html#styling-basic-density",
    "title": "üé® R Data Visualization Adventure",
    "section": "Styling Basic Density",
    "text": "Styling Basic Density\nWe can also have the densities by gender. We do so by adding color=group, fill=group to our code, letting R know to color things by our grouping variable (gender, called group in the dataset).\n\np&lt;-ggplot(genderweight, aes(x=weight, color=group, fill=group, alpha=0.1)) +\n  geom_density()+geom_vline(aes(xintercept=mean(weight)),\n            color=\"blue\", linetype=\"dashed\", size=1) + plot_aes \n\nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Verical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#overlay-two-different-geom-wrappers",
    "href": "coding/Data-viz-basics/Data-viz.html#overlay-two-different-geom-wrappers",
    "title": "üé® R Data Visualization Adventure",
    "section": "Overlay two different geom wrappers",
    "text": "Overlay two different geom wrappers\nWith ggplot2, we can also overlay different types of geom wrappers.\nFor example, we can overlay scatter plots (using geom_jitter) and boxplots (using geom_boxplot). Notice how we introduce grouping variables, plot title, and axis labels\n\n # Create a box plot with jittered data points\nggplot(genderweight, aes(x = group, y = weight,color = group)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, size = 2,alpha=0.2) +\n  # Add axis labels\n  xlab(\"Groups\") +\n  ylab(\"Weight\") +\n  plot_aes +\n  # Add plot title\n  ggtitle(\"Weight by Groups\") + theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#load-in-the-data",
    "href": "coding/Data-viz-basics/Data-viz.html#load-in-the-data",
    "title": "üé® R Data Visualization Adventure",
    "section": "Load in the Data",
    "text": "Load in the Data\nFirst, we load in the raw data from github. Second, we subset the data for a range of dates we are interested in (March 2019-March 2021). We then filter out word counts that are too noisy. Since LIWC is a bag of words program, we want to make sure our observations aren‚Äôt too noisy or large. For example, in the sentence ‚ÄúI love you‚Äù pronouns take up about 66% of the words the sentence and doing any type of analyses with that would not be very informative. We can get more into LIWC at a later time\nOne we have formatted our data appropriately, we can move to tidy it up. That is, we get the 4 variables of interest and use dplyr to group them by the month of their observation and then summarize them by getting the means and standard errors for each variable within each month\n\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Big_CEO.csv\") #read in the data from github \ndf &lt;- df %&gt;% filter(WC&lt;=5400)   %&gt;% \n  filter(WC&gt;=25)\n\ndf$month_year &lt;- format(as.Date(df$Date), \"%Y-%m\") ###extracting month and year to build fiscal quarter graphs, need a new variable bc if not it'll give us issues\n\ndf2 &lt;- df %&gt;%#converting our dates to quarterly dates \n  group_by(month_year) %&gt;% ###grouping by the Top100 tag and date \n  summarise_at(vars(\"Date\",\"WC\",\"Analytic\",\"cogproc\",'we','i'),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n\ndf2 &lt;- df2[\"2019-01\"&lt;= df2$month_year & df2$month_year &lt;= \"2021-03\",] #covid dates \n\nLet‚Äôs take a look at our data structure so you get an idea of what we‚Äôre working with using head().\n\nhead(df2)\n\n# A tibble: 6 √ó 13\n  month_year Date_mean  WC_mean Analytic_mean cogproc_mean we_mean i_mean\n  &lt;chr&gt;      &lt;date&gt;       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 2019-01    2019-01-25   1774.          39.2         11.5    5.76   1.77\n2 2019-02    2019-02-15   1711.          40.8         11.4    5.69   1.66\n3 2019-03    2019-03-11   1427.          40.8         11.3    5.71   1.60\n4 2019-04    2019-04-23   1691.          39.3         11.4    5.67   1.77\n5 2019-05    2019-05-08   1527.          40.3         11.4    5.58   1.64\n6 2019-06    2019-06-13   1763.          40.8         11.4    5.85   1.65\n# ‚Ñπ 6 more variables: Date_std.error &lt;dbl&gt;, WC_std.error &lt;dbl&gt;,\n#   Analytic_std.error &lt;dbl&gt;, cogproc_std.error &lt;dbl&gt;, we_std.error &lt;dbl&gt;,\n#   i_std.error &lt;dbl&gt;"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#build-our-graphs",
    "href": "coding/Data-viz-basics/Data-viz.html#build-our-graphs",
    "title": "üé® R Data Visualization Adventure",
    "section": "Build our Graphs",
    "text": "Build our Graphs\nNow, we‚Äôll run our code. Here‚Äôs a general explanation of what each line does using our first example:\nAnalytic &lt;- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) + creates a ggplot object and names it Analytic. We have Date_mean as our X axis and Analytic_mean as our y variable\ngeom_line(colour = \"dodgerblue3\") + writes our line graph using the parameters we specified above\nscale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") + Tells the graph we want 3 month partitions on our X-axis using the format \"%Y-%m\"\ngeom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) + Graphs the standard error around our linegraph\nggtitle(\"Analytic Thinking\") + titles our plot\nlabs(x = \"Month\", y = 'Standardized score') + adds our x and y axis labels\nplot_aes + here‚Äôs our plot aes object\ngeom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) + specifying we want a vertical line at this specific date\ngeom_rect(data = df2, #summer surge give us a rectangle using the data\naes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), let the lower bound on the x plane be this date\nxmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"), let the upper bound on the x plane be this date\nymin = -Inf, let the lower bound on the y plane be this value\nymax = Inf), let the upper bound on the x plane be this value\nfill = \"gray\", color it grey\nalpha = 0.009) + let it be pretty transparent\ngeom_rect(data = df2, #winter surge give us a rectangle using the data\naes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), let the lower bound on the x plane be this date\nxmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"), let the upper bound on the x plane be this date\nymin = -Inf, let the lower bound on the y plane be this value\nymax = Inf), let the upper bound on the x plane be this value\nfill = \"gray\", color it grey\nalpha = 0.009)let it be pretty transparent\nAnalytic &lt;- Analytic + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"), save a new object called Analytic and write text at this specific date on the x plane\ny=43,label=\"Summer 2020 surge\", size = 3) + specifying what our label is, what the y plane value is and font size\nannotate(geom=\"text\",x=as.Date(\"2020-12-03\"), his specific date on the x plane\ny=43,label=\"Winter 2020 surge\", size = 3) specifying what our label is, what the y plane value is and font size\nWe can use the {r fig.height=6, fig.width=8} in the block of code to specify the dimensions of our figures. This is super helpful when building stuff for presentations, project, etc. I forgot to mention earlier that in the set-up chunk we used fig.path = \"figs/data-viz/\" to specify where our figures to output to within our working directroy. So, if you go to wherever your directorty is you should be able to find them in that file path :).\n\nAnalytic Thinking\n\nAnalytic &lt;- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) +\n  ggtitle(\"Analytic Thinking\") +\n  labs(x = \"Month\", y = 'Standardized score') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nAnalytic &lt;- Analytic + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=43,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=43,label=\"Winter 2020 surge\", size = 3)\nAnalytic\n\n\n\n\n\n\n\n\n\n\nCogproc\n\nCogproc &lt;- ggplot(data=df2, aes(x=Date_mean, y=cogproc_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=cogproc_mean-cogproc_std.error, ymax=cogproc_mean+cogproc_std.error), alpha=0.2) +\n  ggtitle(\"Cognitive Processing\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nCogproc &lt;- Cogproc + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=12.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=12.5,label=\"Winter 2020 surge\", size = 3)\nCogproc\n\n\n\n\n\n\n\n\n\n\nI-words\n\ni &lt;- ggplot(data=df2, aes(x=Date_mean, y=i_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=i_mean-i_std.error, ymax=i_mean+i_std.error), alpha=0.2) +\n  ggtitle(\"I-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\ni &lt;- i + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=1.95,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=1.95,label=\"Winter 2020 surge\", size = 3)\ni\n\n\n\n\n\n\n\n\n\n\nWe-words\n\nwe &lt;- ggplot(data=df2, aes(x=Date_mean, y=we_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=we_mean-we_std.error, ymax=we_mean+we_std.error), alpha=0.2) +\n  ggtitle(\"We-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nwe &lt;- we + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=6.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=6.5,label=\"Winter 2020 surge\", size = 3)\nwe\n\n\n\n\n\n\n\n\n\n\nTie them all together\n\ngraphs &lt;- ggpubr::ggarrange(Analytic,Cogproc,i,we,ncol=2, nrow=2, common.legend = TRUE, legend = \"bottom\")\nannotate_figure(graphs,\n                top = text_grob(\"CEOs' Language Change\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Note. Vertical Line Represents the onset of the pandemic. \\n\\ Horizontal shading represents Standard Error. Vertical bars represent virus surges.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 16))"
  },
  {
    "objectID": "coding/t-test/T-tests.html",
    "href": "coding/t-test/T-tests.html",
    "title": "T-tests",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n\n\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD.\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, caret, broom, kableExtra, reactable, Hmisc, datarium, car, DT,install = T) \n#use pacman to load packages quickly \n\nOne of the great things about R is its ability to be super flexible. This comes from R‚Äôs ability to use different packages. You can load packages into your current work environment by using the library(PACKAGE) function. It is important to note that in order to library a package you must first have it installed. To install a package you can use the install.packages(\"PACKAGE\") command. You can learn more about the different types of packages hosted on the Comprehensive R Archive Network (CRAN) here! One other important thing is that some packages often have similar commands (e.g., plyr and hmisc both use summarize) that are masked meaning that you will call a function and may not get the function you expect. To get around this you can use PACKAGE::FUNCTION to call package-specific function.\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion).\n\n\n\nThis is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())\n\n\n\n\nUsing stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\nmystats &lt;- function(x, na.omit=FALSE){\n  if (na.omit)\n    x &lt;- x[!is.na(x)]\n  m &lt;- mean(x)\n  n &lt;- length(x)\n  s &lt;- sd(x)\n  skew &lt;- sum((x-m)^3/s^3)/n\n  kurt &lt;- sum((x-m)^4/s^4)/n - 3\n  return(c(n=n, mean=m, stdev=s, skew=skew, kurtosis=kurt))\n}\n\n\n\n\nSince we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\n\n# Load the data\ndata(\"genderweight\", package = \"datarium\")\ngenderweight &lt;- as.data.frame(genderweight)\n\n# Show a sample of the data by group"
  },
  {
    "objectID": "coding/t-test/T-tests.html#set-working-directory",
    "href": "coding/t-test/T-tests.html#set-working-directory",
    "title": "T-tests",
    "section": "",
    "text": "Change to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD."
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-packages",
    "href": "coding/t-test/T-tests.html#load-packages",
    "title": "T-tests",
    "section": "",
    "text": "if (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, caret, broom, kableExtra, reactable, Hmisc, datarium, car, DT,install = T) \n#use pacman to load packages quickly \n\nOne of the great things about R is its ability to be super flexible. This comes from R‚Äôs ability to use different packages. You can load packages into your current work environment by using the library(PACKAGE) function. It is important to note that in order to library a package you must first have it installed. To install a package you can use the install.packages(\"PACKAGE\") command. You can learn more about the different types of packages hosted on the Comprehensive R Archive Network (CRAN) here! One other important thing is that some packages often have similar commands (e.g., plyr and hmisc both use summarize) that are masked meaning that you will call a function and may not get the function you expect. To get around this you can use PACKAGE::FUNCTION to call package-specific function.\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion)."
  },
  {
    "objectID": "coding/t-test/T-tests.html#get-our-plot-aesthetics-set-up",
    "href": "coding/t-test/T-tests.html#get-our-plot-aesthetics-set-up",
    "title": "T-tests",
    "section": "",
    "text": "This is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "coding/t-test/T-tests.html#build-relevant-functions",
    "href": "coding/t-test/T-tests.html#build-relevant-functions",
    "title": "T-tests",
    "section": "",
    "text": "Using stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\nmystats &lt;- function(x, na.omit=FALSE){\n  if (na.omit)\n    x &lt;- x[!is.na(x)]\n  m &lt;- mean(x)\n  n &lt;- length(x)\n  s &lt;- sd(x)\n  skew &lt;- sum((x-m)^3/s^3)/n\n  kurt &lt;- sum((x-m)^4/s^4)/n - 3\n  return(c(n=n, mean=m, stdev=s, skew=skew, kurtosis=kurt))\n}"
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-data",
    "href": "coding/t-test/T-tests.html#load-data",
    "title": "T-tests",
    "section": "",
    "text": "Since we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\n\n# Load the data\ndata(\"genderweight\", package = \"datarium\")\ngenderweight &lt;- as.data.frame(genderweight)\n\n# Show a sample of the data by group"
  },
  {
    "objectID": "coding/t-test/T-tests.html#statistical-assumptions",
    "href": "coding/t-test/T-tests.html#statistical-assumptions",
    "title": "T-tests",
    "section": "Statistical Assumptions",
    "text": "Statistical Assumptions\nAssumption are also important. That is, data need to possess certain qualities for us to be able to use this type of test. For a t-test these are:\n\nThe DV data are continuous (not ordinal or nominal).\nThe sample data have been randomly sampled from a population.\nThere is homogeneity of variance (i.e., the variability of the data in each group is similar).\nThe distribution is approximately normal.\n\nClick through the tabs to see how to check each assumption.\n\nContinuous\nWe can check this by looking at the structure of our data using the class function (for one variable) or str function (for all the variables in our dataset). We can see that weight is numeric and therefore continuous! Therefore, we can move forward with our analyses.\n\nclass(genderweight$weight)\n\n[1] \"numeric\"\n\n\n\nstr(genderweight)\n\n'data.frame':   40 obs. of  3 variables:\n $ id    : Factor w/ 40 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ group : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ weight: num  61.6 64.6 66.2 59.3 64.9 ...\n\n\n\n\nRandomly Sampled\nThis is something you do when you design the study‚Äìwe can‚Äôt do anything in R to check this.\n\n\nHomogeneity of Variance\nWe need to make sure the variability of the data in each group is similar. We can use something called Levene‚Äôs Test for equality of error variances to do this. If we violate this assumption (p &lt;. 05 in our test) we will have to use a Welch‚Äôs T-test. We violate this assumption so if we were actually doing a meaningful project we would need to use a different statistical test. For the sake of brevity we‚Äôll pretend we are ok for now.\n\nleveneTest(genderweight$weight ~ genderweight$group)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)  \ngroup  1    6.12  0.018 *\n      38                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nThe distribution is approximately normal.\nTo check the distribution of the data we can use density plots in the ggplot within tidyverse to visualize this. It‚Äôs also important to get some statistics behind this, and to do that we can look at skewness and kurtosis via the mystats function that we wrote earlier. You can also use psych::describe to get similar information. For skewness and kurtosis, we want values of skewness fall between ‚àí 3 and + 3, and kurtosis is appropriate from a range of ‚àí 10 to + 10\n\n# Basic density\np &lt;- ggplot(genderweight, aes(x=weight)) + \n  geom_density(color=\"dodgerblue4\", fill=\"dodgerblue3\", alpha=0.2) + plot_aes +\n  geom_vline(aes(xintercept=mean(weight)),\n            color=\"dodgerblue3\", linetype=\"dashed\", size=1) \nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Verical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))\n\n\n\n\n\n\n\n\nWe can also have the densities by gender. Looks like we should have some interesting results!\n\np&lt;-ggplot(genderweight, aes(x=weight, color=group, fill=group, alpha=0.1)) +\n  geom_density()+geom_vline(aes(xintercept=mean(weight)),\n            color=\"blue\", linetype=\"dashed\", size=1) + plot_aes \n\nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Verical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))\n\n\n\n\n\n\n\n\n\nmystats(genderweight$weight)\n\n       n     mean    stdev     skew kurtosis \n 40.0000  74.6624  11.7924   0.1486  -1.7419"
  },
  {
    "objectID": "coding/t-test/T-tests.html#sample-size-means-and-standard-deviations",
    "href": "coding/t-test/T-tests.html#sample-size-means-and-standard-deviations",
    "title": "T-tests",
    "section": "Sample Size, Means, and Standard Deviations",
    "text": "Sample Size, Means, and Standard Deviations\n\ngenderweight %&gt;%\n  group_by(group) %&gt;%\n  get_summary_stats(weight, type = \"mean_sd\") %&gt;% \n  reactable::reactable(striped = TRUE)"
  },
  {
    "objectID": "coding/t-test/T-tests.html#build-a-simple-graph-to-visualize",
    "href": "coding/t-test/T-tests.html#build-a-simple-graph-to-visualize",
    "title": "T-tests",
    "section": "Build a simple graph to visualize",
    "text": "Build a simple graph to visualize\nWe also visualize our data with a box plot, while overlaying the scatter plots!\n\n # Create a box plot with jittered data points\nggplot(genderweight, aes(x = group, y = weight,color = group)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, size = 2,alpha=0.2) +\n  # Add axis labels\n  xlab(\"Groups\") +\n  ylab(\"Weight\") +\n  plot_aes +\n  # Add plot title\n  ggtitle(\"Weight by Groups\") + theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "coding/t-test/T-tests.html#independent-t-test",
    "href": "coding/t-test/T-tests.html#independent-t-test",
    "title": "T-tests",
    "section": "Independent T-test",
    "text": "Independent T-test\nWe can see that we have a significant effect of gender on weight. That is, when conducting our independent t-test we observed a large effect [t(26.87) =-20.79, p &lt; .001, d = -6.575], such that Men (M = 85.826, SD = 4.354) possessed significantly greater body weight compared to their female counterparts (M = 63.499, SD = 2.028).\n\nstat.test &lt;- genderweight %&gt;% \n  t_test(weight ~ group) %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 √ó 9\n  .y.    group1 group2    n1    n2 statistic    df        p p.signif\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 weight F      M         20    20     -20.8  26.9 4.30e-18 ****"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cohens-d",
    "href": "coding/t-test/T-tests.html#cohens-d",
    "title": "T-tests",
    "section": "Cohen‚Äôs D",
    "text": "Cohen‚Äôs D\nFrom inspecting our output, we can see we have a large effect of gender\n\ngenderweight %&gt;%  cohens_d(weight ~ group, var.equal = TRUE)\n\n# A tibble: 1 √ó 7\n  .y.    group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 weight F      M        -6.57    20    20 large"
  },
  {
    "objectID": "coding/t-test/T-tests.html#visualize-our-results-using-gg-plot-with-stats",
    "href": "coding/t-test/T-tests.html#visualize-our-results-using-gg-plot-with-stats",
    "title": "T-tests",
    "section": "Visualize our results using GG-Plot with stats",
    "text": "Visualize our results using GG-Plot with stats\n\nplot &lt;- ggplot(genderweight, aes(x = group, y = weight, color = group)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, size = 2) +\n  # Add axis labels\n  xlab(\"Gender\") +\n  ylab(\"Weight\") +\n  plot_aes +\n  # Add plot title\n  ggtitle(\"Weight by Gender\") + theme(plot.title = element_text(hjust = 0.5))\n\nstat.test &lt;- stat.test %&gt;% add_xy_position(x = \"group\")\nplot &lt;- plot +  stat_pvalue_manual(stat.test, tip.length = 0) +\n  labs(subtitle = get_test_label(stat.test, detailed = TRUE)) \n\nannotate_figure(plot,\n                bottom = text_grob(\"D = -6.575\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 16))"
  },
  {
    "objectID": "coding/t-test/T-tests.html#abstract",
    "href": "coding/t-test/T-tests.html#abstract",
    "title": "T-tests",
    "section": "Abstract",
    "text": "Abstract\nThe COVID-19 pandemic sent shockwaves across the fabric of our society. Examining the impact of the pandemic on business leadership is particularly important to understanding how this event affected their decision-making. The present study documents the psychological effects of the COVID-19 pandemic on chief executive officers (CEOs). This was accomplished by analyzing CEOs‚Äô language from quarterly earnings calls (N = 19,536) for a year before and after lockdown. CEOs had large shifts in language in the months immediately following the start of the pandemic lockdowns. Analytic thinking plummeted after the world went into lockdown, with CEOs‚Äô language becoming less technical and more personal and intuitive. In parallel, CEOs‚Äô language showed signs of increased cognitive load, as they were processing the effect of the pandemic on their business practices. Business leaders‚Äô use of collective-focused language (we-usage) dropped substantially after the pandemic began, perhaps suggesting CEOs felt disconnected from their companies. Self-focused (I-usage) language increased, showing the increased preoccupation of business leaders. The size of the observed shifts in language during the pandemic also dwarfed responses to other events that occurred dating back to 2010, with the effect lasting around seven months."
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-necessary-packages-and-set-working-directory",
    "href": "coding/t-test/T-tests.html#load-necessary-packages-and-set-working-directory",
    "title": "T-tests",
    "section": "Load necessary packages and set Working Directory",
    "text": "Load necessary packages and set Working Directory\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,zoo,lubridate,plotrix,ggpubr, caret, broom, kableExtra, reactable, effsize, install = T)"
  },
  {
    "objectID": "coding/t-test/T-tests.html#define-aesthetics",
    "href": "coding/t-test/T-tests.html#define-aesthetics",
    "title": "T-tests",
    "section": "Define aesthetics",
    "text": "Define aesthetics\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() +\n  theme(text = element_text(size = 16, family = \"Futura Medium\")) + \n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)) + \n  theme(axis.text=element_text(size=16),\n        axis.title=element_text(size=20,face=\"bold\"))+\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 20)) +\n  theme(axis.text=element_text(size = 14),\n        axis.title=element_text(size = 20,face=\"bold\"))"
  },
  {
    "objectID": "coding/t-test/T-tests.html#write-our-table-funcions",
    "href": "coding/t-test/T-tests.html#write-our-table-funcions",
    "title": "T-tests",
    "section": "Write our Table Funcions",
    "text": "Write our Table Funcions\n\nbaseline_ttest &lt;- function(ttest_list) {\n  # Extract relevant information from each test and store in a data frame\n  ttest_df &lt;- data.frame(\n    Group1 = seq(0,0,1),\n    Group2 = seq(1,24,1),\n    t = sapply(ttest_list, function(x) x$statistic),\n    df = sapply(ttest_list, function(x) x$parameter),\n    p_value = sapply(ttest_list, function(x) x$p.value)\n  )\n  \n  # Format p-values as scientific notation\n  ttest_df$p_value &lt;- format(ttest_df$p_value, scientific = T)\n  \n  # Rename columns\n  colnames(ttest_df) &lt;- c(\"t\", \"t + 1 \", \"t-value\", \"Degrees of Freedom\", \"p-value\")\n  \n  # Create table using kableExtra\n  kable(ttest_df, caption = \"Summary of Welch's t-Tests\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\npost_pandemic_summary &lt;- function(ttest_list) {\n  # Extract relevant information from each test and store in a data frame\n  ttest_df &lt;- data.frame(\n    Group1 = seq(12,23,1),\n    Group2 = seq(13,24,1),\n    t = sapply(ttest_list, function(x) x$statistic),\n    df = sapply(ttest_list, function(x) x$parameter),\n    p_value = sapply(ttest_list, function(x) x$p.value)\n  )\n  \n  # Format p-values as scientific notation\n  ttest_df$p_value &lt;- format(ttest_df$p_value, scientific = T)\n  \n  # Rename columns\n  colnames(ttest_df) &lt;- c(\"t\", \"t + 1 \", \"t-value\", \"Degrees of Freedom\", \"p-value\")\n  \n  # Create table using kableExtra\n  kable(ttest_df, caption = \"Summary of Welch's t-Tests\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\n\n\nbaseline_cohen_d &lt;- function(cohen_d_list) {\n  # Extract relevant information from each test and store in a data frame\n  cohen_d_df &lt;- data.frame(\n    Group1 = seq(0,0,1),\n    Group2 = seq(1,24,1),\n    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)\n  )\n  \n  # Rename columns\n  colnames(cohen_d_df) &lt;- c(\"t\", \"t + 1\", \"Cohen's d\")\n  \n  # Create table using kableExtra\n  kable(cohen_d_df, caption = \"Summary of Cohen's D\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\npost_cohen_d &lt;- function(cohen_d_list) {\n  # Extract relevant information from each test and store in a data frame\n  cohen_d_df &lt;- data.frame(\n    Group1 = seq(12,23,1),\n    Group2 = seq(13,24,1),\n    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)\n  )\n  \n  # Rename columns\n  colnames(cohen_d_df) &lt;- c(\"t\", \"t+1\", \"Cohen's d\")\n  \n  # Create table using kableExtra\n  kable(cohen_d_df, caption = \"Summary of Cohen's D\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\nbaseline_mean_diff &lt;- function(mean_diff_list) {\n  # Extract relevant information from each mean difference calculation and store in a data frame\n  mean_diff_df &lt;- data.frame(\n    Group1 = seq(0,0,1),\n    Group2 = seq(1,24,1),\n    mean_diff = mean_diff_list\n  )\n  \n  # Rename columns\n  colnames(mean_diff_df) &lt;- c(\"t\", \"t+1\", \"Mean Difference\")\n  \n  # Create table using kableExtra\n  kable(mean_diff_df, caption = \"Summary of Mean Differences\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\n\npost_mean_diff &lt;- function(mean_diff_list) {\n  # Extract relevant information from each mean difference calculation and store in a data frame\n  mean_diff_df &lt;- data.frame(\n    Group1 = seq(12,23,1),\n    Group2 = seq(13,24,1),\n    mean_diff = mean_diff_list\n  )\n  \n  # Rename columns\n  colnames(mean_diff_df) &lt;- c(\"t\", \"t+1\", \"Mean Difference\")\n  \n  # Create table using kableExtra\n  kable(mean_diff_df, caption = \"Summary of Mean Differences\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}"
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-in-the-data",
    "href": "coding/t-test/T-tests.html#load-in-the-data",
    "title": "T-tests",
    "section": "Load in the Data",
    "text": "Load in the Data\n\ndata  &lt;-  read_csv(\"https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Big_CEO.csv\") #read in the data from github \n\ndata &lt;- data[\"2019-03-01\"&lt;= data$Date & data$Date &lt;= \"2021-04-01\",] #subsetting covid dates \n\ndata &lt;- data %&gt;% filter(WC&lt;=5400) %&gt;% #filter out based on our exclusion criteria\n  filter(WC&gt;=25)\n\ndata$month_year &lt;- format(as.Date(data$Date), \"%Y-%m\") #reformat \n\ndata_tidy &lt;- data %&gt;% dplyr::select(Date, Speaker, Analytic, cogproc,allnone,we,i,emo_anx) %&gt;%\n  mutate(Date = lubridate::ymd(Date),\n         time_month = as.numeric(Date - ymd(\"2019-03-01\")) / 30, #centering at start of march\n         time_month_quad = time_month * time_month) #making our quadratic term\n\ndata_tidy$Date_off &lt;- floor(data_tidy$time_month) #rounding off dates to whole months using ceiling function (0 = 2019-03, 24 = 2021-04)\ndata_tidy$Date_covid &lt;- as.factor(data_tidy$Date_off) #factorize"
  },
  {
    "objectID": "coding/t-test/T-tests.html#create-tidy-data-for-graphs",
    "href": "coding/t-test/T-tests.html#create-tidy-data-for-graphs",
    "title": "T-tests",
    "section": "Create Tidy Data for Graphs",
    "text": "Create Tidy Data for Graphs\n\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Big_CEO.csv\")#put code here to read in Big CEO data\ndf &lt;- df %&gt;% filter(WC&lt;=5400)   %&gt;% \n  filter(WC&gt;=25)\n\ndf$month_year &lt;- format(as.Date(df$Date), \"%Y-%m\") ###extracting month and year to build fiscal quarter graphs, need a new variable bc if not it'll give us issues\n\ndf2 &lt;- df %&gt;%#converting our dates to quarterly dates \n  group_by(month_year) %&gt;% ###grouping by the Top100 tag and date \n  summarise_at(vars(\"Date\",\"WC\",\"Analytic\",\"cogproc\",'we','i'),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n\ndf2 &lt;- df2[\"2019-01\"&lt;= df2$month_year & df2$month_year &lt;= \"2021-03\",] #covid dates"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thinking",
    "href": "coding/t-test/T-tests.html#analytic-thinking",
    "title": "T-tests",
    "section": "Analytic Thinking",
    "text": "Analytic Thinking\n\nanalytic_my.t = function(fac1, fac2){\n  t.test(data_tidy$Analytic[data_tidy$Date_covid==fac1], \n         data_tidy$Analytic[data_tidy$Date_covid==fac2])\n} #writing our t-test function to compare t to t[i] \n\nanalytic_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$Analytic[data_tidy$Date_covid==fac1], \n          data_tidy$Analytic[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\nanalytic_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$Analytic[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$Analytic[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cognitive-processing",
    "href": "coding/t-test/T-tests.html#cognitive-processing",
    "title": "T-tests",
    "section": "Cognitive Processing",
    "text": "Cognitive Processing\n\ncogproc_my.t = function(fac1, fac2){\n  t.test(data_tidy$cogproc[data_tidy$Date_covid==fac1], \n         data_tidy$cogproc[data_tidy$Date_covid==fac2])\n} #writing our t-test function to compare t to t[i] \n\n\ncogproc_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$cogproc[data_tidy$Date_covid==fac1], \n          data_tidy$cogproc[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\ncogproc_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$cogproc[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$cogproc[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words",
    "href": "coding/t-test/T-tests.html#i-words",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\ni_my.t = function(fac1, fac2){\n  t.test(data_tidy$i[data_tidy$Date_covid==fac1], \n         data_tidy$i[data_tidy$Date_covid==fac2])\n} #writing our t-test function to compare t to t + 1 \n\ni_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$i[data_tidy$Date_covid==fac1], \n          data_tidy$i[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\n\ni_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$i[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$i[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words",
    "href": "coding/t-test/T-tests.html#we-words",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nwe_my.t = function(fac1, fac2){\n  t.test(data_tidy$we[data_tidy$Date_covid==fac1], \n         data_tidy$we[data_tidy$Date_covid==fac2])\n} \n\nwe_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$we[data_tidy$Date_covid==fac1], \n          data_tidy$we[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\nwe_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$we[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$we[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#tidy-data",
    "href": "coding/t-test/T-tests.html#tidy-data",
    "title": "T-tests",
    "section": "Tidy data",
    "text": "Tidy data\nData transformations\n\nNone\n\nExclusions\n\nExcluded texts that were shorter than ** 25 words ** and greater than ** 5,400 words **!"
  },
  {
    "objectID": "coding/t-test/T-tests.html#range-of-dates",
    "href": "coding/t-test/T-tests.html#range-of-dates",
    "title": "T-tests",
    "section": "Range of Dates",
    "text": "Range of Dates\n\nrange(data$Date)\n\n[1] \"2019-03-01\" \"2021-04-01\""
  },
  {
    "objectID": "coding/t-test/T-tests.html#number-of-speakers",
    "href": "coding/t-test/T-tests.html#number-of-speakers",
    "title": "T-tests",
    "section": "Number of Speakers",
    "text": "Number of Speakers\n\nspeakers &lt;- data %&gt;%\n  select(Speaker) %&gt;%\n  unique() %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  DT::datatable()\nspeakers"
  },
  {
    "objectID": "coding/t-test/T-tests.html#number-of-transcripts",
    "href": "coding/t-test/T-tests.html#number-of-transcripts",
    "title": "T-tests",
    "section": "Number of Transcripts",
    "text": "Number of Transcripts\n\ntranscripts &lt;- data %&gt;%\n  select(1) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  DT::datatable()\n\ntranscripts"
  },
  {
    "objectID": "coding/t-test/T-tests.html#mean-word-count",
    "href": "coding/t-test/T-tests.html#mean-word-count",
    "title": "T-tests",
    "section": "Mean Word Count",
    "text": "Mean Word Count\n\nword_count &lt;- data %&gt;%\n  select(WC) %&gt;%\n  dplyr::summarize(mean = mean(WC)) %&gt;%\n  DT::datatable()\nword_count"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thinking-1",
    "href": "coding/t-test/T-tests.html#analytic-thinking-1",
    "title": "T-tests",
    "section": "Analytic Thinking",
    "text": "Analytic Thinking\n\nT-test\n\nanalytic_ttest&lt;- mapply(analytic_my.t,seq(12,23,1), seq(13,24,1),SIMPLIFY=F) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1\npost_pandemic_summary(analytic_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n5.0849\n525.79\n5.124e-07\n\n\n13\n14\n-2.5948\n373.06\n9.839e-03\n\n\n14\n15\n-1.6726\n252.04\n9.565e-02\n\n\n15\n16\n1.9242\n377.62\n5.508e-02\n\n\n16\n17\n-2.2122\n200.57\n2.808e-02\n\n\n17\n18\n-1.6872\n218.93\n9.298e-02\n\n\n18\n19\n0.6199\n262.61\n5.358e-01\n\n\n19\n20\n0.8738\n128.22\n3.839e-01\n\n\n20\n21\n-1.5398\n230.76\n1.250e-01\n\n\n21\n22\n1.9533\n94.32\n5.374e-02\n\n\n22\n23\n-1.1498\n55.55\n2.552e-01\n\n\n23\n24\n-1.7179\n2141.37\n8.596e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nanalytic_d &lt;- mapply(analytic_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) \npost_cohen_d(analytic_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n0.3275\n\n\n13\n14\n-0.1598\n\n\n14\n15\n-0.1320\n\n\n15\n16\n0.1936\n\n\n16\n17\n-0.1617\n\n\n17\n18\n-0.1481\n\n\n18\n19\n0.0710\n\n\n19\n20\n0.0899\n\n\n20\n21\n-0.1246\n\n\n21\n22\n0.2682\n\n\n22\n23\n-0.1598\n\n\n23\n24\n-0.0739\n\n\n\n\n\n\n\n\n\nMean Differences\n\nanalytic_meandiff &lt;- mapply(analytic_mean, seq(12,23,1), seq(13,24,1)) #across all of the months comparing to time zero\npost_mean_diff(analytic_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n4.7346\n\n\n13\n14\n-2.1905\n\n\n14\n15\n-1.8443\n\n\n15\n16\n2.7483\n\n\n16\n17\n-2.2318\n\n\n17\n18\n-2.1013\n\n\n18\n19\n1.1589\n\n\n19\n20\n1.2765\n\n\n20\n21\n-1.7791\n\n\n21\n22\n4.0651\n\n\n22\n23\n-2.0756\n\n\n23\n24\n-0.9941"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cogproc",
    "href": "coding/t-test/T-tests.html#cogproc",
    "title": "T-tests",
    "section": "Cogproc",
    "text": "Cogproc\n\nT-test\n\ncogproc_ttest &lt;-mapply(cogproc_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\npost_pandemic_summary(cogproc_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n-4.3161\n534.57\n1.893e-05\n\n\n13\n14\n1.4046\n366.54\n1.610e-01\n\n\n14\n15\n4.0193\n257.87\n7.665e-05\n\n\n15\n16\n-3.1317\n367.30\n1.877e-03\n\n\n16\n17\n0.9868\n199.24\n3.249e-01\n\n\n17\n18\n4.1804\n223.61\n4.178e-05\n\n\n18\n19\n-1.1984\n285.88\n2.318e-01\n\n\n19\n20\n-1.4930\n133.62\n1.378e-01\n\n\n20\n21\n3.2109\n234.85\n1.508e-03\n\n\n21\n22\n-1.7045\n87.35\n9.183e-02\n\n\n22\n23\n0.9968\n55.38\n3.232e-01\n\n\n23\n24\n-0.9994\n2145.13\n3.177e-01\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ncogproc_d &lt;-mapply(cogproc_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)\npost_cohen_d(cogproc_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n-0.2755\n\n\n13\n14\n0.0887\n\n\n14\n15\n0.3007\n\n\n15\n16\n-0.3205\n\n\n16\n17\n0.0733\n\n\n17\n18\n0.3436\n\n\n18\n19\n-0.1329\n\n\n19\n20\n-0.1294\n\n\n20\n21\n0.2477\n\n\n21\n22\n-0.2453\n\n\n22\n23\n0.1405\n\n\n23\n24\n-0.0430\n\n\n\n\n\n\n\n\n\nMean Differences\n\ncogproc_meandiff &lt;- mapply(cogproc_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2019]across all of the months\npost_mean_diff(cogproc_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n-0.6107\n\n\n13\n14\n0.1785\n\n\n14\n15\n0.6095\n\n\n15\n16\n-0.6540\n\n\n16\n17\n0.1560\n\n\n17\n18\n0.7442\n\n\n18\n19\n-0.2962\n\n\n19\n20\n-0.2746\n\n\n20\n21\n0.5305\n\n\n21\n22\n-0.5358\n\n\n22\n23\n0.2776\n\n\n23\n24\n-0.0887"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words-1",
    "href": "coding/t-test/T-tests.html#i-words-1",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\nT-test\n\ni_ttest &lt;- mapply(i_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first paratheses) to t[i] (second parentheses) increasing by 1\npost_pandemic_summary(i_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n-5.1026\n477.85\n4.842e-07\n\n\n13\n14\n2.9683\n362.97\n3.194e-03\n\n\n14\n15\n2.7352\n261.20\n6.661e-03\n\n\n15\n16\n-3.5895\n336.98\n3.805e-04\n\n\n16\n17\n1.7614\n191.52\n7.976e-02\n\n\n17\n18\n3.4394\n240.73\n6.870e-04\n\n\n18\n19\n-2.6019\n255.11\n9.813e-03\n\n\n19\n20\n0.4503\n134.91\n6.532e-01\n\n\n20\n21\n1.5059\n248.77\n1.334e-01\n\n\n21\n22\n2.0159\n84.28\n4.700e-02\n\n\n22\n23\n-3.8068\n57.56\n3.437e-04\n\n\n23\n24\n4.4095\n2135.84\n1.088e-05\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ni_d &lt;- mapply(i_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)\npost_cohen_d(i_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n-0.3468\n\n\n13\n14\n0.1902\n\n\n14\n15\n0.1991\n\n\n15\n16\n-0.3758\n\n\n16\n17\n0.1452\n\n\n17\n18\n0.2370\n\n\n18\n19\n-0.3007\n\n\n19\n20\n0.0378\n\n\n20\n21\n0.1020\n\n\n21\n22\n0.2972\n\n\n22\n23\n-0.4622\n\n\n23\n24\n0.1900\n\n\n\n\n\n\n\n\n\nMean Differences\n\ni_meandiff &lt;- mapply(i_mean,seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months\npost_mean_diff(i_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n-0.2878\n\n\n13\n14\n0.1551\n\n\n14\n15\n0.1625\n\n\n15\n16\n-0.3242\n\n\n16\n17\n0.1289\n\n\n17\n18\n0.2083\n\n\n18\n19\n-0.2364\n\n\n19\n20\n0.0329\n\n\n20\n21\n0.0886\n\n\n21\n22\n0.2293\n\n\n22\n23\n-0.3912\n\n\n23\n24\n0.1657"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words-1",
    "href": "coding/t-test/T-tests.html#we-words-1",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nT-test\n\nwe_ttest &lt;- mapply(we_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\npost_pandemic_summary(we_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n4.1038\n527.08\n4.709e-05\n\n\n13\n14\n0.9117\n378.82\n3.625e-01\n\n\n14\n15\n-3.3226\n253.14\n1.023e-03\n\n\n15\n16\n2.4647\n373.96\n1.416e-02\n\n\n16\n17\n-0.3375\n197.52\n7.361e-01\n\n\n17\n18\n-4.2759\n229.50\n2.794e-05\n\n\n18\n19\n2.5510\n262.60\n1.131e-02\n\n\n19\n20\n-0.1422\n131.79\n8.871e-01\n\n\n20\n21\n-1.9395\n238.21\n5.362e-02\n\n\n21\n22\n-0.2952\n84.06\n7.685e-01\n\n\n22\n23\n0.8557\n55.76\n3.958e-01\n\n\n23\n24\n-0.3495\n2137.77\n7.267e-01\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nwe_d &lt;- mapply(we_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)\npost_cohen_d(we_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n0.2639\n\n\n13\n14\n0.0550\n\n\n14\n15\n-0.2595\n\n\n15\n16\n0.2501\n\n\n16\n17\n-0.0256\n\n\n17\n18\n-0.3276\n\n\n18\n19\n0.2920\n\n\n19\n20\n-0.0130\n\n\n20\n21\n-0.1444\n\n\n21\n22\n-0.0436\n\n\n22\n23\n0.1170\n\n\n23\n24\n-0.0151\n\n\n\n\n\n\n\n\n\nMean Differences\n\nwe_meandiff &lt;- mapply(we_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months\npost_mean_diff(we_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n0.3778\n\n\n13\n14\n0.0763\n\n\n14\n15\n-0.3676\n\n\n15\n16\n0.3649\n\n\n16\n17\n-0.0365\n\n\n17\n18\n-0.4711\n\n\n18\n19\n0.4169\n\n\n19\n20\n-0.0183\n\n\n20\n21\n-0.2042\n\n\n21\n22\n-0.0609\n\n\n22\n23\n0.1583\n\n\n23\n24\n-0.0210"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thining",
    "href": "coding/t-test/T-tests.html#analytic-thining",
    "title": "T-tests",
    "section": "Analytic Thining",
    "text": "Analytic Thining\n\nT-test\n\nanalytic_ttest_baseline &lt;-mapply(analytic_my.t,0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1\nbaseline_ttest(analytic_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n1.5025\n1161.5\n1.332e-01\n\n\n0\n2\n0.6860\n1036.8\n4.929e-01\n\n\n0\n3\n0.2508\n245.1\n8.022e-01\n\n\n0\n4\n2.6728\n1120.1\n7.631e-03\n\n\n0\n5\n0.4785\n1004.8\n6.324e-01\n\n\n0\n6\n1.0343\n280.4\n3.019e-01\n\n\n0\n7\n2.6675\n1049.9\n7.760e-03\n\n\n0\n8\n1.4046\n993.4\n1.605e-01\n\n\n0\n9\n1.0147\n328.1\n3.110e-01\n\n\n0\n10\n1.5505\n286.2\n1.221e-01\n\n\n0\n11\n1.9738\n1061.6\n4.867e-02\n\n\n0\n12\n1.3054\n1272.1\n1.920e-01\n\n\n0\n13\n5.7770\n623.9\n1.201e-08\n\n\n0\n14\n5.1516\n929.5\n3.153e-07\n\n\n0\n15\n1.4219\n370.2\n1.559e-01\n\n\n0\n16\n3.9258\n316.9\n1.061e-04\n\n\n0\n17\n3.2572\n918.1\n1.166e-03\n\n\n0\n18\n0.1171\n302.2\n9.068e-01\n\n\n0\n19\n0.8463\n164.4\n3.986e-01\n\n\n0\n20\n3.7364\n920.4\n1.981e-04\n\n\n0\n21\n0.6393\n331.8\n5.231e-01\n\n\n0\n22\n2.6168\n63.2\n1.109e-02\n\n\n0\n23\n3.7687\n1112.0\n1.727e-04\n\n\n0\n24\n2.4326\n1125.2\n1.515e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nanalytic_D_baseline &lt;- mapply(analytic_my.d,0, seq(1,24,1),SIMPLIFY=FALSE) \nbaseline_cohen_d(analytic_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n0.0880\n\n\n0\n2\n0.0330\n\n\n0\n3\n0.0206\n\n\n0\n4\n0.1587\n\n\n0\n5\n0.0235\n\n\n0\n6\n0.0867\n\n\n0\n7\n0.1621\n\n\n0\n8\n0.0687\n\n\n0\n9\n0.0806\n\n\n0\n10\n0.1283\n\n\n0\n11\n0.1024\n\n\n0\n12\n0.0694\n\n\n0\n13\n0.3954\n\n\n0\n14\n0.2534\n\n\n0\n15\n0.1138\n\n\n0\n16\n0.3057\n\n\n0\n17\n0.1588\n\n\n0\n18\n0.0102\n\n\n0\n19\n0.0861\n\n\n0\n20\n0.1803\n\n\n0\n21\n0.0530\n\n\n0\n22\n0.3237\n\n\n0\n23\n0.2019\n\n\n0\n24\n0.1263\n\n\n\n\n\n\n\n\n\nMean Differences\n\nanalytic_mean_baseline &lt;- mapply(analytic_mean, 0, seq(1,24,1)) #across all of the months comparing to time zero\nbaseline_mean_diff(analytic_mean_baseline)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n1.3114\n\n\n0\n2\n0.4935\n\n\n0\n3\n0.3040\n\n\n0\n4\n2.3251\n\n\n0\n5\n0.3412\n\n\n0\n6\n1.3028\n\n\n0\n7\n2.3954\n\n\n0\n8\n0.9976\n\n\n0\n9\n1.1987\n\n\n0\n10\n1.9189\n\n\n0\n11\n1.4369\n\n\n0\n12\n1.0438\n\n\n0\n13\n5.7785\n\n\n0\n14\n3.5880\n\n\n0\n15\n1.7437\n\n\n0\n16\n4.4920\n\n\n0\n17\n2.2602\n\n\n0\n18\n0.1590\n\n\n0\n19\n1.3178\n\n\n0\n20\n2.5943\n\n\n0\n21\n0.8152\n\n\n0\n22\n4.8803\n\n\n0\n23\n2.8046\n\n\n0\n24\n1.8106"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cogproc-1",
    "href": "coding/t-test/T-tests.html#cogproc-1",
    "title": "T-tests",
    "section": "Cogproc",
    "text": "Cogproc\n\nT-test\n\ncogproc_ttest_baseline &lt;- mapply(cogproc_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\nbaseline_ttest(cogproc_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n-0.5097\n1156.51\n6.103e-01\n\n\n0\n2\n-0.7179\n1035.97\n4.730e-01\n\n\n0\n3\n-0.2391\n218.72\n8.112e-01\n\n\n0\n4\n-1.8417\n1119.70\n6.579e-02\n\n\n0\n5\n-0.3764\n1051.94\n7.067e-01\n\n\n0\n6\n0.2442\n282.79\n8.072e-01\n\n\n0\n7\n-1.7142\n1029.21\n8.680e-02\n\n\n0\n8\n-0.9538\n1076.64\n3.404e-01\n\n\n0\n9\n1.0446\n320.31\n2.970e-01\n\n\n0\n10\n-0.8169\n255.26\n4.148e-01\n\n\n0\n11\n-0.7245\n1147.57\n4.689e-01\n\n\n0\n12\n-2.0280\n1307.90\n4.276e-02\n\n\n0\n13\n-5.7012\n609.25\n1.855e-08\n\n\n0\n14\n-6.5911\n924.04\n7.329e-11\n\n\n0\n15\n-0.3856\n395.99\n7.000e-01\n\n\n0\n16\n-4.0812\n298.22\n5.758e-05\n\n\n0\n17\n-5.4650\n949.00\n5.916e-08\n\n\n0\n18\n0.9265\n310.67\n3.549e-01\n\n\n0\n19\n-0.5797\n184.74\n5.628e-01\n\n\n0\n20\n-3.7994\n936.81\n1.544e-04\n\n\n0\n21\n0.7639\n341.61\n4.455e-01\n\n\n0\n22\n-1.3820\n61.97\n1.719e-01\n\n\n0\n23\n-1.0691\n1140.02\n2.853e-01\n\n\n0\n24\n-1.8593\n1172.33\n6.323e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ncogproc_D_baseline &lt;- mapply(cogproc_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)\nbaseline_cohen_d(cogproc_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n-0.0299\n\n\n0\n2\n-0.0345\n\n\n0\n3\n-0.0213\n\n\n0\n4\n-0.1094\n\n\n0\n5\n-0.0180\n\n\n0\n6\n0.0204\n\n\n0\n7\n-0.1048\n\n\n0\n8\n-0.0446\n\n\n0\n9\n0.0841\n\n\n0\n10\n-0.0732\n\n\n0\n11\n-0.0364\n\n\n0\n12\n-0.1070\n\n\n0\n13\n-0.3939\n\n\n0\n14\n-0.3256\n\n\n0\n15\n-0.0298\n\n\n0\n16\n-0.3292\n\n\n0\n17\n-0.2601\n\n\n0\n18\n0.0789\n\n\n0\n19\n-0.0527\n\n\n0\n20\n-0.1809\n\n\n0\n21\n0.0622\n\n\n0\n22\n-0.1778\n\n\n0\n23\n-0.0568\n\n\n0\n24\n-0.0951\n\n\n\n\n\n\n\n\n\nMean Differences\n\ncogproc_mean_baseline &lt;- mapply(cogproc_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months\nbaseline_mean_diff(cogproc_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n-0.6107\n\n\n0\n2\n0.1785\n\n\n0\n3\n0.6095\n\n\n0\n4\n-0.6540\n\n\n0\n5\n0.1560\n\n\n0\n6\n0.7442\n\n\n0\n7\n-0.2962\n\n\n0\n8\n-0.2746\n\n\n0\n9\n0.5305\n\n\n0\n10\n-0.5358\n\n\n0\n11\n0.2776\n\n\n0\n12\n-0.0887\n\n\n0\n13\n-0.6107\n\n\n0\n14\n0.1785\n\n\n0\n15\n0.6095\n\n\n0\n16\n-0.6540\n\n\n0\n17\n0.1560\n\n\n0\n18\n0.7442\n\n\n0\n19\n-0.2962\n\n\n0\n20\n-0.2746\n\n\n0\n21\n0.5305\n\n\n0\n22\n-0.5358\n\n\n0\n23\n0.2776\n\n\n0\n24\n-0.0887"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words-2",
    "href": "coding/t-test/T-tests.html#i-words-2",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\nT-test\n\ni_ttest_baseline &lt;- mapply(i_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first paratheseses) to t[i] (second parentheses) increasing by 1\nbaseline_ttest(i_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n-3.3450\n1143.82\n8.495e-04\n\n\n0\n2\n-1.1963\n1155.18\n2.318e-01\n\n\n0\n3\n-0.1911\n213.55\n8.486e-01\n\n\n0\n4\n-4.1439\n1114.31\n3.672e-05\n\n\n0\n5\n-0.6477\n1056.56\n5.173e-01\n\n\n0\n6\n-1.6111\n278.03\n1.083e-01\n\n\n0\n7\n-3.3533\n1035.23\n8.274e-04\n\n\n0\n8\n-2.0582\n1066.96\n3.981e-02\n\n\n0\n9\n-1.4168\n265.19\n1.577e-01\n\n\n0\n10\n-2.7747\n284.30\n5.891e-03\n\n\n0\n11\n-1.9849\n1154.30\n4.739e-02\n\n\n0\n12\n-0.3320\n1263.50\n7.399e-01\n\n\n0\n13\n-5.0280\n571.49\n6.644e-07\n\n\n0\n14\n-3.7093\n958.88\n2.198e-04\n\n\n0\n15\n0.2214\n390.58\n8.249e-01\n\n\n0\n16\n-3.9255\n253.44\n1.116e-04\n\n\n0\n17\n-4.4733\n1005.42\n8.580e-06\n\n\n0\n18\n0.4135\n350.62\n6.795e-01\n\n\n0\n19\n-2.6460\n180.60\n8.864e-03\n\n\n0\n20\n-4.3779\n986.11\n1.326e-05\n\n\n0\n21\n-1.3222\n371.13\n1.869e-01\n\n\n0\n22\n1.3508\n63.34\n1.816e-01\n\n\n0\n23\n-5.6223\n1250.84\n2.322e-08\n\n\n0\n24\n-1.8931\n1254.80\n5.858e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ni_D_baseline &lt;- mapply(i_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)\nbaseline_cohen_d(i_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n-0.1966\n\n\n0\n2\n-0.0544\n\n\n0\n3\n-0.0174\n\n\n0\n4\n-0.2467\n\n\n0\n5\n-0.0310\n\n\n0\n6\n-0.1358\n\n\n0\n7\n-0.2047\n\n\n0\n8\n-0.0967\n\n\n0\n9\n-0.1296\n\n\n0\n10\n-0.2305\n\n\n0\n11\n-0.0996\n\n\n0\n12\n-0.0177\n\n\n0\n13\n-0.3562\n\n\n0\n14\n-0.1786\n\n\n0\n15\n0.0172\n\n\n0\n16\n-0.3537\n\n\n0\n17\n-0.2047\n\n\n0\n18\n0.0327\n\n\n0\n19\n-0.2453\n\n\n0\n20\n-0.2011\n\n\n0\n21\n-0.1028\n\n\n0\n22\n0.1664\n\n\n0\n23\n-0.2904\n\n\n0\n24\n-0.0945\n\n\n\n\n\n\n\n\n\nMean Differences\n\ni_mean_baseline &lt;- mapply(i_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months\nbaseline_mean_diff(i_mean_baseline)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n-0.1748\n\n\n0\n2\n-0.0504\n\n\n0\n3\n-0.0149\n\n\n0\n4\n-0.2082\n\n\n0\n5\n-0.0266\n\n\n0\n6\n-0.1159\n\n\n0\n7\n-0.1744\n\n\n0\n8\n-0.0846\n\n\n0\n9\n-0.1162\n\n\n0\n10\n-0.1958\n\n\n0\n11\n-0.0843\n\n\n0\n12\n-0.0150\n\n\n0\n13\n-0.3028\n\n\n0\n14\n-0.1477\n\n\n0\n15\n0.0147\n\n\n0\n16\n-0.3094\n\n\n0\n17\n-0.1805\n\n\n0\n18\n0.0278\n\n\n0\n19\n-0.2086\n\n\n0\n20\n-0.1757\n\n\n0\n21\n-0.0871\n\n\n0\n22\n0.1422\n\n\n0\n23\n-0.2490\n\n\n0\n24\n-0.0833"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words-2",
    "href": "coding/t-test/T-tests.html#we-words-2",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nT-test\n\nwe_ttest_baseline &lt;- mapply(we_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\nbaseline_ttest(we_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n0.5718\n1161.88\n5.676e-01\n\n\n0\n2\n1.5919\n1008.45\n1.117e-01\n\n\n0\n3\n-1.0685\n214.75\n2.865e-01\n\n\n0\n4\n0.6154\n1116.23\n5.384e-01\n\n\n0\n5\n0.9396\n979.10\n3.476e-01\n\n\n0\n6\n-1.1796\n280.32\n2.392e-01\n\n\n0\n7\n-0.2036\n1067.88\n8.387e-01\n\n\n0\n8\n0.6497\n972.54\n5.160e-01\n\n\n0\n9\n-0.6307\n351.29\n5.286e-01\n\n\n0\n10\n-0.9677\n309.04\n3.340e-01\n\n\n0\n11\n-0.9268\n1073.79\n3.543e-01\n\n\n0\n12\n-0.4002\n1197.17\n6.891e-01\n\n\n0\n13\n3.3604\n676.59\n8.220e-04\n\n\n0\n14\n5.6601\n890.34\n2.040e-08\n\n\n0\n15\n0.4232\n395.82\n6.724e-01\n\n\n0\n16\n3.3898\n317.82\n7.876e-04\n\n\n0\n17\n5.1356\n889.20\n3.457e-07\n\n\n0\n18\n-0.7164\n361.98\n4.742e-01\n\n\n0\n19\n2.3094\n191.38\n2.199e-02\n\n\n0\n20\n4.1802\n873.54\n3.205e-05\n\n\n0\n21\n0.8667\n390.06\n3.866e-01\n\n\n0\n22\n0.2288\n64.77\n8.198e-01\n\n\n0\n23\n2.5427\n1081.13\n1.114e-02\n\n\n0\n24\n2.2873\n1080.95\n2.237e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nwe_D_baseline &lt;- mapply(we_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)\nbaseline_cohen_d(we_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n0.0334\n\n\n0\n2\n0.0778\n\n\n0\n3\n-0.0967\n\n\n0\n4\n0.0362\n\n\n0\n5\n0.0469\n\n\n0\n6\n-0.0989\n\n\n0\n7\n-0.0123\n\n\n0\n8\n0.0322\n\n\n0\n9\n-0.0483\n\n\n0\n10\n-0.0764\n\n\n0\n11\n-0.0479\n\n\n0\n12\n-0.0216\n\n\n0\n13\n0.2229\n\n\n0\n14\n0.2874\n\n\n0\n15\n0.0327\n\n\n0\n16\n0.2636\n\n\n0\n17\n0.2567\n\n\n0\n18\n-0.0557\n\n\n0\n19\n0.2040\n\n\n0\n20\n0.2103\n\n\n0\n21\n0.0657\n\n\n0\n22\n0.0271\n\n\n0\n23\n0.1374\n\n\n0\n24\n0.1205\n\n\n\n\n\n\n\n\n\nMean Differences\n\nwe_mean_baseline &lt;- mapply(we_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months\nbaseline_mean_diff(we_mean_baseline)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n0.0531\n\n\n0\n2\n0.1227\n\n\n0\n3\n-0.1575\n\n\n0\n4\n0.0545\n\n\n0\n5\n0.0718\n\n\n0\n6\n-0.1605\n\n\n0\n7\n-0.0191\n\n\n0\n8\n0.0495\n\n\n0\n9\n-0.0766\n\n\n0\n10\n-0.1218\n\n\n0\n11\n-0.0731\n\n\n0\n12\n-0.0335\n\n\n0\n13\n0.3443\n\n\n0\n14\n0.4207\n\n\n0\n15\n0.0531\n\n\n0\n16\n0.4180\n\n\n0\n17\n0.3815\n\n\n0\n18\n-0.0896\n\n\n0\n19\n0.3273\n\n\n0\n20\n0.3090\n\n\n0\n21\n0.1048\n\n\n0\n22\n0.0439\n\n\n0\n23\n0.2022\n\n\n0\n24\n0.1813"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thinking-2",
    "href": "coding/t-test/T-tests.html#analytic-thinking-2",
    "title": "T-tests",
    "section": "Analytic Thinking",
    "text": "Analytic Thinking\n\nAnalytic &lt;- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) +\n  ggtitle(\"Analytic Thinking\") +\n  labs(x = \"Month\", y = 'Standardized score') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nAnalytic &lt;- Analytic + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=43,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=43,label=\"Winter 2020 surge\", size = 3)\nAnalytic"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cogproc-2",
    "href": "coding/t-test/T-tests.html#cogproc-2",
    "title": "T-tests",
    "section": "Cogproc",
    "text": "Cogproc\n\nCogproc &lt;- ggplot(data=df2, aes(x=Date_mean, y=cogproc_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=cogproc_mean-cogproc_std.error, ymax=cogproc_mean+cogproc_std.error), alpha=0.2) +\n  ggtitle(\"Cognitive Processing\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nCogproc &lt;- Cogproc + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=12.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=12.5,label=\"Winter 2020 surge\", size = 3)\nCogproc"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words-3",
    "href": "coding/t-test/T-tests.html#i-words-3",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\ni &lt;- ggplot(data=df2, aes(x=Date_mean, y=i_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=i_mean-i_std.error, ymax=i_mean+i_std.error), alpha=0.2) +\n  ggtitle(\"I-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\ni &lt;- i + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=1.95,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=1.95,label=\"Winter 2020 surge\", size = 3)\ni"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words-3",
    "href": "coding/t-test/T-tests.html#we-words-3",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nwe &lt;- ggplot(data=df2, aes(x=Date_mean, y=we_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=we_mean-we_std.error, ymax=we_mean+we_std.error), alpha=0.2) +\n  ggtitle(\"We-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nwe &lt;- we + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=6.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=6.5,label=\"Winter 2020 surge\", size = 3)\nwe"
  },
  {
    "objectID": "coding/t-test/T-tests.html#tie-them-all-together",
    "href": "coding/t-test/T-tests.html#tie-them-all-together",
    "title": "T-tests",
    "section": "Tie them all together",
    "text": "Tie them all together\n\ngraphs &lt;- ggpubr::ggarrange(Analytic,Cogproc,i,we,ncol=2, nrow=2, common.legend = TRUE, legend = \"bottom\")\nannotate_figure(graphs,\n                top = text_grob(\"CEOs' Language Change\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Note. Vertical Line Represents the onset of the pandemic. \\n\\ Horizontal shading represents Standard Error. Vertical bars represent virus surges.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 16))"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Department of Psychology\nPrinceton University\nsm9518@princeton.edu\n\n\n\nPrinceton University (2024-Present)\nPh.D.¬†in Psychology\nAdvisor: Dr.¬†Erik Nook\nTexas State University (2019-2022)\nM.A.¬†in Psychological Research\nAdvisor: Dr.¬†Randall Osborne\nSouthwestern University (2015-2019)\nB.A. in Psychology (Minor: Spanish)\n\n\n\n\n* Denotes Shared First Authorship\nMesquiti, S. & Nook, E.C. (in Press.). Large Language Models in. Mental Health Treatment and Research. Annual Reviews BioMedical Data Science. DOI\nMuzekari, B., Cooper, N., Resnick, A., Paul, A. M., Torres-Grillo, O. E., Andrews, M. E., Mattan, B., Scholz, C., Johnson, D., Carreras-Tartak, J., Cakar, M. E., Hao, S., Zhou, E., Beard, E., Mesquiti, S., Sayed, F., Fichman, M. A., Lydon-Staley, D. M., Barnett, I. J., ‚Ä¶ Falk, E. B. (2025). Naturalistic Tobacco Retail Exposure and Smoking Outcomes in Adults Who Smoke Cigarettes Daily. JAMA Network Open. DOI\nMesquiti, S., Cosme, D., Nook, E.C., Falk E.B., Burns S. (Under Revision). Predicting Psychological and Subjective Well-being through Language-based Assessments of Well-being.\nMesquiti, S.,* Seraj, S.,* Weyland, A. H., Ashokkumar, A., Boyd, R. L., Mihalcea, R., & Pennebaker, J. W. (2025). Analysis of social media language reveals the psychological interaction of three successive upheavals. Scientific Reports, 15(1), 5740. DOI.\nKang, Y., Mesquiti, S., Baik, E.S., & Falk, E. B. (2024). Empathy and Helping: The Role of Affect in Response to Others‚Äô Suffering. Scientific Reports, 15, 3256 DOI.\nMesquiti, S., Seraj, S. (2023). The Psychological Impacts of the COVID-19 Pandemic on Business Leadership. PLOS One. DOI\n\n\n\n\n\nKang, Y., Mesquiti, S. & Falk, E. B. (in prep). The Effect of Compassion Training on Emotional Support Giving.\nMesquiti, S., Stade E. C., Hull, T.D., Nook, E. C. (in prep.) Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset.\n\n\n\n\n\n\n2025: Princeton University Data Driven Social Science Graduate Fellowship ($1,200)\n2025: Princeton University Data Driven Social Science: Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments ($4,283)\n2025: New Jersey Health Foundation: Investigating Racial Equity of Psycholinguistic Measures of Mental Health (PI: Erik C. Nook; $34,867; # PC 42-25)\n2022: Northwestern University Kellogg School of Management Behavioral Research Fellowship (Finalist [Declined]; $35,000)\n2020: Texas State University Thesis Research Support Fellowship ($1,070)\n\n\n\n\n\n\n2025: Princeton University Data Driven Social Science Graduate Fellowship\n2024-25: Princeton University William G. Bowen Merit Fellowship\n2020-21: Texas State University Graduate College of Liberal Arts Scholarship\n2021: Masters in Psychological Research Graduate Competitive Scholarship\n2019: Texas State University Graduate College of Education Scholarship\n2015: Southwestern University Mood Scholar\n\n\n\n\n\n\n2025: Predicting Mental Health Outcomes with the Latent Meaning of ‚ÄúI‚Äù in a Large Therapy Dataset. DDSS Connect: Interdisciplinary Exchange.\n2025: Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset. Social Psychology Seminar, Princeton University, Princeton, NJ.\n2022: Within-and between-person effects of autonomous motivation on goal pursuit. Duckworth Lab.\n2021: The whole truth and nothing but the truth: an analysis of the variability of prosocial lying in the Netherlands and the United States. Texas State University Psychology Department Brown Bag.\n2021: The truth about prosocial lying: Cross-cultural differences in prosocial lying. Southwestern Psychological Association (Virtual).\n\n\n\n\n\n* Denotes Undergraduate Mentored\n\nMesquiti, S. C., Nook, E. (2024, March). Investigating Racial Sensitivity in Language-Based Assessments of Mental Health. Poster presented at the annual meeting for the Society for Affective Science.\nMesquiti, S. C., Cosme, D., Nook, E., Falk E., Burns S. (2025, February). Predicting Well-being with Language-based Assessments. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Burns S., Cosme, D., Falk E. (2024, February). Enhancing Psychological Well-being Prediction Through Natural Language Analysis. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Mobasser, A., Falk, E., Pfeifer, J. H., & Cosme, D (2023, February). Within-and between-person effects of autonomous motivation on goal pursuit. Poster submitted to the annual meeting for the Society for Personality and Social Psychology.\nSpehar, A.,* Muzekari, B., Butler, T. B., Mesquiti, S.C., Cosme, D., Falk, E. (2022, August). Exploring the Relationships between Linguistic Arousal, Emotional Tone, and Sharing Behaviors Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa. Github\nAndrews, M. E., Cooper, N., Paul, A., Johnson, D., Muzekari, B., Mesquiti, S.C., Torres, O., Resnick, A., Scholz, C., Mattan, B., Barnet, I., Henriksen, L., Strasser, A., & Falk, E. B. (2023, May). Compounding effect of microaggressions and exposure to tobacco advertising on stress and smoking. Annual conference of the International Communication Association, Toronto, Ontario, Canada.\nSantana, C.,* Mesquiti, S.C., Carreras-Tartak, J., Kang, Y., Falk, E. (2022, August). Relating Collective and Self-Focus Language in Social Support with Self-Reported Depression. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nWoolfolk, Z*., Cosme, D., Butler, T., Carreras-Tartak, J., Mesquiti, S.C., Kang, Y., Falk, E. (2022, August). Racial Homophily in Emotion Sharing Network. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nMesquiti, S.C. (2022, April). Exploring Cognitive Language of CEOs during the COVID-19 Pandemic Over Time and its Connection to Decision Processing. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Tsai, J. L., Marion, J., Olivarez, O., Blackburn, K., Durland, M. (2022, February). The linguistic lifespan of a CEO: Exploring the way cognitive language unfolds over time and its connection to decision processing. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nHaskard-Zolnierek, K., Mesquiti, S.C., & Snyder, M. (2022, February). Associations between patient satisfaction and physician and nurse word use. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C., Kok, R., Clegg, J. M., Warnell, K. R. (2021, April). The truth behind prosocial lies: A linguistic analysis of lying to be polite. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Domer, K., Warnell, K. R., Clegg, J. M. (2021, February). More than a disappointing gift: Examining variability in when and how we prosocially lie. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nDomer, K., Mesquiti, S.C., Warnell, K. R., Clegg, J. M. (2021, February). More than a social norm: Examining who we lie to and what we lie about. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C. (2019, April). Internship at the Central Texas Treatment Center. Poster presented at Southwestern University Creative Works Symposium, Georgetown, Tx.\n\n\n\n\n\n\nSong Ting Tang | Thesis Student | Princeton University (2025-Present)\nEmi Yun | Thesis Student | Princeton University (2024-Present)\nArden Spehar | Summer MindCORE REU Student | University of Pennsylvania (2022)\nCarlos Santana | Summer MindCORE REU Student | University of Pennsylvania (2022)\n\n\n\n\n\n\nEmily Falk\nProfessor of Communication, Psychology, Marketing, and OID (Operations, Informatics, and Decisions), Vice Dean of the Annenberg School for Communication, University of Pennsylvania\nefalk@falklab.org\nJames Pennebaker\nProfessor Emeritus of Psychology, The University of Texas at Austin\npennebaker@utexas.edu\nErik Nook\nAssistant Professor of Psychology, Princeton University\nenook@princeton.edu"
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "CV",
    "section": "",
    "text": "Princeton University (2024-Present)\nPh.D.¬†in Psychology\nAdvisor: Dr.¬†Erik Nook\nTexas State University (2019-2022)\nM.A.¬†in Psychological Research\nAdvisor: Dr.¬†Randall Osborne\nSouthwestern University (2015-2019)\nB.A. in Psychology (Minor: Spanish)"
  },
  {
    "objectID": "CV.html#publications",
    "href": "CV.html#publications",
    "title": "CV",
    "section": "",
    "text": "* Denotes Shared First Authorship\nMesquiti, S. & Nook, E.C. (in Press.). Large Language Models in. Mental Health Treatment and Research. Annual Reviews BioMedical Data Science. DOI\nMuzekari, B., Cooper, N., Resnick, A., Paul, A. M., Torres-Grillo, O. E., Andrews, M. E., Mattan, B., Scholz, C., Johnson, D., Carreras-Tartak, J., Cakar, M. E., Hao, S., Zhou, E., Beard, E., Mesquiti, S., Sayed, F., Fichman, M. A., Lydon-Staley, D. M., Barnett, I. J., ‚Ä¶ Falk, E. B. (2025). Naturalistic Tobacco Retail Exposure and Smoking Outcomes in Adults Who Smoke Cigarettes Daily. JAMA Network Open. DOI\nMesquiti, S., Cosme, D., Nook, E.C., Falk E.B., Burns S. (Under Revision). Predicting Psychological and Subjective Well-being through Language-based Assessments of Well-being.\nMesquiti, S.,* Seraj, S.,* Weyland, A. H., Ashokkumar, A., Boyd, R. L., Mihalcea, R., & Pennebaker, J. W. (2025). Analysis of social media language reveals the psychological interaction of three successive upheavals. Scientific Reports, 15(1), 5740. DOI.\nKang, Y., Mesquiti, S., Baik, E.S., & Falk, E. B. (2024). Empathy and Helping: The Role of Affect in Response to Others‚Äô Suffering. Scientific Reports, 15, 3256 DOI.\nMesquiti, S., Seraj, S. (2023). The Psychological Impacts of the COVID-19 Pandemic on Business Leadership. PLOS One. DOI"
  },
  {
    "objectID": "CV.html#works-in-progress",
    "href": "CV.html#works-in-progress",
    "title": "CV",
    "section": "",
    "text": "Kang, Y., Mesquiti, S. & Falk, E. B. (in prep). The Effect of Compassion Training on Emotional Support Giving.\nMesquiti, S., Stade E. C., Hull, T.D., Nook, E. C. (in prep.) Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset."
  },
  {
    "objectID": "CV.html#funding",
    "href": "CV.html#funding",
    "title": "CV",
    "section": "",
    "text": "2025: Princeton University Data Driven Social Science Graduate Fellowship ($1,200)\n2025: Princeton University Data Driven Social Science: Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments ($4,283)\n2025: New Jersey Health Foundation: Investigating Racial Equity of Psycholinguistic Measures of Mental Health (PI: Erik C. Nook; $34,867; # PC 42-25)\n2022: Northwestern University Kellogg School of Management Behavioral Research Fellowship (Finalist [Declined]; $35,000)\n2020: Texas State University Thesis Research Support Fellowship ($1,070)"
  },
  {
    "objectID": "CV.html#fellowships",
    "href": "CV.html#fellowships",
    "title": "CV",
    "section": "",
    "text": "2025: Princeton University Data Driven Social Science Graduate Fellowship\n2024-25: Princeton University William G. Bowen Merit Fellowship\n2020-21: Texas State University Graduate College of Liberal Arts Scholarship\n2021: Masters in Psychological Research Graduate Competitive Scholarship\n2019: Texas State University Graduate College of Education Scholarship\n2015: Southwestern University Mood Scholar"
  },
  {
    "objectID": "CV.html#invited-talks",
    "href": "CV.html#invited-talks",
    "title": "CV",
    "section": "",
    "text": "2025: Predicting Mental Health Outcomes with the Latent Meaning of ‚ÄúI‚Äù in a Large Therapy Dataset. DDSS Connect: Interdisciplinary Exchange.\n2025: Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset. Social Psychology Seminar, Princeton University, Princeton, NJ.\n2022: Within-and between-person effects of autonomous motivation on goal pursuit. Duckworth Lab.\n2021: The whole truth and nothing but the truth: an analysis of the variability of prosocial lying in the Netherlands and the United States. Texas State University Psychology Department Brown Bag.\n2021: The truth about prosocial lying: Cross-cultural differences in prosocial lying. Southwestern Psychological Association (Virtual)."
  },
  {
    "objectID": "CV.html#conference-presentations",
    "href": "CV.html#conference-presentations",
    "title": "CV",
    "section": "",
    "text": "* Denotes Undergraduate Mentored\n\nMesquiti, S. C., Nook, E. (2024, March). Investigating Racial Sensitivity in Language-Based Assessments of Mental Health. Poster presented at the annual meeting for the Society for Affective Science.\nMesquiti, S. C., Cosme, D., Nook, E., Falk E., Burns S. (2025, February). Predicting Well-being with Language-based Assessments. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Burns S., Cosme, D., Falk E. (2024, February). Enhancing Psychological Well-being Prediction Through Natural Language Analysis. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Mobasser, A., Falk, E., Pfeifer, J. H., & Cosme, D (2023, February). Within-and between-person effects of autonomous motivation on goal pursuit. Poster submitted to the annual meeting for the Society for Personality and Social Psychology.\nSpehar, A.,* Muzekari, B., Butler, T. B., Mesquiti, S.C., Cosme, D., Falk, E. (2022, August). Exploring the Relationships between Linguistic Arousal, Emotional Tone, and Sharing Behaviors Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa. Github\nAndrews, M. E., Cooper, N., Paul, A., Johnson, D., Muzekari, B., Mesquiti, S.C., Torres, O., Resnick, A., Scholz, C., Mattan, B., Barnet, I., Henriksen, L., Strasser, A., & Falk, E. B. (2023, May). Compounding effect of microaggressions and exposure to tobacco advertising on stress and smoking. Annual conference of the International Communication Association, Toronto, Ontario, Canada.\nSantana, C.,* Mesquiti, S.C., Carreras-Tartak, J., Kang, Y., Falk, E. (2022, August). Relating Collective and Self-Focus Language in Social Support with Self-Reported Depression. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nWoolfolk, Z*., Cosme, D., Butler, T., Carreras-Tartak, J., Mesquiti, S.C., Kang, Y., Falk, E. (2022, August). Racial Homophily in Emotion Sharing Network. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nMesquiti, S.C. (2022, April). Exploring Cognitive Language of CEOs during the COVID-19 Pandemic Over Time and its Connection to Decision Processing. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Tsai, J. L., Marion, J., Olivarez, O., Blackburn, K., Durland, M. (2022, February). The linguistic lifespan of a CEO: Exploring the way cognitive language unfolds over time and its connection to decision processing. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nHaskard-Zolnierek, K., Mesquiti, S.C., & Snyder, M. (2022, February). Associations between patient satisfaction and physician and nurse word use. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C., Kok, R., Clegg, J. M., Warnell, K. R. (2021, April). The truth behind prosocial lies: A linguistic analysis of lying to be polite. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Domer, K., Warnell, K. R., Clegg, J. M. (2021, February). More than a disappointing gift: Examining variability in when and how we prosocially lie. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nDomer, K., Mesquiti, S.C., Warnell, K. R., Clegg, J. M. (2021, February). More than a social norm: Examining who we lie to and what we lie about. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C. (2019, April). Internship at the Central Texas Treatment Center. Poster presented at Southwestern University Creative Works Symposium, Georgetown, Tx."
  },
  {
    "objectID": "CV.html#mentorship",
    "href": "CV.html#mentorship",
    "title": "CV",
    "section": "",
    "text": "Song Ting Tang | Thesis Student | Princeton University (2025-Present)\nEmi Yun | Thesis Student | Princeton University (2024-Present)\nArden Spehar | Summer MindCORE REU Student | University of Pennsylvania (2022)\nCarlos Santana | Summer MindCORE REU Student | University of Pennsylvania (2022)"
  },
  {
    "objectID": "CV.html#references",
    "href": "CV.html#references",
    "title": "CV",
    "section": "",
    "text": "Emily Falk\nProfessor of Communication, Psychology, Marketing, and OID (Operations, Informatics, and Decisions), Vice Dean of the Annenberg School for Communication, University of Pennsylvania\nefalk@falklab.org\nJames Pennebaker\nProfessor Emeritus of Psychology, The University of Texas at Austin\npennebaker@utexas.edu\nErik Nook\nAssistant Professor of Psychology, Princeton University\nenook@princeton.edu"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Coding Help",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSimple Correlation examples for students\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nT-tests\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nüé® R Data Visualization Adventure\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications and Preprints",
    "section": "",
    "text": "Publications and Preprints\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nEmpathy and helping: the role of affect in response to others‚Äô suffering\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of social media language reveals the psychological interaction of three successive upheavals\n\n\n\n\n\n\n\n\n\n\n\n\n\nArtificial Intelligence in Mental Health Treatment and Research\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Psychological and Subjective Well-being through Language-based Assessment\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe psychological impacts of the COVID-19 pandemic on business leadership\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/the-psychological-impacts-of-the-covid-19-pandemic-on-business-leadership/index.html",
    "href": "publications/the-psychological-impacts-of-the-covid-19-pandemic-on-business-leadership/index.html",
    "title": "The psychological impacts of the COVID-19 pandemic on business leadership",
    "section": "",
    "text": "Abstract\nThe COVID-19 pandemic had a profound impact on business leadership, specifically on chief executive officers (CEOs). To document the psychological impacts of the pandemic on corporate leadership, this study analyzed the language of CEOs during company quarterly earnings calls (N = 19,536) one year before and after the onset of the pandemic. Following the start of lockdowns, CEOs exhibited significant language shifts. Analytic thinking declined, and their language became less technical and more personal and intuitive. CEOs also showed signs of increased cognitive load as they grappled with the pandemic‚Äôs impact on their business practices. The study observed a substantial decrease in collective-focused language (we-usage) among CEOs, indicative of disconnection from their companies. Concurrently, there was an increase in self-focused (I-usage) language, suggesting heightened preoccupation among business leaders. The observed language changes reflect the unique effect of the pandemic on CEOs, which had some notable differences compared to the general population. This study sheds light on how the COVID-19 pandemic influenced business leaders‚Äô psychological states and decision-making strategies‚Äîprocesses that have a substantial impact on a company‚Äôs performance. The findings underscore the importance of language data in understanding large-scale societal events.\nCitation: Mesquiti, S., & Seraj, S. (2023). The psychological impacts of the COVID-19 pandemic on business leadership. PLoS ONE, 18(10), e0290621. https://doi.org/10.1371/journal.pone.0290621"
  },
  {
    "objectID": "publications/artificial-intelligence-in-mental-health-treatment-and-research/index.html",
    "href": "publications/artificial-intelligence-in-mental-health-treatment-and-research/index.html",
    "title": "Artificial Intelligence in Mental Health Treatment and Research",
    "section": "",
    "text": "Abstract\nMental health challenges add immensely to the global burden of disease, yet traditional approaches to psychological assessment and care remain resource-intensive and often inaccessible. There is widespread interest in testing whether advances in artificial intelligence, particularly large language models (LLMs), could address these constraints. This review focuses on LLMs, given the field‚Äôs explosive interest in testing whether their ability to generate context-sensitive language representations can aid large-scale assessment and intervention. We synthesize recent applications of LLMs, including language-based assessment of psychopathology, digital phenotyping, electronic health record analysis, and early integrations into psychotherapy. However, we highlight deep challenges of AI that loom large in the highly sensitive space of mental health treatment, including clear risks of bias, hallucinations, inappropriate (or even dangerous) therapeutic recommendations, and limited regulatory oversight. We conclude with future directions that are critical for the safe and equitable use of LLMs in mental health.\nCitation: Mesquiti, S., & Nook, E. C. (2025). Artificial Intelligence in Mental Health Treatment and Research. Artificial Intelligence and Mental Health."
  },
  {
    "objectID": "publications/empathy-and-helping-the-role-of-affect-in-response-to-others-suffering/index.html",
    "href": "publications/empathy-and-helping-the-role-of-affect-in-response-to-others-suffering/index.html",
    "title": "Empathy and helping: the role of affect in response to others‚Äô suffering",
    "section": "",
    "text": "Abstract\nDecades of research hold that empathy is a multifaceted construct. A related challenge in empathy research is to describe how each subcomponent of empathy uniquely contributes to social outcomes. Here, we examined distinct mechanisms through which different components of empathy‚ÄîEmpathic Concern, Perspective Taking, and Personal Distress‚Äîmay relate to prosociality. Participants (N = 77) watched a prerecorded video of a person sharing an emotional real-life story and provided verbal support in response. The listeners then reported how positive and negative they felt while listening to the story. We found that individuals with greater tendencies to experience Empathic Concern and Perspective Taking felt more positive (e.g., connected, compassionate), whereas those with higher Personal Distress felt more negative (e.g., nervous, anxious) in response to another‚Äôs suffering. We also observed indirect relationships between Empathic Concern / Perspective Taking and the tendency to help others through positive affective responses to the other‚Äôs suffering. These findings build upon the growing literature that distinguishes different components of empathy and their mechanisms that relate to divergent behavioral consequences. Results also highlight the role of positive affect that may motivate prosociality in the face of others‚Äô suffering.\nCitation: Kang, Y., Mesquiti, S., Baik, E. S., & Falk, E. B. (2025). Empathy and helping: the role of affect in response to others‚Äô suffering. Scientific Reports, 15, 3256. https://doi.org/10.1038/s41598-025-87221-2"
  },
  {
    "objectID": "posts/I-projections/Post.html#under-the-hood",
    "href": "posts/I-projections/Post.html#under-the-hood",
    "title": "I-Projection Results",
    "section": "Under the hood",
    "text": "Under the hood\nWhat this analysis does\n\nLoads a RoBERTa (a LLM) to embed participant text into high-dimensional vectors.\nDefines psychological axes (valence, ability) using controlled ‚ÄúI am‚Äù / ‚ÄúI feel‚Äù statements; each axis is the normalized difference between mean embeddings of positive vs.¬†negative statements.\nThis allows us to capture the psychological dimensions of interest in the embedding space (i.e., good-bad and able-unable).\nProjects target words (e.g., I, me, my) onto these axes by computing the dot product between each word‚Äôs embedding and the axis vector, capturing how strongly that word aligns with a psychological dimension.\nAggregates these projections at the text level to create text-level measures of valence and ability projections for each participant, which we can then use for down-stream analyses."
  },
  {
    "objectID": "posts/I-projections/Post.html#first_person_sing_valence_combined",
    "href": "posts/I-projections/Post.html#first_person_sing_valence_combined",
    "title": "I-Projection Results",
    "section": "first_person_sing_valence_combined",
    "text": "first_person_sing_valence_combined"
  },
  {
    "objectID": "posts/I-projections/Post.html#first_person_sing_ability_combined",
    "href": "posts/I-projections/Post.html#first_person_sing_ability_combined",
    "title": "I-Projection Results",
    "section": "first_person_sing_ability_combined",
    "text": "first_person_sing_ability_combined"
  },
  {
    "objectID": "posts/I-projections/Post.html#valence-projection",
    "href": "posts/I-projections/Post.html#valence-projection",
    "title": "I-Projection Results",
    "section": "Valence Projection",
    "text": "Valence Projection\n\n\nNon-disaggregatedDisaggregated"
  },
  {
    "objectID": "posts/I-projections/Post.html#ability-projection",
    "href": "posts/I-projections/Post.html#ability-projection",
    "title": "I-Projection Results",
    "section": "Ability Projection",
    "text": "Ability Projection\n\n\nNon-disaggregatedDisaggregated"
  },
  {
    "objectID": "posts/I-projections/Post.html#valence-projections",
    "href": "posts/I-projections/Post.html#valence-projections",
    "title": "I-Projection Results",
    "section": "Valence Projections",
    "text": "Valence Projections\n\n\nNon-disaggregated (Valence)Disaggregated (valence)"
  },
  {
    "objectID": "posts/I-projections/Post.html#ability-projections",
    "href": "posts/I-projections/Post.html#ability-projections",
    "title": "I-Projection Results",
    "section": "Ability Projections",
    "text": "Ability Projections\n\n\nNon-disaggregated (Ability)Disaggregated (ability)"
  },
  {
    "objectID": "posts/I-projections/Post.html#good-bad",
    "href": "posts/I-projections/Post.html#good-bad",
    "title": "I-Projection Results",
    "section": "Good-bad",
    "text": "Good-bad"
  },
  {
    "objectID": "posts/I-projections/Post.html#able-unable",
    "href": "posts/I-projections/Post.html#able-unable",
    "title": "I-Projection Results",
    "section": "Able-Unable",
    "text": "Able-Unable"
  },
  {
    "objectID": "posts/I-projections/Post.html#changes-over-therapy",
    "href": "posts/I-projections/Post.html#changes-over-therapy",
    "title": "I-Projection Results",
    "section": "Changes over Therapy",
    "text": "Changes over Therapy"
  },
  {
    "objectID": "posts/I-projections/Post.html#track-with-changes-in-symptoms",
    "href": "posts/I-projections/Post.html#track-with-changes-in-symptoms",
    "title": "I-Projection Results",
    "section": "Track with Changes in Symptoms",
    "text": "Track with Changes in Symptoms\n\n\nNon-disaggregatedDisaggregated"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#depression-related-open-ended-questions",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#depression-related-open-ended-questions",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Depression-related Open-ended Questions",
    "text": "Depression-related Open-ended Questions\n\n\nAt initial time point, participants will provide written responses to 9 open-ended questions about mood, motivation, sleep, energy, self-perception, and life satisfaction (‚âà150 words total; see below for full list of questions).\nAdapted from Hur et al.¬†(2024)\nConducted before mental health measures to avoid biasing responses.\n\n\nlibrary(DT)\n\n# Create the data frame\ndepression_questions &lt;- data.frame(\n  Question_Order = 1:9,\n  Question_Wording = c(\n    \"Could you describe your general mood in the past 2 weeks? Has your mood been higher or lower than usual? Are there any particular emotions that you have been feeling a lot lately?\",\n    \"How would you describe your level of interest in these things in the past 2 weeks? Has it been higher or lower than usual?\",\n    \"How have you been eating in the past 2 weeks? Is there anything that has been different compared to usual (e.g., eating more or less than usual)?\",\n    \"How would you describe your sleep patterns lately? Is there anything that has been different compared to usual?\",\n    \"In the past 2 weeks, has sitting still, moving, or talking been harder or easier than usual?\",\n    \"Sometimes we feel tired and exhausted, and sometimes we feel full of energy. How would you describe your energy level in the past 2 weeks?\",\n    \"How have you been feeling about yourself in the past 2 weeks?\",\n    \"How would you describe your thinking, concentration, and decision making in the past 2 weeks? Is there anything that has been harder or easier than usual?\",\n    \"Think about your life overall. Is there anything that you are particularly satisfied or dissatisfied with?\"\n  ),\n  stringsAsFactors = FALSE\n)\n\n# Render as a DT table\ndatatable(\n  depression_questions,\n  options = list(\n    scrollX = TRUE,      # allow horizontal scrolling\n     scrollY = \"300px\",   # vertical scrolling height\n    pageLength = 9,      # show all 9 questions at once\n    autoWidth = TRUE\n  ),\n  rownames = FALSE\n)"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#primary-mental-health-measures",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#primary-mental-health-measures",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Primary Mental Health Measures",
    "text": "Primary Mental Health Measures\n\nGAD-7 (anxiety; measured at both time points)\nPHQ-8 (depression; measured at both time points)\n\nIf highly correlated, will collapse."
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#standard-demographics",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#standard-demographics",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Standard Demographics",
    "text": "Standard Demographics\nAssessed at initial timepoint\n\nAge & ResidenceGenderRaceSES\n\n\n\nAge: ___\nState of residence: ___\n\n\n\n\n\ngender_df &lt;- tibble::tibble(\n  Question = c(\n    \"Sex assigned at birth\",\n    \"Identify with a gender different from sex assigned at birth?\",\n    \"Gender identity\",\n    \"Sexual identity/orientation (select all)\"\n  ),\n  Options = c(\n    \"Male; Female; Other (specify); I prefer not to answer\",\n    \"Yes; No; Unsure; Prefer not to answer\",\n    \"Male; Female; Non-binary; Unsure; Other (specify); Prefer not to answer\",\n    \"Straight/heterosexual; Gay / Lesbian / Homosexual; Bisexual / Pansexual / Queer; Asexual / Unsure / Other (specify); Prefer not to answer\"\n  )\n)\n\n# Render as a DT table\ndatatable(\n  gender_df,\n  rownames = FALSE,\n  escape = FALSE,\n  options = list(\n    scrollX = TRUE,       # horizontal scrolling\n    scrollY = \"400px\",    # vertical scrolling\n    paging = FALSE,       # show all rows in scrollable panel\n    autoWidth = TRUE\n  )\n)\n\n\n\n\n\n\n\n\n\n\nrace_df &lt;- tibble::tibble(\n  Question = c(\n    \"Race (select all)\",\n    \"Ethnicity\"\n  ),\n  Options = c(\n    \"White; Black or African American; Asian or Asian American; American Indian or Alaska Native; Middle Eastern; Native Hawaiian or Other Pacific Islander; Other (specify); Prefer not to answer\",\n    \"Hispanic/Latino; Not Hispanic/Latino; Prefer not to answer\"\n  )\n)\n\n# Render as a DT table\ndatatable(\n  race_df,\n  rownames = FALSE,\n  escape = FALSE,\n  options = list(\n    scrollX = TRUE,        # horizontal scrolling\n    scrollY = \"300px\",     # vertical scrolling\n    paging = FALSE,        # show all rows\n    autoWidth = TRUE\n  )\n)\n\n\n\n\n\n\n\n\n\n\nses_df &lt;- tibble::tibble(\n  Question = c(\n    \"Education (highest degree attained)\",\n    \"Household income\",\n    \"Household composition\",\n    \"Perceived SES\"\n  ),\n  Options = c(\n    \"Less than high school; High school / GED; Associate‚Äôs; Bachelor‚Äôs; Master‚Äôs; Professional (MD, JD, etc.); Doctorate; Other (specify)\",\n    \"&lt; $5,000 ‚Ä¶ &gt; $200,000; Don‚Äôt know / Prefer not to answer\",\n    \"# people in household; # children; # adults; # adults bringing in income\",\n    \"MacArthur Scale of Subjective Social Status\"\n  )\n)\n\n# Render as a DT table\ndatatable(\n  ses_df,\n  rownames = FALSE,\n  escape = FALSE,\n  options = list(\n    scrollX = TRUE,        # horizontal scrolling\n    scrollY = \"400px\",     # vertical scrolling\n    paging = FALSE,        # show all rows\n    autoWidth = TRUE\n  )\n)"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#other-measures",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#other-measures",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Other Measures‚Ä¶",
    "text": "Other Measures‚Ä¶\n\nInitial Timepoint\n\nBilingualism (using LEAP-Q)\n\nBoth Timepoints\n\nSatisfaction with Life Short Scale\n\nUCLA Loneliness Scale\n\nFollow-up Only\n\nSocial Media Use\n\nSocial Media‚Äìderived Language-based Assessments"
  },
  {
    "objectID": "posts/Federal research funding cuts will stall scientific progress, hurt Texas students/Federal research funding cuts will stall scientific progress, hurt Texas students.html",
    "href": "posts/Federal research funding cuts will stall scientific progress, hurt Texas students/Federal research funding cuts will stall scientific progress, hurt Texas students.html",
    "title": "Federal research funding cuts will stall scientific progress, hurt Texas students",
    "section": "",
    "text": "This piece was published as an Op-ed in the San Antonio Express News\nScientific progress in the United States faces a series of threats. These disruptions have slashed federal research funding; upended the scientific process; dismantled diversity, equity and inclusion ‚Äî DEI ‚Äî initiatives; and endangered the careers of future and current scientists.\nIn particular, the federal government has frozen billions in funding for the National Institutes of Health and the National Science Foundation ‚Äî both of which provide substantial medical and economic benefits to San Antonio and Texas.\nLast year, the NIH awarded nearly $2 billion in funding to Texan institutes, which supported more than 30,000 Texan jobs and generated more than $6 billion in economic activity.\nAs a scientist, I am deeply concerned about these funding cuts. I strongly believe in the direct and indirect opportunities that science provides to improve the lives of our community members.\nI am a Mexican American who was born and raised on San Antonio‚Äôs West Side, where programs like the Pre-Freshman Engineering Program, or PREP, gave me early exposure to careers in science and laid the foundation for my academic journey.\nToday, I am fortunate enough to be a psychology doctoral student at Princeton University, where I am conducting research showing how to identify individuals struggling with poor mental health by examining patterns in their language use.\nPrograms like PREP provided my first introduction to research, which was further developed in labs at Texas State University, the University of Texas at Austin and the University of Pennsylvania ‚Äî all institutions supported by grants from the NIH and NSF.\nPrograms like PREP, often labeled as ‚ÄúDEI initiatives,‚Äù are not about exclusion. They are about giving everyone a shot at the American Dream. They ensure young people without built-in academic connections or financial resources have opportunities to pursue careers in science.\nWithout these programs, countless students ‚Äî regardless of race, gender or political affiliation ‚Äî will lose their chance to break into fields that shape our nation‚Äôs future.\nStripping funding from anything perceived as ‚ÄúDEI-adjacent‚Äù harms us all. It reduces available jobs in the state, freezes life-changing research and shuts the door on talented, hardworking students.\nTo make matters worse, recent government budget cuts have slashed funding for universities, including critical ‚Äúindirect costs‚Äù that fund lab space, technical support, and salaries for scientists and staff.\nUniversities across the country, including the University of Texas at San Antonio, have already begun reducing graduate admissions. This disruption will shrink the pipeline of trained scientists, limit opportunities for young San Antonians, and jeopardize scientific progress in Texas and beyond.\nDue to decades of federally funded research, we have breakthroughs like revolutionary cancer treatments and lifesaving vaccines. Similar advancements have been made in our ability to detect mental health issues with language and provide more personalized support ‚Äî progress that can save countless lives.\nThese advancements depend on continued support from institutions like the NIH and NSF, which drive our economy, research and the training of future scientists.\nPrograms that provide early exposure to science ‚Äî especially for underrepresented students ‚Äî ensure that the next generation is prepared to tackle issues like the mental health epidemic.\nCutting funding for research and training will stall progress and limit opportunities for young minds. Now more than ever, we must fight to protect science and its future to build a healthier, more resilient society for everyone.\n\nAbout the Author\nSteven Mesquiti was born and raised in San Antonio‚Äôs West Side. He completed his undergraduate studies at Southwestern University, has a master‚Äôs degree from Texas State University and is a doctoral student at Princeton University."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html",
    "href": "posts/LLM on HPC/LLM-to-HPC.html",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "This guide walks you through configuring your environment, authenticating access, and downloading the LLaMA 3 models from Hugging Face on the Adroit HPC system. It assumes you have basic familiarity with command line, Python, and HPC usage.\nThis process is infinitely easier if you have connected VSCode to the adroit cluster. Here is some info on that. IT also runs help sessions on this, which are very useful.\n\n\nBy default, Hugging Face stores downloaded models in your home directory (e.g., /home/username/.cache/huggingface), which may have limited storage on HPC systems. Redirect the cache to your scratch directory for ample space.\nYou can do that by using the checkquota command in the terminal to find out how much space you have in your scratch directory.\n\nSet the environment variable HF_HOME to your scratch directory:\n\nOn the Adroit login node, run this command to append the export statement to your .bashrc:\necho \"export HF_HOME=/scratch/network/$USER/.cache/huggingface/\" &gt;&gt; $HOME/.bashrc\n\nReload your shell configuration:\n\nsource ~/.bashrc\n\nVerify the variable is set:\n\necho $HF_HOME\nThe excpected output should be:\n/scratch/network/&lt;YourNetID&gt;/.cache/huggingface/\n\n\n\nMeta requires users to accept a license and gain explicit access to the LLaMA 3 models on Hugging Face. So, this means you‚Äôll need sign up for a Hugging Face account and request access to the LLaMA 3 models.\n\nGo to the LLaMA 3 model page on Hugging Face: https://huggingface.co/meta-llama/Llama-3.1-8B (or whatever model you want access to)\nLog in or create a Hugging Face account if you haven‚Äôt already.\nAccept the model license terms: Click the ‚ÄúAccess repository‚Äù button and agree to the license to request access.\nWait for access to be granted. This should be relatively quick, but may take a few minutes to a few hours depending on demand.\n\n\n\n\nOnce access is granted, authenticate your HPC environment to allow downloading protected models.\n\nLog in to Hugging Face CLI:\n\nOn the Adroit login node, run the following command:\nhuggingface-cli login\n\nEnter your Hugging Face token:\n\nYou will be prompted to enter your Hugging Face access token. You can find this token in your Hugging Face account settings under ‚ÄúAccess Tokens‚Äù. Copy and paste it into the terminal when prompted. Make sure not to share this with anyone since this is a personal access token that allows downloading models.\n\n\n\nNow that you have authenticated, you can download the LLaMA 3 model to your scratch directory.\n\nCreate a Python script download_llama3.py with this content:\n\nMake sure to replace meta-llama/Llama-3.1-8B with the specific model you want\nto download if different. Also, make sure you have transformers installed in your Python environment.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"meta-llama/Llama-3.1-8B\"\ncache_path = \"/scratch/network/sm9518/.cache/huggingface\"  # replace with your actual NetID\n\n# Download model and tokenizer to cache\nAutoTokenizer.from_pretrained(model_id, cache_dir=cache_path)\nAutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_path)\n\nprint(f\"{model_id} Downloaded Successfully! to {cache_path}\")\n\nRun the script on the login node:\n\npython download_llama3.py\n\nThis will download all necessary model files into your scratch cache directory set by HF_HOME.\n\n\n\n\nNow that you have downloaded the LLaMA 3 model to your scratch directory, you can run inference on an HPC compute node.\n\n\nSave the following code to /scratch/network/$USER/python_test/run_test_llama.py. This script loads the model and runs a short text generation example using the transformers pipeline API.\nfrom transformers import pipeline\nimport torch\nimport os\n\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif not torch.cuda.is_available():\n    raise ValueError(\n        \"CUDA is not available. Make sure you are running this on a GPU node. \"\n        \"For example, run with Slurm requesting GPU:\\n\\n\"\n        \"\\tsalloc -t 0:10:00 --ntasks=1 --gres=gpu:1 python run_test_llama.py\"\n    )\n\nmodel_path = \"/scratch/network/$USER/.cache/huggingface/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n\nprint(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n\npipe = pipeline(\"text-generation\", model=model_path, tokenizer=model_path)\n\nprompt = \"You are an expert psychologist. Tell me something interesting about psychology regarding Erik Nook's research:\"\noutput = pipe(prompt, max_new_tokens=50)\n\nprint(\"\\nModel output:\\n\", output)\n\nMake sure to replace $USER in the path with your actual NetID or use a variable if running programmatically.\n\n\n\n\nThis Slurm batch script requests one A100 GPU on the Adroit cluster and runs the above Python test script.\n#!/bin/bash\n#SBATCH --job-name=llama3-textgen           # Job name\n#SBATCH --nodes=1                           # Use one node\n#SBATCH --ntasks=1                          # One task\n#SBATCH --cpus-per-task=1                   # One CPU core\n#SBATCH --mem=36G                          # Memory request # can be ess\n#SBATCH --gres=gpu:1                        # Request 1 GPU\n#SBATCH --time=00:10:00                     # Max runtime (adjust as needed)\n#SBATCH --constraint=a100                   # Use A100 GPU\n#SBATCH --nodelist=adroit-h11g1             # Run on node with free GPUs\n#SBATCH --mail-type=ALL                     # Email on start, end, fail\n#SBATCH --mail-user=sm9518@princeton.edu   # Your email\n\nmodule purge\nmodule load anaconda3/2024.6\nmodule load cudatoolkit/11.8 \n\nsource activate talkspaceMADS\n\ncd /scratch/network/$USER/python_test\n\necho \"Job started at $(date)\"\n\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\npython run_test_llama.py\n\necho \"Job completed at $(date)\"\n\nüí° Check GPU Node Usage Before Selecting a Node or GPU.\nBefore submitting your job or manually specifying a GPU node (e.g., with #SBATCH --nodelist=adroit-h11g1).\nIt‚Äôs a good idea to check which nodes and GPUs have free memory or are under low load. Otherwise, your job might be assigned to a GPU that is already fully used, causing CUDA out-of-memory errors.\n\nOn Adroit, you can use commands like these from the login node to check GPU availability:\n# Show GPU status and free GPUs per node \nshownodes -p gpu\nIf you don‚Äôt specify a node, Slurm will pick one for you, but it might not always be the best choice if GPUs on that node are busy."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-1-configure-the-hugging-face-cache-directory",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-1-configure-the-hugging-face-cache-directory",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "By default, Hugging Face stores downloaded models in your home directory (e.g., /home/username/.cache/huggingface), which may have limited storage on HPC systems. Redirect the cache to your scratch directory for ample space.\nYou can do that by using the checkquota command in the terminal to find out how much space you have in your scratch directory.\n\nSet the environment variable HF_HOME to your scratch directory:\n\nOn the Adroit login node, run this command to append the export statement to your .bashrc:\necho \"export HF_HOME=/scratch/network/$USER/.cache/huggingface/\" &gt;&gt; $HOME/.bashrc\n\nReload your shell configuration:\n\nsource ~/.bashrc\n\nVerify the variable is set:\n\necho $HF_HOME\nThe excpected output should be:\n/scratch/network/&lt;YourNetID&gt;/.cache/huggingface/"
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-2-get-authentication-access-from-meta-required-for-llama-models",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-2-get-authentication-access-from-meta-required-for-llama-models",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Meta requires users to accept a license and gain explicit access to the LLaMA 3 models on Hugging Face. So, this means you‚Äôll need sign up for a Hugging Face account and request access to the LLaMA 3 models.\n\nGo to the LLaMA 3 model page on Hugging Face: https://huggingface.co/meta-llama/Llama-3.1-8B (or whatever model you want access to)\nLog in or create a Hugging Face account if you haven‚Äôt already.\nAccept the model license terms: Click the ‚ÄúAccess repository‚Äù button and agree to the license to request access.\nWait for access to be granted. This should be relatively quick, but may take a few minutes to a few hours depending on demand."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-3-log-in-to-hugging-face-cli-on-hpc",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-3-log-in-to-hugging-face-cli-on-hpc",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Once access is granted, authenticate your HPC environment to allow downloading protected models.\n\nLog in to Hugging Face CLI:\n\nOn the Adroit login node, run the following command:\nhuggingface-cli login\n\nEnter your Hugging Face token:\n\nYou will be prompted to enter your Hugging Face access token. You can find this token in your Hugging Face account settings under ‚ÄúAccess Tokens‚Äù. Copy and paste it into the terminal when prompted. Make sure not to share this with anyone since this is a personal access token that allows downloading models."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-4-download-the-llama-3-model-on-the-login-node",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-4-download-the-llama-3-model-on-the-login-node",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Now that you have authenticated, you can download the LLaMA 3 model to your scratch directory.\n\nCreate a Python script download_llama3.py with this content:\n\nMake sure to replace meta-llama/Llama-3.1-8B with the specific model you want\nto download if different. Also, make sure you have transformers installed in your Python environment.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"meta-llama/Llama-3.1-8B\"\ncache_path = \"/scratch/network/sm9518/.cache/huggingface\"  # replace with your actual NetID\n\n# Download model and tokenizer to cache\nAutoTokenizer.from_pretrained(model_id, cache_dir=cache_path)\nAutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_path)\n\nprint(f\"{model_id} Downloaded Successfully! to {cache_path}\")\n\nRun the script on the login node:\n\npython download_llama3.py\n\nThis will download all necessary model files into your scratch cache directory set by HF_HOME."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-5-test-the-downloaded-model",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-5-test-the-downloaded-model",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Now that you have downloaded the LLaMA 3 model to your scratch directory, you can run inference on an HPC compute node.\n\n\nSave the following code to /scratch/network/$USER/python_test/run_test_llama.py. This script loads the model and runs a short text generation example using the transformers pipeline API.\nfrom transformers import pipeline\nimport torch\nimport os\n\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif not torch.cuda.is_available():\n    raise ValueError(\n        \"CUDA is not available. Make sure you are running this on a GPU node. \"\n        \"For example, run with Slurm requesting GPU:\\n\\n\"\n        \"\\tsalloc -t 0:10:00 --ntasks=1 --gres=gpu:1 python run_test_llama.py\"\n    )\n\nmodel_path = \"/scratch/network/$USER/.cache/huggingface/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n\nprint(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n\npipe = pipeline(\"text-generation\", model=model_path, tokenizer=model_path)\n\nprompt = \"You are an expert psychologist. Tell me something interesting about psychology regarding Erik Nook's research:\"\noutput = pipe(prompt, max_new_tokens=50)\n\nprint(\"\\nModel output:\\n\", output)\n\nMake sure to replace $USER in the path with your actual NetID or use a variable if running programmatically.\n\n\n\n\nThis Slurm batch script requests one A100 GPU on the Adroit cluster and runs the above Python test script.\n#!/bin/bash\n#SBATCH --job-name=llama3-textgen           # Job name\n#SBATCH --nodes=1                           # Use one node\n#SBATCH --ntasks=1                          # One task\n#SBATCH --cpus-per-task=1                   # One CPU core\n#SBATCH --mem=36G                          # Memory request # can be ess\n#SBATCH --gres=gpu:1                        # Request 1 GPU\n#SBATCH --time=00:10:00                     # Max runtime (adjust as needed)\n#SBATCH --constraint=a100                   # Use A100 GPU\n#SBATCH --nodelist=adroit-h11g1             # Run on node with free GPUs\n#SBATCH --mail-type=ALL                     # Email on start, end, fail\n#SBATCH --mail-user=sm9518@princeton.edu   # Your email\n\nmodule purge\nmodule load anaconda3/2024.6\nmodule load cudatoolkit/11.8 \n\nsource activate talkspaceMADS\n\ncd /scratch/network/$USER/python_test\n\necho \"Job started at $(date)\"\n\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\npython run_test_llama.py\n\necho \"Job completed at $(date)\"\n\nüí° Check GPU Node Usage Before Selecting a Node or GPU.\nBefore submitting your job or manually specifying a GPU node (e.g., with #SBATCH --nodelist=adroit-h11g1).\nIt‚Äôs a good idea to check which nodes and GPUs have free memory or are under low load. Otherwise, your job might be assigned to a GPU that is already fully used, causing CUDA out-of-memory errors.\n\nOn Adroit, you can use commands like these from the login node to check GPU availability:\n# Show GPU status and free GPUs per node \nshownodes -p gpu\nIf you don‚Äôt specify a node, Slurm will pick one for you, but it might not always be the best choice if GPUs on that node are busy."
  },
  {
    "objectID": "publications/analysis-of-social-media-language-reveals-the-psychological-interaction-of-three-successive-upheavals/index.html",
    "href": "publications/analysis-of-social-media-language-reveals-the-psychological-interaction-of-three-successive-upheavals/index.html",
    "title": "Analysis of social media language reveals the psychological interaction of three successive upheavals",
    "section": "",
    "text": "Abstract\nUsing social media data, the present study documents how three successive upheavals: the COVID pandemic, the Black Lives Matter (BLM) protests of 2020, and the US Supreme Court decision to overturn Roe v. Wade interacted to impact the cognitive, emotional, and social styles of people in the US. Text analyses were conducted on 45,225,895 Reddit comments from 2,451,289 users and 889,402 news headlines from four news sources. Results revealed significant shifts in language related to self-focus (e.g., first-person singular pronouns), collective-focus (e.g., first-person plural pronouns), negative emotion (anxiety and anger words), and engagement (e.g., discussion of upheaval-related topics) after each event. Language analyses captured how social justice-related upheavals (BLM, Roe v. Wade) may have affected people in different ways emotionally than those that affected them personally (COVID). The onset of COVID was related to people becoming increasingly anxious and people turned inward to focus on their personal situations. However, BLM and the overturning of Roe v. Wade aroused anger and action, as people may have looked beyond themselves to address these issues. Analysis of upheaval-related discussions captured the public‚Äôs sustained interest in BLM and COVID, whereas interest in Roe v. Wade declined relatively quickly. Shifts in discussions also showed how events interacted as people focused on only one national event at a time, with interest in other events dampening when a new event occurred. The findings underscore the dynamic nature of culturally shared events that are apparent in everyday online language use.\nCitation: Mesquiti, S., Seraj, S., Weyland, A. H., Ashokkumar, A., Boyd, R. L., Mihalcea, R., & Pennebaker, J. W. (2025). Analysis of social media language reveals the psychological interaction of three successive upheavals. Scientific Reports, 15, 5740. https://doi.org/10.1038/s41598-025-89165-z"
  },
  {
    "objectID": "publications/predicting-psychological-and-subjective-well-being-through-language-based-assessment/index.html",
    "href": "publications/predicting-psychological-and-subjective-well-being-through-language-based-assessment/index.html",
    "title": "Predicting Psychological and Subjective Well-being through Language-based Assessment",
    "section": "",
    "text": "Abstract\nWell-being is commonly defined in terms of comfort, happiness, functioning, and flourishing. Scholars distinguish between subjective well-being (i.e., perceiving life as pleasant) and psychological well-being (i.e., perceiving life as meaningful). While advances in natural language processing have enabled automated assessments of subjective well-being from language, their ability to capture psychological well-being remains underexplored. Across three studies (one preregistered), we examined how well language-based models predict self-reported subjective and psychological well-being. Participants provided verbal or written responses about their satisfaction with life and autonomy, along with standard questionnaire measures. We used contextual word embeddings from transformer-based models to predict well-being scores. Language-based predictions correlated moderately with questionnaire measures of both constructs (rs = .16‚Äì.63) and generalized across well-being domains (rs = .15‚Äì.50), though these associations were weaker than previously work (rs = .72‚Äì.85). Autonomy was consistently less predictable than satisfaction with life. Comparisons with GPT-3.5 and GPT-4 revealed that both models outperformed BERT in predicting satisfaction with life (r = .71 and .75) and modestly improved predictions of autonomy (rGPT‚Äë4 = .49). Supervised dimension projections revealed that satisfaction with life responses clustered around positive emotion and social themes, whereas autonomy responses showed more individualized linguistic patterns. These findings suggest that language-based tools are well-suited for assessing hedonic well-being but face challenges with more abstract, eudaimonic constructs. Future research should refine modeling approaches to enhance the detection of complex psychological states while striking a balance between interpretability, accuracy, and usability.\nCitation: Mesquiti, S., Cosme, D., Nook, E. C., Falk, E. B., & Burns, S. (2025). Predicting psychological and subjective well-being through language-based assessment. Preprint."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steven Mesquiti",
    "section": "",
    "text": "My name is Steven Mesquiti! I‚Äôm a second-year PhD Student in the Department of Psychology at Princeton University advised by Erik Nook.\nI study how we can use Natural Language Processing techniques and Artificial Intelligence to better understand people‚Äôs mental health.\nBefore Princeton, I worked as a Lab Manager for Emily Falk in the Communication Neuroscience Lab at the University of Pennsylvania. Along the way, I have worked with excessively generous scientists like Jamie Pennebaker to answer questions at the nexus of Computer Science, Computational Linguistics, and Psychology."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Steven Mesquiti",
    "section": "üß† Research Interests",
    "text": "üß† Research Interests\n\nNatural Language Processing\nMental Health\nLanguage-based Assessments of Mental Health\nArtificial Intelligence"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Steven Mesquiti",
    "section": "üéì Education",
    "text": "üéì Education\n\n\n\n\n\n\n\n\n\nDegree\nField\nYear(s)\nInstitution\n\n\n\n\nPh.D.\nPsychology\n2024‚ÄìPresent\nPrinceton University\n\n\nM.A.\nPsychological Research\n2019‚Äì2022\nTexas State University\n\n\nB.A.\nPsychology (Minor: Spanish)\n2015‚Äì2019\nSouthwestern University"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Steven Mesquiti",
    "section": "",
    "text": "My name is Steven Mesquiti! I‚Äôm a second-year PhD Student in the Department of Psychology at Princeton University advised by Erik Nook.\nI study how we can use Natural Language Processing techniques and Artificial Intelligence to better understand people‚Äôs mental health.\nBefore Princeton, I worked as a Lab Manager for Emily Falk in the Communication Neuroscience Lab at the University of Pennsylvania. Along the way, I have worked with excessively generous scientists like Jamie Pennebaker to answer questions at the nexus of Computer Science, Computational Linguistics, and Psychology."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Steven Mesquiti",
    "section": "üß† Research Interests",
    "text": "üß† Research Interests\n\nNatural Language Processing\nMental Health\nLanguage-based Assessments of Mental Health\nArtificial Intelligence"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Steven Mesquiti",
    "section": "üéì Education",
    "text": "üéì Education\n\n\n\n\n\n\n\n\n\nDegree\nField\nYear(s)\nInstitution\n\n\n\n\nPh.D.\nPsychology\n2024‚ÄìPresent\nPrinceton University\n\n\nM.A.\nPsychological Research\n2019‚Äì2022\nTexas State University\n\n\nB.A.\nPsychology (Minor: Spanish)\n2015‚Äì2019\nSouthwestern University"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLLM to HPC Tutorial\n\n\n\n\n\n\nCoding\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nData-Driven Approaches to Building Equitable Language-Based Mental Health Assessments\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nFederal research funding cuts will stall scientific progress, hurt Texas students\n\n\n\n\n\n\nOpinion\n\n\nScience Policy\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nI-Projection Results\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nWORDLE Lab Manual\n\n\nWelcome to WORDLE Lab!\n\n\n\n\n\n\n\n\nDec 16, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "coding/correlation/Correlations.html",
    "href": "coding/correlation/Correlations.html",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/correlation\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD.\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, zoo, rlang,caret, broom, kableExtra, reactable, Hmisc, datarium, car,corrplot, plotrix, DT,install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion).\n\n\n\nThis is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())\n\n\n\n\nUsing stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n#summary stats function \n\nmystats_df &lt;- function(df, na.omit=FALSE) {\n  if (na.omit) {\n    df &lt;- df[complete.cases(df), ]\n  }\n  \n  stats_df &lt;- data.frame(\n    n = rep(NA, ncol(df)),\n    mean = rep(NA, ncol(df)),\n    stdev = rep(NA, ncol(df)),\n    skew = rep(NA, ncol(df)),\n    kurtosis = rep(NA, ncol(df))\n  )\n  \n  for (i in seq_along(df)) {\n    x &lt;- df[[i]]\n    m &lt;- mean(x)\n    n &lt;- length(x)\n    s &lt;- sd(x)\n    skew &lt;- sum((x-m)^3/s^3)/n\n    kurt &lt;- sum((x-m)^4/s^4)/n - 3\n    stats_df[i, ] &lt;- c(n, m, s, skew, kurt)\n  }\n  \n  row.names(stats_df) &lt;- colnames(df)\n  return(stats_df)\n}\n\n\n# correlation table function \n\napply_if &lt;- function(mat, p, f) {\n  # Fill NA with FALSE\n  p[is.na(p)] &lt;- FALSE\n  mat[p] &lt;- f(mat[p])\n  mat\n}\n\ncorr_table &lt;- function(mat, corrtype = \"pearson\") {\n  matCorr &lt;- mat\n  if (class(matCorr) != \"rcorr\") {\n    matCorr &lt;- rcorr(mat, type = corrtype)\n  }\n  \n  # Remove upper diagonal\n  matCorr$r[upper.tri(matCorr$r)] &lt;- NA\n  matCorr$P[upper.tri(matCorr$P)] &lt;- NA\n\n  # Add one star for each p &lt; 0.05, 0.01, 0.001\n  stars &lt;- apply_if(round(matCorr$r, 2), matCorr$P &lt; 0.05, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.01, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.001, function(x) paste0(x, \"*\"))\n  \n  # Put - on diagonal and blank on upper diagonal\n  stars[upper.tri(stars, diag = T)] &lt;- \"-\"\n  stars[upper.tri(stars, diag = F)] &lt;- \"\"\n  n &lt;- length(stars[1,])\n  colnames(stars) &lt;- 1:n\n  # Remove _ and convert to title case\n  row.names(stars) &lt;- tools::toTitleCase(sapply(row.names(stars), gsub, pattern=\"_\", replacement = \" \"))\n  # Add index number to row names\n  row.names(stars) &lt;- paste(paste0(1:n,\".\"), row.names(stars))\n  kable(stars) %&gt;% \n    kableExtra::kable_styling()\n}\n\n\n\n\nSince we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the mtcars dataset which is built into R and we are going to call it df.\n\n# Load the data\ndata(\"mtcars\")"
  },
  {
    "objectID": "coding/correlation/Correlations.html#set-working-directory",
    "href": "coding/correlation/Correlations.html#set-working-directory",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/correlation\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD."
  },
  {
    "objectID": "coding/correlation/Correlations.html#load-packages",
    "href": "coding/correlation/Correlations.html#load-packages",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "if (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, zoo, rlang,caret, broom, kableExtra, reactable, Hmisc, datarium, car,corrplot, plotrix, DT,install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion)."
  },
  {
    "objectID": "coding/correlation/Correlations.html#get-our-plot-aesthetics-set-up",
    "href": "coding/correlation/Correlations.html#get-our-plot-aesthetics-set-up",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "This is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "coding/correlation/Correlations.html#build-relevant-functions",
    "href": "coding/correlation/Correlations.html#build-relevant-functions",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "Using stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n#summary stats function \n\nmystats_df &lt;- function(df, na.omit=FALSE) {\n  if (na.omit) {\n    df &lt;- df[complete.cases(df), ]\n  }\n  \n  stats_df &lt;- data.frame(\n    n = rep(NA, ncol(df)),\n    mean = rep(NA, ncol(df)),\n    stdev = rep(NA, ncol(df)),\n    skew = rep(NA, ncol(df)),\n    kurtosis = rep(NA, ncol(df))\n  )\n  \n  for (i in seq_along(df)) {\n    x &lt;- df[[i]]\n    m &lt;- mean(x)\n    n &lt;- length(x)\n    s &lt;- sd(x)\n    skew &lt;- sum((x-m)^3/s^3)/n\n    kurt &lt;- sum((x-m)^4/s^4)/n - 3\n    stats_df[i, ] &lt;- c(n, m, s, skew, kurt)\n  }\n  \n  row.names(stats_df) &lt;- colnames(df)\n  return(stats_df)\n}\n\n\n# correlation table function \n\napply_if &lt;- function(mat, p, f) {\n  # Fill NA with FALSE\n  p[is.na(p)] &lt;- FALSE\n  mat[p] &lt;- f(mat[p])\n  mat\n}\n\ncorr_table &lt;- function(mat, corrtype = \"pearson\") {\n  matCorr &lt;- mat\n  if (class(matCorr) != \"rcorr\") {\n    matCorr &lt;- rcorr(mat, type = corrtype)\n  }\n  \n  # Remove upper diagonal\n  matCorr$r[upper.tri(matCorr$r)] &lt;- NA\n  matCorr$P[upper.tri(matCorr$P)] &lt;- NA\n\n  # Add one star for each p &lt; 0.05, 0.01, 0.001\n  stars &lt;- apply_if(round(matCorr$r, 2), matCorr$P &lt; 0.05, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.01, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.001, function(x) paste0(x, \"*\"))\n  \n  # Put - on diagonal and blank on upper diagonal\n  stars[upper.tri(stars, diag = T)] &lt;- \"-\"\n  stars[upper.tri(stars, diag = F)] &lt;- \"\"\n  n &lt;- length(stars[1,])\n  colnames(stars) &lt;- 1:n\n  # Remove _ and convert to title case\n  row.names(stars) &lt;- tools::toTitleCase(sapply(row.names(stars), gsub, pattern=\"_\", replacement = \" \"))\n  # Add index number to row names\n  row.names(stars) &lt;- paste(paste0(1:n,\".\"), row.names(stars))\n  kable(stars) %&gt;% \n    kableExtra::kable_styling()\n}"
  },
  {
    "objectID": "coding/correlation/Correlations.html#load-data",
    "href": "coding/correlation/Correlations.html#load-data",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "Since we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the mtcars dataset which is built into R and we are going to call it df.\n\n# Load the data\ndata(\"mtcars\")"
  },
  {
    "objectID": "coding/correlation/Correlations.html#some-other-things",
    "href": "coding/correlation/Correlations.html#some-other-things",
    "title": "Simple Correlation examples for students",
    "section": "Some other things",
    "text": "Some other things\n\nSimple Correlation tests can only be calculated between continuous variables. However, there are other types of correlations tests that can be used to deal with different data types (that‚Äôs outside the scope of this tutorial).\nSpurious correlations exist (i.e., correlations != causation)! Just because something appears to be related, due to it‚Äôs correlation coefficient, doesn‚Äôt mean there‚Äôs actually a relationship there. For example, consumption of ice cream and boating accidents are often related. However, does eating more ice cream reallyyyy lead to people having boat accidents? Think about it. Also, if we are going to infer causation we have to manipulate variables experimentally. We often do not do that in studies where we use correlational analyses.\nYou should become familiar with how to interpret correlation coefficients (esp within your specific field). That is, what is a small, medium, and large correlation? At what point is a correlation coefficient too large (e.g., are you measuring the same construct)? Here‚Äôs an article that may help!"
  },
  {
    "objectID": "coding/correlation/Correlations.html#statistical-assumptions",
    "href": "coding/correlation/Correlations.html#statistical-assumptions",
    "title": "Simple Correlation examples for students",
    "section": "Statistical Assumptions",
    "text": "Statistical Assumptions\nAssumption are also important. That is, data need to possess certain qualities for us to be able to use this type of test. For a t-test these are:\n\nThe data are continuous (not ordinal or nominal).\nData from both variables follow normal distributions.\nYour data have no outliers.\nYour data is from a random or representative sample.\nYou expect a linear relationship between the two variables.\n\nClick through the tabs to see how to check each assumption.\n\nContinuous\nWe can check this by looking at the structure of our data using the str function (for all the variables in our dataset). We can see what variables R is treating as continuous and move forward with our analyses!\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\nRandomly Sampled\nThis is something you do when you design the study‚Äìwe can‚Äôt do anything in R to check this.\n\n\nNo outliers\nWe can use the QQ-plot to inspect for outliers, which is in the ggpubr package. To do this we are going to utilize R‚Äôs ability to write functions and for loops. First, we grab all of the names of the variables we want to get qq-plots for using vars &lt;- colnames(mtcars) and save them as a list in R. This will allow us to specify what variables we want to graph. Depending on what datset we are working with, that can be as few as 2 or as many as the entire dataset! Second, we write our qqplot_all function which allows us to write the same graph as many times as we want without having to write out graphing code every. single. time. This is especially useful when graphs don‚Äôt need unique customizations. Next, we write out for loop which allows us to use the qqplot_all function for each of the 11 graphs and save them as 11 unique objects named qq_var[i]. We then arrange all 11 using ggarrange so we can take a look. Lastly, we use Markdown‚Äôs customizability to specify how large (or small) we want out figure to be. Here we go with a 10 x 10 figure. We can see that the variable don‚Äôt have any observations &gt; abs(3) and therefore no outliers.\n\nvars &lt;- colnames(mtcars)\n\nqqplot_all &lt;- function(data) {\n  vars &lt;- names(data)\n  n_vars &lt;- length(vars)\n  \n  for(i in 1:n_vars) {\n    qqplot &lt;- ggqqplot(data[[i]], ylab = vars[i],color = \"dodgerblue\") + plot_aes\n    assign(paste0(\"qq_\", vars[i]), qqplot, envir = .GlobalEnv)\n  }\n}\n\nqqplot_all(mtcars) # create QQ plots for all variables\n\nggarrange(qq_am,qq_carb,qq_cyl,qq_disp,qq_drat,qq_gear,qq_hp,qq_mpg,qq_qsec,qq_vs,qq_wt, common.legend = T, legend = 'right')\n\n\n\n\n\n\n\n\n\n\nNormal Distribution\nTo check the distribution of the data we can use density plots in the ggplot within tidyverse to visualize this. It‚Äôs also important to get some statistics behind this, and to do that we can look at skewness and kurtosis via the mystats function that we wrote earlier. You can also use psych::describe to get similar information. For skewness and kurtosis, we want values of skewness fall between ‚àí 3 and + 3, and kurtosis is appropriate from a range of ‚àí 10 to + 10.\nFor this example we are also going to visualize all of the variables in the dataset! To do this we are going to agian utilize R‚Äôs ability to write functions and for loops. First, we grab all of the names of the variables we want to get density plots for using vars &lt;- colnames(mtcars) and save them as a list in R. Second, we write our density function which allows us to write the same graph as many times as we want without having to write out graphing code every. single. time. Next, we write out for loop which allows us to use the density function for each of the 11 graphs and save them as 11 unique objects named d[i]. We then arrange all 11 using ggarrange so we can take a look. Lastly, we use Markdown‚Äôs customizability to specify how large (or small) we want out figure to be.\n\n#names &lt;- select(data,14:130) #get names of certain variables \n#names &lt;- colnames(df) #if you wanna graph ALL the variables\n\nvars &lt;- colnames(mtcars)\n\n#loop to create density plots \n\ndensity &lt;- function(data, x, y){ #create graphing function \nggplot(data = data) +\n  geom_density(aes_string(x = vars[i]),\n               adjust = 1.5, \n               alpha = 0.5, fill = \"dodgerblue\") + plot_aes\n}\n\nfor(i in 1:11) { #loop use graphing function 11 times\n  nam &lt;- paste(\"d\", i, sep = \"\")\n  assign(nam, density(mtcars,vars[i]))\n}\nggarrange(d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11, common.legend = T, legend = 'right')\n\n\n\n\n\n\n\n\n\n\nYou expect a linear relationship between the two variables.\nTo check this assumption you can plot a scatter and plot between two variables and plot a line of best fit using ggplot. Since we have a bunch of variables in our dataset that we might be interested in, we are only ging to do a few for simplicity‚Äôs sake (weight and MPG).\n\nggplot(data = mtcars, aes(x = mpg, y = wt, color = cyl)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(x = \"Miles/(US) gallon\", y = \"Weight (1000 lbs)\") +\n  plot_aes"
  },
  {
    "objectID": "coding/correlation/Correlations.html#sample-size-means-standard-deviations-etc.",
    "href": "coding/correlation/Correlations.html#sample-size-means-standard-deviations-etc.",
    "title": "Simple Correlation examples for students",
    "section": "Sample Size, Means, Standard Deviations, etc.!",
    "text": "Sample Size, Means, Standard Deviations, etc.!\n\nmtcars %&gt;%\n  get_summary_stats(type = \"full\") %&gt;% \n  DT::datatable()"
  },
  {
    "objectID": "coding/correlation/Correlations.html#get-summary-stats-for-entire-dataframe",
    "href": "coding/correlation/Correlations.html#get-summary-stats-for-entire-dataframe",
    "title": "Simple Correlation examples for students",
    "section": "Get summary stats for entire dataframe",
    "text": "Get summary stats for entire dataframe\nAlso can grab things like skewness and kurtosis!\n\nmystats_df(mtcars)\n\n      n     mean    stdev    skew kurtosis\nmpg  32  20.0906   6.0269  0.6107 -0.37277\ncyl  32   6.1875   1.7859 -0.1746 -1.76212\ndisp 32 230.7219 123.9387  0.3817 -1.20721\nhp   32 146.6875  68.5629  0.7260 -0.13555\ndrat 32   3.5966   0.5347  0.2659 -0.71470\nwt   32   3.2172   0.9785  0.4231 -0.02271\nqsec 32  17.8487   1.7869  0.3690  0.33511\nvs   32   0.4375   0.5040  0.2403 -2.00194\nam   32   0.4062   0.4990  0.3640 -1.92474\ngear 32   3.6875   0.7378  0.5289 -1.06975\ncarb 32   2.8125   1.6152  1.0509  1.25704"
  },
  {
    "objectID": "coding/correlation/Correlations.html#simple-example",
    "href": "coding/correlation/Correlations.html#simple-example",
    "title": "Simple Correlation examples for students",
    "section": "Simple example",
    "text": "Simple example\nNow, we‚Äôll conduct our correlation analyses. For simplicity, we‚Äôll again start with just two variables (weight and mpg) and then scale up.\nFrom inspecting our output we can see there is a significant, large, negative correlation between mpg and car weight [r = -0.8677, p &lt; .001].\n\nres &lt;- cor.test(mtcars$wt, mtcars$mpg, \n                    method = \"pearson\")\nres\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$wt and mtcars$mpg\nt = -9.6, df = 30, p-value = 1e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9338 -0.7441\nsample estimates:\n    cor \n-0.8677"
  },
  {
    "objectID": "coding/correlation/Correlations.html#scaling",
    "href": "coding/correlation/Correlations.html#scaling",
    "title": "Simple Correlation examples for students",
    "section": "Scaling!",
    "text": "Scaling!\nNow that we‚Äôve seen how to conduct a correlation between two variables, let‚Äôs scale it. R is flexible and can let us run a correlation between every variable in the dataset or just a select few. Since mtcars is a nice dataset to work with, we‚Äôll do the entire dataset using corr &lt;- rcorr(as.matrix(mtcars[1:11])) saving our output as an object. However, when inspect our object we get the r-values (correlation coefficients) and p-values in two separate matricies. Having these in two separate matricies is ok. However, it doesn‚Äôt do much for us in terms of trying to make sense of our data. So, let‚Äôs visualize it.\n\ncorr &lt;- rcorr(as.matrix(mtcars[1:11]))\ncorr\n\n       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\nmpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\ncyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\ndisp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\nhp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\ndrat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\nwt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\nqsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\nvs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\nam    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\ngear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\ncarb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n\nn= 32 \n\n\nP\n     mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear  \nmpg         0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054\ncyl  0.0000        0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042\ndisp 0.0000 0.0000        0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010\nhp   0.0000 0.0000 0.0000        0.0100 0.0000 0.0000 0.0000 0.1798 0.4930\ndrat 0.0000 0.0000 0.0000 0.0100        0.0000 0.6196 0.0117 0.0000 0.0000\nwt   0.0000 0.0000 0.0000 0.0000 0.0000        0.3389 0.0010 0.0000 0.0005\nqsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389        0.0000 0.2057 0.2425\nvs   0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000        0.3570 0.2579\nam   0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570        0.0000\ngear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000       \ncarb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290\n     carb  \nmpg  0.0011\ncyl  0.0019\ndisp 0.0253\nhp   0.0000\ndrat 0.6212\nwt   0.0146\nqsec 0.0000\nvs   0.0007\nam   0.7545\ngear 0.1290\ncarb"
  },
  {
    "objectID": "coding/correlation/Correlations.html#visualize-using-corrplot",
    "href": "coding/correlation/Correlations.html#visualize-using-corrplot",
    "title": "Simple Correlation examples for students",
    "section": "Visualize using corrplot",
    "text": "Visualize using corrplot\nWe can use the corrplot package to visualize our matrix. This is a great package for using shapes and colors to indicate the strength and direction of variable relationships.\nNow we‚Äôll breakdown what each part of this function does:\n\ncorr$r Allows us to feed in the correlation coefficients from our matrix\ntype=\"upper\" Specifies we want the upper diagonal for our matrix\nbg = \"white\" Set the background to white\nmethod = \"number\" Lets us use numbers as a way to visualize the results. We can also use things liek squares, circles, etc.\nnumber.cex = 15/ncol(corr)Allows for us to tinker with the size of the numbers to make them fit.\ninsig = \"blank\" Let the correlations that are p &gt; .05 be blank (or whatever threshold you specify).\n\n\n#Visualize \ncorrplot(corr$r, type=\"upper\",bg = \"white\", method = \"number\",number.cex= 15/ncol(corr),insig = \"blank\")"
  },
  {
    "objectID": "coding/correlation/Correlations.html#build-a-table",
    "href": "coding/correlation/Correlations.html#build-a-table",
    "title": "Simple Correlation examples for students",
    "section": "Build a table",
    "text": "Build a table\nWhile building a data visualiaton is cool, sometimes we want to build a professional table. We can do this using a function and kable styling to make it very pretty and presentable :). We do some by feeding our correlation objcet into our corr_table function. This not only gives us the correlation coefficients, but also the p-value thresholds they are are significant at.\n\ncorr_table(corr)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n1. Mpg\n-\n\n\n\n\n\n\n\n\n\n\n\n\n2. Cyl\n-0.85***\n-\n\n\n\n\n\n\n\n\n\n\n\n3. Disp\n-0.85***\n0.9***\n-\n\n\n\n\n\n\n\n\n\n\n4. Hp\n-0.78***\n0.83***\n0.79***\n-\n\n\n\n\n\n\n\n\n\n5. Drat\n0.68***\n-0.7***\n-0.71***\n-0.45**\n-\n\n\n\n\n\n\n\n\n6. Wt\n-0.87***\n0.78***\n0.89***\n0.66***\n-0.71***\n-\n\n\n\n\n\n\n\n7. Qsec\n0.42*\n-0.59***\n-0.43*\n-0.71***\n0.09\n-0.17\n-\n\n\n\n\n\n\n8. Vs\n0.66***\n-0.81***\n-0.71***\n-0.72***\n0.44*\n-0.55***\n0.74***\n-\n\n\n\n\n\n9. Am\n0.6***\n-0.52**\n-0.59***\n-0.24\n0.71***\n-0.69***\n-0.23\n0.17\n-\n\n\n\n\n10. Gear\n0.48**\n-0.49**\n-0.56***\n-0.13\n0.7***\n-0.58***\n-0.21\n0.21\n0.79***\n-\n\n\n\n11. Carb\n-0.55**\n0.53**\n0.39*\n0.75***\n-0.09\n0.43*\n-0.66***\n-0.57***\n0.06\n0.27\n-"
  },
  {
    "objectID": "coding/correlation/Correlations.html#load-the-data",
    "href": "coding/correlation/Correlations.html#load-the-data",
    "title": "Simple Correlation examples for students",
    "section": "Load the data",
    "text": "Load the data\n\ninaug &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Inaug_ALL_VARS.csv') #read in the data\n\ninaug &lt;- inaug %&gt;% mutate(we_i_ratio = we/i) \n\ntidy_df_Inaug&lt;- inaug %&gt;%\n group_by(year) %&gt;% ###grouping by the year \n   summarise_at(vars(\"WPS\",\"readability\",\"grade_level\",'i','we','pronoun','det','syllables_per_word','syllables_per_sentence', \"% words POS possessive\",\"% words 'of'\", \"Contractions\",\"we_i_ratio\"),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n# Get the mean values for the first year in the dataset\nyear_means &lt;- tidy_df_Inaug %&gt;%\n  filter(year == 1789)"
  },
  {
    "objectID": "coding/correlation/Correlations.html#variable-description",
    "href": "coding/correlation/Correlations.html#variable-description",
    "title": "Simple Correlation examples for students",
    "section": "Variable Description",
    "text": "Variable Description\nFlesch-Kincaid Ease of Readability: higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. Calculated using spaCy in python.\nThe Flesch‚ÄìKincaid Grade Level Score: presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts.Calculated using spaCy in python.\nI-usage: First-person singular pronoun usage (% of total words). Calculated using LIWC.\nWe-usage: First-person plural pronoun usage (% of total words). Calculated using LIWC.\nPronoun-usage: Overall pronoun usage (% of total words). Calculated using LIWC.\nPossessive-usage:First-person singular pronoun usage (% of total words). Calculated using NLTK POS, PRP, and PRP$ parser\nOf-usage: Usage of the word ‚Äòof‚Äô (% of total words). Calculated using NLTK parser.\nContraction-usage: Usage of 85 most common contractions in English (% of total words). Calculated using custom LIWC dictionary.\nDeterminers-usage: Determiner usage (% of total words). Calculated using LIWC."
  },
  {
    "objectID": "coding/correlation/Correlations.html#dates",
    "href": "coding/correlation/Correlations.html#dates",
    "title": "Simple Correlation examples for students",
    "section": "Dates",
    "text": "Dates\n\ninaug %&gt;% \n  select(year) %&gt;% \n  range()\n\n[1] 1789 2021"
  },
  {
    "objectID": "coding/correlation/Correlations.html#raw-count-of-speeches",
    "href": "coding/correlation/Correlations.html#raw-count-of-speeches",
    "title": "Simple Correlation examples for students",
    "section": "Raw count of Speeches",
    "text": "Raw count of Speeches\n\ninaug %&gt;%\n  select(Filename) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  DT::datatable()"
  },
  {
    "objectID": "coding/correlation/Correlations.html#speeches-per-year",
    "href": "coding/correlation/Correlations.html#speeches-per-year",
    "title": "Simple Correlation examples for students",
    "section": "Speeches per year",
    "text": "Speeches per year\n\narticles_year &lt;- inaug %&gt;%\n  select(Filename,year) %&gt;%\n  unique() %&gt;%\n  group_by(year) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  DT::datatable()\n articles_year"
  },
  {
    "objectID": "coding/Regression/Regression.html",
    "href": "coding/Regression/Regression.html",
    "title": "Regression",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Regression\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD.\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, broom, kableExtra, reactable, datarium, car,corrplot, DT, install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion).\n\n\n\nThis is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())\n\n\n\n\nUsing stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n table_model = function(model_data,reference = \"Intercept\") {\n   model_data %&gt;% \n     tidy() %&gt;% \n     rename(\"SE\" = std.error,\n            \"t\" = statistic,\n            \"p\" = p.value) %&gt;%\n     DT::datatable()\n   \n }\n\n\n\n\nSince we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the marketing dataset which is built into the R package datarium and we are going to call it df."
  },
  {
    "objectID": "coding/Regression/Regression.html#set-working-directory",
    "href": "coding/Regression/Regression.html#set-working-directory",
    "title": "Regression",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Regression\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD."
  },
  {
    "objectID": "coding/Regression/Regression.html#load-packages",
    "href": "coding/Regression/Regression.html#load-packages",
    "title": "Regression",
    "section": "",
    "text": "if (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, broom, kableExtra, reactable, datarium, car,corrplot, DT, install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion)."
  },
  {
    "objectID": "coding/Regression/Regression.html#get-our-plot-aesthetics-set-up",
    "href": "coding/Regression/Regression.html#get-our-plot-aesthetics-set-up",
    "title": "Regression",
    "section": "",
    "text": "This is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "coding/Regression/Regression.html#build-relevant-functions",
    "href": "coding/Regression/Regression.html#build-relevant-functions",
    "title": "Regression",
    "section": "",
    "text": "Using stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n table_model = function(model_data,reference = \"Intercept\") {\n   model_data %&gt;% \n     tidy() %&gt;% \n     rename(\"SE\" = std.error,\n            \"t\" = statistic,\n            \"p\" = p.value) %&gt;%\n     DT::datatable()\n   \n }"
  },
  {
    "objectID": "coding/Regression/Regression.html#load-data",
    "href": "coding/Regression/Regression.html#load-data",
    "title": "Regression",
    "section": "",
    "text": "Since we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the marketing dataset which is built into the R package datarium and we are going to call it df."
  },
  {
    "objectID": "coding/Regression/Regression.html#the-basics",
    "href": "coding/Regression/Regression.html#the-basics",
    "title": "Regression",
    "section": "The basics",
    "text": "The basics\nThe mathematical formula of the linear regression can be written as y = b0 + b1*x + e, where:\n\nb0 and b1 are known as the regression beta coefficients or parameters:\n\nb0 is the intercept (sometimes referred to as the constant) of the regression line; that is the predicted value when x = 0.\nb1 is the slope of the regression line.\n\ne is the error term (also known as the residual errors ‚Äì the distance between the observed data and the expected aka the line of best fit), the part of y that can be explained by the regression model\n\nThe figure below illustrates the linear regression model, where:\n\nthe best-fit regression line is in blue\nthe intercept (b0) and the slope (b1) are shown in legend\nthe error terms (e) are represented by vertical red lines extending from the regression line\n\n\n# Create some example data\nx &lt;- rnorm(100, 0, 100)\ny &lt;- rnorm(100, 0, 30)\n\n# Fit a linear regression model\nfit &lt;- lm(y ~ x)\n\n# Plot the data points\nplot(x, y, main = \"Linear Regression Example\")\n\n# Add the regression line in blue\nabline(fit, col = \"blue\")\n\n# Add the intercept and slope in green\nlegend(\"topleft\", \n       legend = paste0(\"b0 = \", round(coef(fit)[1], 2), \", b1 = \", round(coef(fit)[2], 2)), \n       col = \"blue\", lty = 1)\n\n# Add the error terms in red\ny_hat &lt;- predict(fit)\nsegments(x, y, x, y_hat, col = \"red\")\n\n\n\n\n\n\n\n\nFrom the scatter plot above, it can be seen that not all the data points fall exactly on the fitted regression line. Some of the points are above the blue curve and some are below it; overall, the residual errors (e) have approximately mean zero.\nThe sum of the squares of the residual errors are called the Residual Sum of Squares or RSS.\nThe average variation of points around the fitted regression line is called the Residual Standard Error (RSE; often just referred to as the standard error [SE]). This is one the metrics used to evaluate the overall quality of the fitted regression model. The lower the RSE, the better it is.\nSince the mean error term is zero, the outcome variable y can be approximately estimated as follow:\ny ~ b0 + b1*x\nMathematically, the beta coefficients (b0 and b1) are determined so that the RSS is as minimal as possible. This method of determining the beta coefficients is technically called least squares regression or ordinary least squares (OLS) regression.\nOnce, the beta coefficients are calculated, a t-test is performed to check whether or not these coefficients are significantly different from zero. A non-zero beta coefficients means that there is a significant relationship between the predictors (x) and the outcome variable (y)."
  },
  {
    "objectID": "coding/Regression/Regression.html#load-in-data",
    "href": "coding/Regression/Regression.html#load-in-data",
    "title": "Regression",
    "section": "Load in data",
    "text": "Load in data\n\n# Load the package\ndata(\"marketing\", package = \"datarium\")\nhead(marketing, 4)\n\n  youtube facebook newspaper sales\n1  276.12    45.36     83.04 26.52\n2   53.40    47.16     54.12 12.48\n3   20.64    55.08     83.16 11.16\n4  181.80    49.56     70.20 22.20\n\n\nFor this dataset we are interested in predicting sales on the basis of advertising budget spent on youtube."
  },
  {
    "objectID": "coding/Regression/Regression.html#data-visualization",
    "href": "coding/Regression/Regression.html#data-visualization",
    "title": "Regression",
    "section": "Data visualization",
    "text": "Data visualization\nNow that we have our research question mapped out we can visually represent our data doing the following:\n\nCreate a scatter plot displaying the sales units versus youtube advertising budget.\nAdd a smoothed line\n\n\nggplot(marketing, aes(x = youtube, y = sales)) +\n  geom_point() +\n  stat_smooth() +\n  plot_aes\n\n\n\n\n\n\n\n\nThe graph above suggests a linearly increasing relationship between the sales and the youtube variables. This is a good thing, because, one important assumption of the linear regression is that the relationship between the outcome and predictor variables is linear and additive.\nIt‚Äôs also possible to compute the correlation coefficient between the two variables using the R function cor():\n\ncor(marketing$sales, marketing$youtube)\n\n[1] 0.7822\n\n\nThe correlation coefficient measures the level of the association between two variables x and y. Its value ranges between -1 (perfect negative correlation: when x increases, y decreases) and +1 (perfect positive correlation: when x increases, y increases).\nA value closer to 0 suggests a weak relationship between the variables. A low correlation (-0.2 &lt; x &lt; 0.2) probably suggests that much of variation of the outcome variable (y) is not explained by the predictor (x). In such case, we should probably look for better predictor variables. Also want to note that the ‚Äòsize‚Äô of correlations also depends on the field you‚Äôre working in and the nature of the data. For example, a correlation of (-0.2 &lt; x &lt; 0.2) for working with language data may be considered sizable if there are a ton of observations :). Additionally, if your correlation coefficient is too large, then you start to run into issues with multicollinearlity (or measuring the same thing), but more on that later!\nIn our example, the correlation coefficient is large enough, so we can continue by building a linear model of y as a function of x."
  },
  {
    "objectID": "coding/Regression/Regression.html#interpretation",
    "href": "coding/Regression/Regression.html#interpretation",
    "title": "Regression",
    "section": "Interpretation",
    "text": "Interpretation\nFrom the output above:\n\nthe estimated regression line equation can be written as follow: sales = 8.44 + 0.048*youtube\nthe intercept (b0) is 8.44. It can be interpreted as the predicted sales unit for a zero youtube advertising budget. Recall that, we are operating in units of thousand dollars. This means that, for a youtube advertising budget equal zero, we can expect a sale of 8.44 *1000 = 8440 dollars.\nthe regression beta coefficient for the variable youtube (b1), also known as the slope, is 0.048. This means that, for a youtube advertising budget equal to 1000 dollars, we can expect an increase of 48 units (0.048 * 1000) in sales. That is, sales = 8.44 + 0.048*1000 = 56.44 units. As we are operating in units of thousand dollars, this represents a sale of 56440 dollars."
  },
  {
    "objectID": "coding/Regression/Regression.html#regression-line",
    "href": "coding/Regression/Regression.html#regression-line",
    "title": "Regression",
    "section": "Regression Line",
    "text": "Regression Line\nTo add the regression line onto the scatter plot, you can use the function stat_smooth() [ggplot2]. By default, the fitted line is presented with confidence interval around it. The confidence bands reflect the uncertainty about the line. If you don‚Äôt want to display it, specify the option se = FALSE in the function stat_smooth().\n\nggplot(marketing, aes(youtube, sales)) +\n  geom_point() +\n  stat_smooth(method = lm) + \n  plot_aes\n\n\n\n\n\n\n\n\nYou can also paste your model coefficients onto the graph too! This makes it super easy to share our results\n\nplot &lt;- ggplot(marketing, aes(youtube, sales)) +\n  geom_point() +\n  stat_smooth(method = lm) + \n  plot_aes\n\nmodel &lt;- lm(sales ~ youtube, data = marketing)\nmodel_pvalue &lt;- summary(model)$coefficients[2, 4]\n\ny_max &lt;- max(marketing$sales)\n\nplot + \n  geom_text(aes(x=0, y=y_max, label=paste(\"Youtube: \", format(coef(model)[2], digits=2), \" p = \", format(model_pvalue, digits=2))),\n            color=\"dodgerblue3\", size=5 ,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#model-summary",
    "href": "coding/Regression/Regression.html#model-summary",
    "title": "Regression",
    "section": "Model Summary",
    "text": "Model Summary\nWe can start exploring our model by displaying the statistical summary of the model using the R function summary()\nThe summary outputs shows 6 components, including:\n\nCall. Shows the function call used to compute the regression model.\nResiduals. Provide a quick view of the distribution of the residuals, which by definition have a mean zero. Therefore, the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value.\nCoefficients. Shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.\nResidual standard error (RSE), R-squared (R2) and the F-statistic are metrics that are used to check how well the model fits to our data. The most relevant here is R-squared, which tells us how much variance our model explains or how much information it can provide to our prediction!\n\n\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ youtube, data = marketing)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.06  -2.35  -0.23   2.48   8.65 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.43911    0.54941    15.4   &lt;2e-16 ***\nyoutube      0.04754    0.00269    17.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.91 on 198 degrees of freedom\nMultiple R-squared:  0.612, Adjusted R-squared:  0.61 \nF-statistic:  312 on 1 and 198 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "coding/Regression/Regression.html#coefficient-significance",
    "href": "coding/Regression/Regression.html#coefficient-significance",
    "title": "Regression",
    "section": "Coefficient significance",
    "text": "Coefficient significance\nThe coefficients table, in the model statistical summary, shows:\n\nthe estimates of the beta coefficients\nthe standard errors (SE), which defines the accuracy of beta coefficients. For a given beta coefficient, the SE reflects how the coefficient varies under repeated sampling. It can be used to compute the confidence intervals and the t-statistic.\nthe t-statistic and the associated p-value, which defines the statistical significance of the beta coefficients."
  },
  {
    "objectID": "coding/Regression/Regression.html#t-statistic-and-p-values",
    "href": "coding/Regression/Regression.html#t-statistic-and-p-values",
    "title": "Regression",
    "section": "t-statistic and p-values:",
    "text": "t-statistic and p-values:\nFor a given predictor, the t-statistic (and its associated p-value) tests whether or not there is a statistically significant relationship between a given predictor and the outcome variable, that is whether or not the beta coefficient of the predictor is significantly different from zero.\nThe statistical hypotheses are as follow:\n\nNull hypothesis (H0): the coefficients are equal to zero (i.e., no relationship between x and y)\nAlternative Hypothesis (Ha): the coefficients are not equal to zero (i.e., there is some relationship between x and y)\n\nMathematically, for a given beta coefficient (b), the t-test is computed as t = (b - 0)/SE(b), where SE(b) is the standard error of the coefficient b. The t-statistic measures the number of standard deviations that b is away from 0. Thus a large t-statistic will produce a small p-value.\nThe higher the t-statistic (and the lower the p-value), the more significant the predictor. The symbols to the right visually specifies the level of significance. The line below the table shows the definition of these symbols; one star means 0.01 &lt; p &lt; 0.05. The more the stars beside the variable‚Äôs p-value, the more significant the variable.\nA statistically significant coefficient indicates that there is an association between the predictor (x) and the outcome (y) variable.\nIn our example, both the p-values for the intercept and the predictor variable are highly significant, so we can reject the null hypothesis and accept the alternative hypothesis, which means that there is a significant association between the predictor and the outcome variables.\nThe t-statistic is a very useful guide for whether or not to include a predictor in a model. High t-statistics (which go with low p-values near 0) indicate that a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped (P. Bruce and Bruce 2017)."
  },
  {
    "objectID": "coding/Regression/Regression.html#standard-errors-and-confidence-intervals",
    "href": "coding/Regression/Regression.html#standard-errors-and-confidence-intervals",
    "title": "Regression",
    "section": "Standard errors and confidence intervals",
    "text": "Standard errors and confidence intervals\nThe standard error measures the variability/accuracy of the beta coefficients. It can be used to compute the confidence intervals of the coefficients. Frequentest approach confidence intervals should be interpreted as:\n\nConfidence Level: The first thing to understand is the confidence level associated with the interval. For example, if you have a 95% confidence interval, this means that if you were to repeat the sampling and estimation process many times, 95% of those intervals would contain the true population parameter.\nInterval Range: The interval itself is a range of values, usually expressed as ‚Äúestimate ¬± margin of error.‚Äù For example, if the estimate is 10 and the margin of error is 2, the interval would be 8 to 12. This means that we are 95% confident that the true parameter value falls within this range.\nUncertainty: It‚Äôs important to understand that the interval provides a range of plausible values, but it does not tell us the exact value of the parameter. There is always some uncertainty associated with the estimate.\nSample Size: The width of the interval depends on the sample size and the variability of the data. A larger sample size generally leads to a narrower interval, while more variability leads to a wider interval.\nRelevance: Finally, it‚Äôs important to consider the context of the problem and whether the interval is relevant for making decisions. If the interval is too wide to provide useful information, it may be necessary to collect more data or use a different statistical method. Additionally, it‚Äôs important to consider any assumptions or limitations of the statistical model used to construct the interval.\n\nFor example, the 95% confidence interval for the coefficient b1 is defined as b1 +/- 2*SE(b1), where:\n\nthe lower limits of b1 = b1 - 2SE(b1) = 0.047 - 20.00269 = 0.042\nthe upper limits of b1 = b1 + 2SE(b1) = 0.047 + 20.00269 = 0.052\n\nThat is, there is approximately a 95% chance that the interval [0.042, 0.052] will contain the true value of b1. Similarly the 95% confidence interval for b0 can be computed as b0 +/- 2*SE(b0).\nTo get these information, simply type confint(model):\n\nconfint(model)\n\n              2.5 %  97.5 %\n(Intercept) 7.35566 9.52256\nyoutube     0.04223 0.05284"
  },
  {
    "objectID": "coding/Regression/Regression.html#residual-standard-error",
    "href": "coding/Regression/Regression.html#residual-standard-error",
    "title": "Regression",
    "section": "Residual Standard Error",
    "text": "Residual Standard Error\nThe RSE (also known as the model sigma) is the residual variation, representing the average variation of the observations points around the fitted regression line. This is the standard deviation of residual errors.\nRSE provides an absolute measure of patterns in the data that can‚Äôt be explained by the model. When comparing two models, the model with the small RSE is a good indication that this model fits the best the data.\nDividing the RSE by the average value of the outcome variable will give you the prediction error rate, which should be as small as possible.\nIn our example, RSE = 3.91, meaning that the observed sales values deviate from the true regression line by approximately 3.9 units in average.\nWhether or not an RSE of 3.9 units is an acceptable prediction error is subjective and depends on the problem context. However, we can calculate the percentage error. In our data set, the mean value of sales is 16.827, and so the percentage error is 3.9/16.827 = 23%.\n\nGet the prediciton error\n\nsigma(model)*100/mean(marketing$sales)\n\n[1] 23.24"
  },
  {
    "objectID": "coding/Regression/Regression.html#r-squared-and-adjusted-r-squared",
    "href": "coding/Regression/Regression.html#r-squared-and-adjusted-r-squared",
    "title": "Regression",
    "section": "R-squared and Adjusted R-squared:",
    "text": "R-squared and Adjusted R-squared:\nThe R-squared (R2) ranges from 0 to 1 and represents the proportion of information (i.e.¬†variation) in the data that can be explained by the model. The adjusted R-squared adjusts for the degrees of freedom.\nThe R2 measures, how well the model fits the data. For a simple linear regression, R2 is the square of the Pearson correlation coefficient.\nA high value of R2 is a good indication. However, as the value of R2 tends to increase when more predictors are added in the model, such as in multiple linear regression model, you should mainly consider the adjusted R-squared, which is a penalized R2 for a higher number of predictors.\n\nAn (adjusted) R2 that is close to 1 indicates that a large proportion of the variability in the outcome has been explained by the regression model.\nA number near 0 indicates that the regression model did not explain much of the variability in the outcome."
  },
  {
    "objectID": "coding/Regression/Regression.html#f-statistic",
    "href": "coding/Regression/Regression.html#f-statistic",
    "title": "Regression",
    "section": "F-Statistic",
    "text": "F-Statistic\nThe F-statistic gives the overall significance of the model. It assess whether at least one predictor variable has a non-zero coefficient.\nIn a simple linear regression, this test is not really interesting since it just duplicates the information in given by the t-test, available in the coefficient table. In fact, the F test is identical to the square of the t test: 312.1 = (17.67)^2. This is true in any model with 1 degree of freedom.\nThe F-statistic becomes more important once we start using multiple predictors as in multiple linear regression.\nA large F-statistic will corresponds to a statistically significant p-value (p &lt; 0.05). In our example, the F-statistic equal 312.14 producing a p-value of 1.46e-42, which is highly significant."
  },
  {
    "objectID": "coding/Regression/Regression.html#load-in-the-data",
    "href": "coding/Regression/Regression.html#load-in-the-data",
    "title": "Regression",
    "section": "Load in the data",
    "text": "Load in the data\nAgain, we‚Äôll use the marketing data set [datarium package], which contains the impact of the amount of money spent on three advertising medias (youtube, facebook and newspaper) on sales.\n\ndata(\"marketing\", package = \"datarium\")\nhead(marketing, 4)\n\n  youtube facebook newspaper sales\n1  276.12    45.36     83.04 26.52\n2   53.40    47.16     54.12 12.48\n3   20.64    55.08     83.16 11.16\n4  181.80    49.56     70.20 22.20"
  },
  {
    "objectID": "coding/Regression/Regression.html#build-our-model",
    "href": "coding/Regression/Regression.html#build-our-model",
    "title": "Regression",
    "section": "Build our model",
    "text": "Build our model\nWe want to build a model for estimating sales based on the advertising budget invested in youtube, facebook and newspaper, as follow:\nsales = b0 + b1*youtube + b2*facebook + b3*newspaper\n\nmodel &lt;- lm(sales ~ youtube + facebook + newspaper, data = marketing)\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook + newspaper, data = marketing)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.59  -1.07   0.29   1.43   3.40 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.52667    0.37429    9.42   &lt;2e-16 ***\nyoutube      0.04576    0.00139   32.81   &lt;2e-16 ***\nfacebook     0.18853    0.00861   21.89   &lt;2e-16 ***\nnewspaper   -0.00104    0.00587   -0.18     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.02 on 196 degrees of freedom\nMultiple R-squared:  0.897, Adjusted R-squared:  0.896 \nF-statistic:  570 on 3 and 196 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "coding/Regression/Regression.html#interpret",
    "href": "coding/Regression/Regression.html#interpret",
    "title": "Regression",
    "section": "Interpret",
    "text": "Interpret\nIn our example, it can be seen that p-value of the F-statistic is &lt; 2.2e-16, which is highly significant. This means that, at least, one of the predictor variables is significantly related to the outcome variable.\nTo see which predictor variables are significant, you can examine the coefficients table, which shows the estimate of regression beta coefficients and the associated t-statistic p-values:\n\ntable_model(model)\n\n\n\n\n\nFor a given the predictor, the t-statistic evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.\nIt can be seen that, changing in youtube and facebook advertising budget are significantly associated to changes in sales while changes in newspaper budget is not significantly associated with sales.\nFor a given predictor variable, the coefficient (b) can be interpreted as the average effect on y of a one unit increase in predictor, holding all other predictors fixed.\nFor example, for a fixed amount of youtube and newspaper advertising budget, spending an additional 1 000 dollars on facebook advertising leads to an increase in sales by approximately 0.1885*1000 = 189 sale units, on average.\nThe youtube coefficient suggests that for every 1 000 dollars increase in youtube advertising budget, holding all other predictors constant, we can expect an increase of 0.045*1000 = 45 sales units, on average.\nWe found that newspaper is not significant in the multiple regression model. This means that, for a fixed amount of youtube and newspaper advertising budget, changes in the newspaper advertising budget will not significantly affect sales units.\nAs the newspaper variable is not significant, it is possible to remove it from the model:\n\nmodel  &lt;- lm(sales ~ youtube + facebook, data = marketing)\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook, data = marketing)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.557  -1.050   0.291   1.405   3.399 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.50532    0.35339    9.92   &lt;2e-16 ***\nyoutube      0.04575    0.00139   32.91   &lt;2e-16 ***\nfacebook     0.18799    0.00804   23.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.02 on 197 degrees of freedom\nMultiple R-squared:  0.897, Adjusted R-squared:  0.896 \nF-statistic:  860 on 2 and 197 DF,  p-value: &lt;2e-16\n\n\nFinally, our model equation can be written as follow: sales = 3.5 + 0.045*youtube + 0.187*facebook.\nThe confidence interval of the model coefficient can be extracted as follow:\n\nconfint(model)\n\n              2.5 % 97.5 %\n(Intercept) 2.80841 4.2022\nyoutube     0.04301 0.0485\nfacebook    0.17214 0.2038"
  },
  {
    "objectID": "coding/Regression/Regression.html#visualize-the-regression-model",
    "href": "coding/Regression/Regression.html#visualize-the-regression-model",
    "title": "Regression",
    "section": "Visualize the regression model",
    "text": "Visualize the regression model\n\n# Create a data frame with the actual values and predicted values\ndf &lt;- data.frame(sales = marketing$sales, predicted_sales = predict(model))\n\n# Create a scatterplot with the line of best fit\nplot &lt;- \n  ggplot(marketing, aes(x = youtube + facebook, y = sales)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = T) +\n  labs(x = \"YouTube + Facebook\", y = \"Sales\") +\n  ggtitle(\"Actual vs. Predicted Sales\") +\n  plot_aes\n\nyoutube_pvalue &lt;- summary(model)$coefficients[2, 4]\nfbook_pvalue &lt;- summary(model)$coefficients[3, 4]\n\ny_max &lt;- max(marketing$sales)\n\nplot + \n  geom_text(aes(x=0, y=y_max, label=paste(\"Youtube: \", format(coef(model)[2], digits=2), \" p = \", format(model_pvalue, digits=2))),\n            color=\"red\", size=5 ,hjust = 0) +\n  geom_text(aes(x=0, y=y_max-5, label=paste(\"Facebook: \", format(coef(model)[3], digits=2), \" p = \", format(model_pvalue, digits=2))),\n            color=\"dodgerblue2\", size=5 ,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#load-the-data",
    "href": "coding/Regression/Regression.html#load-the-data",
    "title": "Regression",
    "section": "Load the data",
    "text": "Load the data\n\ninaug &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Inaug_ALL_VARS.csv') #read in the data\n\ninaug &lt;- inaug %&gt;% mutate(we_i_ratio = we/i) \n\ntidy_df_Inaug&lt;- inaug %&gt;%\n group_by(year) %&gt;% ###grouping by the year \n   summarise_at(vars(\"WPS\",\"readability\",\"grade_level\",'i','we','pronoun','det','syllables_per_word','syllables_per_sentence', \"% words POS possessive\",\"% words 'of'\", \"Contractions\",\"we_i_ratio\"),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n# Get the mean values for the first year in the dataset\nyear_means &lt;- tidy_df_Inaug %&gt;%\n  filter(year == 1789) \n\n\ntidy_df_Inaug$i_centered &lt;- tidy_df_Inaug$i_mean - year_means$i_mean\ntidy_df_Inaug$WPS_centered &lt;- tidy_df_Inaug$WPS_mean - year_means$WPS_mean\ntidy_df_Inaug$we_centered &lt;-tidy_df_Inaug$we_mean - year_means$we_mean\ntidy_df_Inaug$pronoun_centered &lt;-tidy_df_Inaug$pronoun_mean - year_means$pronoun_mean\ntidy_df_Inaug$pos_centered &lt;-tidy_df_Inaug$`% words POS possessive_mean` - year_means$`% words POS possessive_mean`\ntidy_df_Inaug$of_centered &lt;-tidy_df_Inaug$`% words 'of'_mean`- year_means$`% words 'of'_mean`\ntidy_df_Inaug$con_centered &lt;-tidy_df_Inaug$Contractions_mean - year_means$Contractions_mean\ntidy_df_Inaug$det_centered &lt;-tidy_df_Inaug$det_mean - year_means$det_mean"
  },
  {
    "objectID": "coding/Regression/Regression.html#variable-description",
    "href": "coding/Regression/Regression.html#variable-description",
    "title": "Regression",
    "section": "Variable Description",
    "text": "Variable Description\nFlesch-Kincaid Ease of Readability: higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. Calculated using spaCy in python.\nThe Flesch‚ÄìKincaid Grade Level Score: presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts.Calculated using spaCy in python.\nI-usage: First-person singular pronoun usage (% of total words). Calculated using LIWC.\nWe-usage: First-person plural pronoun usage (% of total words). Calculated using LIWC.\nPronoun-usage: Overall pronoun usage (% of total words). Calculated using LIWC.\nPossessive-usage:First-person singular pronoun usage (% of total words). Calculated using NLTK POS, PRP, and PRP$ parser\nOf-usage: Usage of the word ‚Äòof‚Äô (% of total words). Calculated using NLTK parser.\nContraction-usage: Usage of 85 most common contractions in English (% of total words). Calculated using custom LIWC dictionary.\nDeterminers-usage: Determiner usage (% of total words). Calculated using LIWC."
  },
  {
    "objectID": "coding/Regression/Regression.html#years",
    "href": "coding/Regression/Regression.html#years",
    "title": "Regression",
    "section": "years",
    "text": "years\n\ninaug %&gt;% \n  select(year) %&gt;% \n  range()\n\n[1] 1789 2021"
  },
  {
    "objectID": "coding/Regression/Regression.html#raw-count-of-speeches",
    "href": "coding/Regression/Regression.html#raw-count-of-speeches",
    "title": "Regression",
    "section": "Raw count of Speeches",
    "text": "Raw count of Speeches\n\ninaug %&gt;%\n  select(Filename) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  DT::datatable()"
  },
  {
    "objectID": "coding/Regression/Regression.html#speeches-per-year",
    "href": "coding/Regression/Regression.html#speeches-per-year",
    "title": "Regression",
    "section": "Speeches per year",
    "text": "Speeches per year\n\ninaug %&gt;%\n  select(Filename,year) %&gt;%\n  unique() %&gt;%\n  group_by(year) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  DT::datatable()"
  },
  {
    "objectID": "coding/Regression/Regression.html#inaugural-addresses",
    "href": "coding/Regression/Regression.html#inaugural-addresses",
    "title": "Regression",
    "section": "Inaugural Addresses",
    "text": "Inaugural Addresses\n\nI-usage\n\ni_centered &lt;- lm(i_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(i_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n11.9676\n3.718\n3.219\n0.0021\n\n\nOriginal Publication year\n-0.0077\n0.002\n-3.926\n0.0002\n\n\n\n\n\n\n\n\n\nWe-usage\n\nwe_centered &lt;- lm(we_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(we_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-49.4826\n4.3154\n-11.47\n0\n\n\nOriginal Publication year\n0.0278\n0.0023\n12.29\n0\n\n\n\n\n\n\n\n\n\nPronouns\n\npronoun_centered &lt;- lm(pronoun_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(pronoun_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-30.1075\n6.7470\n-4.462\n0e+00\n\n\nOriginal Publication year\n0.0155\n0.0035\n4.369\n1e-04\n\n\n\n\n\n\n\n\n\nDeterminers\n\ndet_centered &lt;- lm(det_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(det_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n13.8294\n4.4158\n3.132\n0.0027\n\n\nOriginal Publication year\n-0.0078\n0.0023\n-3.369\n0.0014\n\n\n\n\n\n\n\n\n\nContractions\n\ncon_centered &lt;- lm(con_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(con_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-0.6582\n0.2503\n-2.629\n0.0110\n\n\nOriginal Publication year\n0.0004\n0.0001\n2.689\n0.0094\n\n\n\n\n\n\n\n\n\nPossesives\n\npos_centered &lt;- lm(pos_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(pos_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-3.2954\n0.4958\n-6.647\n0\n\n\nOriginal Publication year\n0.0018\n0.0003\n6.995\n0\n\n\n\n\n\n\n\n\n\nOf-usage\n\nof_centered &lt;- lm(of_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(of_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n20.7280\n3.5511\n5.837\n0\n\n\nOriginal Publication year\n-0.0109\n0.0019\n-5.849\n0\n\n\n\n\n\n\n\n\n\nWords per Sentence\n\nWPS_centered &lt;- lm(WPS_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(WPS_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n223.8545\n23.7224\n9.436\n0\n\n\nOriginal Publication year\n-0.1347\n0.0124\n-10.827\n0"
  },
  {
    "objectID": "coding/Regression/Regression.html#i",
    "href": "coding/Regression/Regression.html#i",
    "title": "Regression",
    "section": "I",
    "text": "I\n\ninaug_i &lt;- lm(i_centered ~ year, data = tidy_df_Inaug)\ninaug_i_pvalue &lt;- summary(inaug_i)$coefficients[2, 4]\n\ni_max &lt;- max(tidy_df_Inaug$i_mean)\n\ni + \n  geom_text(aes(x=1789, y=i_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_i)[2], digits=2), \" p = \", format(inaug_i_pvalue, digits=2))),\n            color=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#we",
    "href": "coding/Regression/Regression.html#we",
    "title": "Regression",
    "section": "We",
    "text": "We\n\ninaug_we &lt;- lm(we_centered ~ year, data = tidy_df_Inaug)\ninaug_we_pvalue &lt;- summary(inaug_we)$coefficients[2, 4]\n\nwe_max &lt;- max(tidy_df_Inaug$we_mean)\n\nwe + \n  geom_text(aes(x=1789, y=we_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_we)[2], digits=2), \" p = \", format(inaug_we_pvalue, digits=2))),\n            color=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#pronouns-1",
    "href": "coding/Regression/Regression.html#pronouns-1",
    "title": "Regression",
    "section": "Pronouns",
    "text": "Pronouns\n\ninaug_pronouns &lt;- lm(pronoun_centered ~ year, data = tidy_df_Inaug)\ninaug_pronouns_pvalue &lt;- summary(inaug_pronouns)$coefficients[2, 4]\n\npronouns_max &lt;- max(tidy_df_Inaug$pronoun_mean)\n\npronouns +\n  geom_text(aes(x=1789, y=pronouns_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_pronouns)[2], digits=2), \" p = \", format(inaug_pronouns_pvalue, digits=2))),\n            color=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#wps",
    "href": "coding/Regression/Regression.html#wps",
    "title": "Regression",
    "section": "WPS",
    "text": "WPS\n\ninaug_WPS &lt;- lm(WPS_centered ~ year, data = tidy_df_Inaug)\ninaug_WPS_pvalue &lt;- summary(inaug_WPS)$coefficients[2, 4]\n\nwps_max &lt;- max(tidy_df_Inaug$WPS_mean)\n\nWPS +\ngeom_text(aes(x=1789, y=wps_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_WPS)[2], digits=2), \" p = \", format(inaug_WPS_pvalue, digits=2))),\ncolor=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#determiners-1",
    "href": "coding/Regression/Regression.html#determiners-1",
    "title": "Regression",
    "section": "Determiners",
    "text": "Determiners\n\ninaug_determiners &lt;- lm(det_centered ~ year, data = tidy_df_Inaug)\ninaug_determiners_pvalue &lt;- summary(inaug_determiners)$coefficients[2, 4]\n\ndet_max &lt;- max(tidy_df_Inaug$det_mean)\n\ndeterminers +\ngeom_text(aes(x = 1789, y = det_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_determiners)[2], digits = 2), \" p = \", format(inaug_determiners_pvalue, digits = 2))), color = \"green4\", size = 3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#possessives",
    "href": "coding/Regression/Regression.html#possessives",
    "title": "Regression",
    "section": "Possessives",
    "text": "Possessives\n\ninaug_possessives &lt;- lm(pos_centered ~ year, data = tidy_df_Inaug)\ninaug_possessives_pvalue &lt;- summary(inaug_possessives)$coefficients[2, 4]\n\npossessives_max &lt;- max(tidy_df_Inaug$`% words POS possessive_mean`)\n\n\npossessives +\ngeom_text(aes(x = 1789, y = possessives_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_possessives)[2], digits = 2), \" p = \", format(inaug_possessives_pvalue, digits = 2))), color = \"green4\", size = 3, hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#of-usage-1",
    "href": "coding/Regression/Regression.html#of-usage-1",
    "title": "Regression",
    "section": "Of-usage",
    "text": "Of-usage\n\ninaug_of &lt;- lm(of_centered ~ year, data = tidy_df_Inaug)\ninaug_of_pvalue &lt;- summary(inaug_of)$coefficients[2, 4]\n\nof_max &lt;- max(tidy_df_Inaug$`% words 'of'_mean`)\n\nof +\ngeom_text(aes(x = 1789, y = of_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_of)[2], digits = 2), \" p = \", format(inaug_of_pvalue, digits = 2))), color = \"green4\", size = 3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#contractions-1",
    "href": "coding/Regression/Regression.html#contractions-1",
    "title": "Regression",
    "section": "Contractions",
    "text": "Contractions\n\ninaug_contractions &lt;- lm(con_centered ~ year, data = tidy_df_Inaug)\ninaug_contractions_pvalue &lt;- summary(inaug_contractions)$coefficients[2, 4]\n\ncont_max &lt;- max(tidy_df_Inaug$Contractions_mean)\n\ncontractions  +\ngeom_text(aes(x = 1789, y = cont_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_contractions)[2], digits = 2), \" p = \", format(inaug_contractions_pvalue, digits = 2))), color = \"green4\", size = 3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#all-graphs",
    "href": "coding/Regression/Regression.html#all-graphs",
    "title": "Regression",
    "section": "All Graphs",
    "text": "All Graphs\n\ntidy_smooth_graphs &lt;- ggpubr::ggarrange(i,we,pronouns, WPS,determiners, possessives, of, contractions,\n                                        ncol=4, nrow=2, common.legend = TRUE, legend = \"top\")\nannotate_figure(tidy_smooth_graphs,\n                top = text_grob(\"LIWC Variables\",  color = \"black\", face = \"bold\", size = 20),\n                 bottom = text_grob(\"Horizontal line represents Fisher Corpus\",  color = \"black\", size = 14))"
  },
  {
    "objectID": "posts/I-projections/Post.html#changes-in-meaning-over-time",
    "href": "posts/I-projections/Post.html#changes-in-meaning-over-time",
    "title": "I-Projection Results",
    "section": "Changes in Meaning over Time",
    "text": "Changes in Meaning over Time"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html",
    "href": "posts/PSY591-Final-Project/Assignment.html",
    "title": "WORDLE Lab Manual",
    "section": "",
    "text": "Welcome!\n\n\n\nWe‚Äôre thrilled that you‚Äôre joining us! This handbook is designed to orient you to the lab‚Äôs culture, practices, and infrastructures. It should give you a sense of what to expect from working with us, as well as what we expect of you.\nWe encourage a collaborative culture in the lab, and it is likely that multiple members will be working together on projects. Collaborations can also occur with researchers at other institutions. In all cases, we expect all lab members to treat collaborators, their time, and data with respect."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#roles-and-expectations",
    "href": "posts/PSY591-Final-Project/Assignment.html#roles-and-expectations",
    "title": "WORDLE Lab Manual",
    "section": "1.1 Roles and Expectations",
    "text": "1.1 Roles and Expectations\n\n1.1.1 Principal Investigator (PI)\nThe PI, Steven Mesquiti, is responsible for the overall direction of the lab and its research. The PI provides ideas and/or consultation on all the lab projects including theory development, empirical work, and grant proposals.\n\n\n1.1.2 Research Directors (RDs)\nResearch Directors are full-time post-PhD research staff who may have different roles depending on their career goals and interests. In general, RDs are here to:\n\nDo research\nSupport the major projects in the lab\nHelp write grants\nMentor and help train others in the lab\nHelp decide the greater direction that the lab goes in (both in terms of lab culture/organization and in terms of research topics/future projects)\n\n\n\n1.1.3 Post-docs\nPostdocs are full-time researchers here to learn new skills, mentor, and help train others in the lab. Postdocs share the mission of graduate students to develop their independent lines of research. This includes but is not limited to:\n\nLearning to write grants\nDesigning and implementing novel research\nAnalyzing new and existing data\nWriting manuscripts\nDeveloping mentorship and management skills required for becoming a PI ‚ú®\n\n\n\n1.1.4 Research Coordinators and Lab Managers (RCLMs)\nRCLMs are full-time staff who have undergrad degrees (and sometimes master‚Äôs, too). All lab managers are research coordinators, but not all research coordinators are lab managers.\nResearch Coordinators (RCs) support the needs of ongoing research projects. They may be assigned to play a large role on one project or many smaller roles across multiple projects. This often includes:\n\nHelping with recruitment\nData collection\nIRB management\nProject documentation\n\nLab Managers (LMs) are the subset of RCs who are also responsible for providing administrative support to the lab and all of its members. This may include but is not limited to:\n\nHelping with meeting scheduling\nDocumenting lab processes (like this handbook)\nManaging lab resources and accounts\nGenerally knowing the ins and outs of how the lab functions.\n\n\n\n1.1.5 Graduate Students\nGraduate students can have many broad goals during their time in the lab, but nearly all of them involve:\n\nCompleting the requirements of their doctoral program\nLearning how to generate and communicate evidence-based knowledge and develop an independent program of research\nHelping mentor junior scholars\n\nPh.D.¬†students might also work on external partnerships, build bridges with other labs, or engage in other activities that relate to their scholarship.\n\n\n1.1.6 Undergraduate RAs\nResearch assistants often take on a range of tasks from planning studies and conducting literature reviews to running participants and managing data. Each study has unique specific requirements but Undergraduate RAs are expected to follow the broad norms and expectations of the lab, while working with us."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#when-in-doubt-ask-the-lab-managers",
    "href": "posts/PSY591-Final-Project/Assignment.html#when-in-doubt-ask-the-lab-managers",
    "title": "WORDLE Lab Manual",
    "section": "2.1 When in Doubt, Ask the Lab Manager(s)",
    "text": "2.1 When in Doubt, Ask the Lab Manager(s)\n\n\n\n\n\n\nContact the Admin Team\n\n\n\nThe lab managers (LMs) often shuffle responsibilities based on their availability, so they have developed 2 ways to contact all of them at once:\n\nThe #ask_admin channel in Slack\nThe shared email address: stevens_future_lab@reallycoolSchool.edu (for anyone who isn‚Äôt on Slack yet)\n\nOnce you pose a question or request to either one, the relevant person will respond. They may not always know the answer, but they will often be able to direct you to someone who does!"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#complete-required-training-modules",
    "href": "posts/PSY591-Final-Project/Assignment.html#complete-required-training-modules",
    "title": "WORDLE Lab Manual",
    "section": "2.2 Complete Required Training Modules",
    "text": "2.2 Complete Required Training Modules\nSee the onboarding page to see which training modules you‚Äôre required to complete. If you need help finding the training modules or have questions about which modules to complete, reach out to one of the Lab Manager at stevens_future_lab@reallycoolSchool.edu or on #ask_admin."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#share-your-contact-information",
    "href": "posts/PSY591-Final-Project/Assignment.html#share-your-contact-information",
    "title": "WORDLE Lab Manual",
    "section": "2.3 Share Your Contact Information",
    "text": "2.3 Share Your Contact Information\nYou should have received an intake form when you joined the lab. If you have not, please contact the Lab Manager. Complete the Contact info survey and add your contact information so that other lab members can reach you."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#google-calendars",
    "href": "posts/PSY591-Final-Project/Assignment.html#google-calendars",
    "title": "WORDLE Lab Manual",
    "section": "2.4 Google Calendars",
    "text": "2.4 Google Calendars\nThe Lab uses the following Google Calendars to document our lab operations:\n\nLab Calendar - lab meetings, absences, conferences, birthdays, and lab-wide events\nSteven Mesquiti - PI-related meetings (e.g., 1:1 meetings, departmental meetings, etc.)\nUniversity Events - Running list of relevant talks, colloquia, and campus events\nProject-specific calendars - Individual calendars for major ongoing projects\nLab Social - Social events, lab outings, and informal gatherings\n\n\n2.4.1 When creating Google Calendar events:\n\nPut the Zoom link as the location for a remote or hybrid meeting, and the physical location if relevant\nAdd the relevant info or the link for relevant doc in the description\nMake the event modifiable for attendees (if needed)\nAdd attendees via their lab email (if they have one) and set reminders as needed\n\nMake sure you can see them on your Google Calendar account and that you know how to add events to the respective calendars."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#sign-into-slack",
    "href": "posts/PSY591-Final-Project/Assignment.html#sign-into-slack",
    "title": "WORDLE Lab Manual",
    "section": "2.5 Sign into Slack",
    "text": "2.5 Sign into Slack\nYou should have received an invitation to the Lab Slack. Be sure to sign into the workspace. We treat Slack like a more casual form of email and use it at a more rapid pace. You can message about anything at any time, and others will respond at their earliest convenience (preferably within 24 hours during the week)."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#lab-safety",
    "href": "posts/PSY591-Final-Project/Assignment.html#lab-safety",
    "title": "WORDLE Lab Manual",
    "section": "2.6 Lab Safety",
    "text": "2.6 Lab Safety\n\nPlease make sure that you lock and close doors to lab rooms and turn off lights whenever you leave\nWe want to ensure that property is not stolen\nDo not leave any valuables or equipment (e.g.¬†laptops, chargers, wallets, etc) unattended in unsecured or open spaces\nMoreover, please leave spaces cleaner than you found them!"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#if-you-need-help",
    "href": "posts/PSY591-Final-Project/Assignment.html#if-you-need-help",
    "title": "WORDLE Lab Manual",
    "section": "2.7 If You Need Help‚Ä¶",
    "text": "2.7 If You Need Help‚Ä¶\nWe‚Äôre here for you! The Lab Wiki likely has the answer you seek ‚Äî we recommend bookmarking this Notion page. If it doesn‚Äôt have the information you need, see the ‚ÄúImportant Contacts‚Äù and ‚ÄúWhere to Go for Help‚Äù sections of this document. You can always contact the lab manager(s) at stevens_future_lab@reallycoolSchool.edu or on the #ask-admin channel on Slack."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#reading-list",
    "href": "posts/PSY591-Final-Project/Assignment.html#reading-list",
    "title": "WORDLE Lab Manual",
    "section": "2.8 Reading List",
    "text": "2.8 Reading List\nAs you get started, it is helpful to read papers that folks on the team (and collaborators you will work with) have written recently. If you‚Äôd like more specific guidance, you can ask Steven or your senior team buddy for recommendations."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#core-values",
    "href": "posts/PSY591-Final-Project/Assignment.html#core-values",
    "title": "WORDLE Lab Manual",
    "section": "3.1 Core Values",
    "text": "3.1 Core Values\n\n3.1.1 Well-being\nHere, we value well-being both on the inside and outside, in ourselves, the lab, the communities we inhabit, and the world we live in. In our lab, we strive to work hard and to make time for ourselves and others. As part of this, we recognize that there is life outside of work, and that the experiences we have beyond our work can enrich what we do here.\nBeyond ‚Äúself-care,‚Äù we also promote ‚Äúother care‚Äù. This means building a community in which we support and care for one another. We aim to be generous to one another in domains where we excel and to lean on each other as we work on areas where we are still learning; and to be there for each other as people.\nWhat this looks like:\n\nCreating norms and values that promote your well-being and that of others in the lab\nEncouraging others to prioritize their own wellbeing\n\nWhat this sounds like:\n\n‚ÄúHey Steven, I see that you‚Äôve got a lot of things you‚Äôre working on. Are you stretching yourself a little too thin and is there anything I can take off your plate?‚Äù\n\n\n‚ÄúHey Steven, I am feeling a bit overwhelmed with my responsibilities for the week. Do you have bandwidth to help me out on ____? Or is it okay if I get this to you at a later date?‚Äù\n\n\n\n3.1.2 Joy\nWe use joy as one guide to direct our work. Additionally, our lab loves to celebrate its members‚Äô accomplishments, no matter the size. Be sure to let your labmates know of any successes ‚Äî such as sharing a plot you‚Äôre proud of, making a project milestone, learning a new skill, or facilitating a meaningful mentorship moment ‚Äî so that we can celebrate your successes with you.\nSuccesses are self-defined and come in various shapes and sizes. We seek to recognize these wins as a means to acknowledge progress, and cultivate and sustain energy and motivation. We also celebrate failure as a learning opportunity. We also love hearing about the joys you are experiencing outside of work, whether that‚Äôs a really fun experience with others, a book that moved you, or a picture of your dog.\nWhen a project or task no longer brings you joy, communicate your needs to find ways to work for you (e.g., taking a pause or finding a work buddy to set intentions and check-in/co-work virtually or in-person). Remember, we are still people doing science.\nWhat this looks like:\n\nRealizing you really like working with someone or on something and ‚Äòleaning into it‚Äô\nSharing mini-wins with the lab during planning, a project team meeting, or on slack\nTaking someone for coffee to celebrate a conference presentation\n\nWhat this sounds like:\n\n‚ÄúI received some strong comments in my R&R and I wanted to share with the lab!‚Äù\n\n\n‚ÄúI really love working on this project with Jamie, is there any way I can work with her more?‚Äù\n\n\n‚ÄúCongratulations to Mia! She gave a great talk in class the other day, and I thought others would be interested to hear about it!‚Äù\n\n\n‚ÄúNice work troubleshooting that code you were stuck on, that‚Äôs progress!‚Äù\n\n\n\n3.1.3 Respect\nIn the Lab, we have respect for others‚Äô ideas, perspectives, needs, boundaries, health, time, and well-being. This is crucial for the development of ideas and the support of others. There is no such thing as a ‚Äòstupid question‚Äô. If something isn‚Äôt immediately clear to you, odds are it isn‚Äôt to others as well, so please ask!\nWhen requesting things (e.g., rec letters) please be mindful of the turnaround time.\nWhat this looks like:\n\n‚ÄòSchedule‚Äô send messages/emails for non-urgent items on weekends, holidays, etc.\nAsking questions that help to enhance an idea rather than tear someone down\nAsking questions about what a person needs or what boundaries (e.g., time constraints) they might have when collaborating\nWe can also decenter ourselves when asking questions. Example: ‚ÄúOne may be curious about how point 2 has implications for clinical practice.‚Äù\n\nWhat this sounds like:\n\n‚ÄúHey Ryan, could you please actually clarify point 2? I think it would really enhance your idea. Here are some specific ways to consider that may help clarify X,Y,Z‚Äù\n\n\n‚ÄúErik (this is a queued message), when you get a chance could you please send me X? No rush at all.‚Äù\n\n\n‚ÄúEmily, I know you have a lot in your queue, when do you need me to send you my SAS submission so that you have enough time to review it before the deadline?‚Äù\n\n\n\n3.1.4 Community Building\nAcademia often promotes a lifestyle of hyper-competitiveness and work-life imbalance, which can lead to decreased well-being and increases in psychopathology. It also stifles creativity and can undermine the very insights we hope to nourish and produce as a research group.\n\nYou belong here. You are enough. You are valued.\n\nWe seek to cultivate an environment in which these statements are embodied.\nWhat this looks like:\n\nCreating systems and standards that promote inclusion, connection, and belonging\nDeveloping norms that foster and support collaboration\nBuilding a lab culture where people feel included and supported\n\nWhat this sounds like:\n\n‚ÄúHey everyone, I‚Äôm going to eat lunch at Sally Frank Cafe! Anyone and everyone is welcome to join.‚Äù\n\n\n‚ÄúDoes anybody want to do a co-working session this week?‚Äù\n\n\n‚ÄúThe Talkspace working group is meeting today. Feel free to join!‚Äù\n\n\n‚ÄúI‚Äôve got an idea for a new project and would love to talk about potential opportunities for collaboration.‚Äù\n\n\n\n3.1.5 Psychological Safety\nWe do our best to be open-minded and welcome differing opinions. It‚Äôs okay to be wrong, change your mind, make mistakes; we are all human and this is necessary for learning and growing. Conflict and disagreement will naturally arise when working in groups and can be generative when dealt with in productive ways.\nWe are committed to working through conflict in ways that respect each other, give grace to each other, and minimize harm. Together, we can work to maintain a safe environment in which everyone feels like they belong, are valued, and can make the world a more inclusive place.\nWe can be mindful of peoples‚Äô different communication styles (e.g., eye contact and seating).\nWhat this looks like:\n\nBeing courageous and presenting an idea that is not fully developed in front of the lab or asking for help\nSpeaking out when you disagree with something, and having a candid and respectful conversation about it either in the moment or at a later time\nIf you feel less comfortable directly expressing a critique, you can offer the question or feedback from a more distanced perspective\nSome people may need to close their eyes or not maintain eye contact to be able to process verbal information\nSitting during meetings may differ (side by side, across from one another, needing to move or stand)\n\nWhat this sounds like:\n\n‚ÄúSteven, actually I disagree with that point. I think we should really emphasize this instead.‚Äù\n\n\n‚ÄúSteven, when you interrupted me, it made me feel like my perspective didn‚Äôt matter, and I‚Äôd appreciate it if you waited until I finished speaking in the future.‚Äù\n\n\n‚ÄúSarah, you‚Äôre right, I conducted the wrong analysis; let me go back and change it.‚Äù\n\n\n\n3.1.6 Communication\nYou won‚Äôt always be able to do everything you think you will‚Äîthat‚Äôs okay, that‚Äôs normal. Do your best to communicate that to your labmates so they‚Äôre aware, and can get you unstuck if needed. Additionally, use lab, project, or team meetings to communicate to others what‚Äôs on your plate, what your priorities are, and where you might need support from them.\nWhat this looks like:\n\nLetting people know when you‚Äôre taking on too much\nNot being afraid to reach out to someone if you don‚Äôt know how to do something\nAs team members, we can also incorporate questions into our meetings that ask people what they need\n\nWhat this sounds like:\n\n‚ÄúHey Dani, can I ask you a stats question?‚Äù\n\n\n‚ÄúActually Steven, I‚Äôm not sure I‚Äôll be able to do that task in the time allotted since I have X going on. I could trade it out for Y task.‚Äù\n\n\n‚ÄúYou mentioned you were unsure how to proceed with the multilevel model. Tell me more about what you need to move forward.‚Äù\n\n\n\n3.1.7 Transparency\nWe are ultimately here to grow and learn. Lab members are encouraged to support and promote each other‚Äôs intellectual and career aspirations and to encourage each other to thrive in our personal lives. Our lab maintains a collection of career development resources in the Career Development section of the Lab Wiki.\nScholarship advances when researchers work with integrity to ensure their work is transparent and reproducible. For quantitative research, the Lab operates as a part of the open research community by:\n\nReporting our research choices in clear and transparent ways via pre-registration\nAcknowledging aspects of our work that are exploratory versus confirmatory\nEngaging in open science best practices\nInternal replication\n\nAdditionally, academia is full of unspoken norms (i.e.¬†the ‚Äúhidden curriculum‚Äù) that perpetuate systemic barriers against marginalized peoples. We try our best to address these disparities by communicating and documenting these norms, and sharing them with early career researchers both within and outside our lab.\n\n\n3.1.8 Process Orientation\nThe research process is as important as the final product. For this reason, we document our work and follow the available protocols or guidelines during each step of the research process, while continuously experimenting and improving upon them.\nWe encourage lab members to focus not only on the quality of the outcome, but on the processes that lead to it! If you tried something new that worked for your research, or that did not work, share your knowledge and add it to the research guides on the Lab Notion.\n\n\n3.1.9 Curiosity and Intellectual Development\nLab members are encouraged to openly ask questions and strive for intellectual growth. We try to be curious not only about the world around us but also about what‚Äôs right in front of us. Our lab uses various methods from Psychology, Computer Science, Natural Language Processing, and Information Theory to investigate a diverse set of questions. Even if the person down the hall has interests that diverge from our own, we cultivate curiosity about the phenomena they study, the methods they employ, and the insights about which they are passionate.\nWhat this looks like:\n\nListening actively\nAsking clarifying questions that may improve an idea, even if they are ‚Äòhard‚Äô\n\nWhat this sounds like:\n\n‚ÄúI would love to hear more!‚Äù\n\n\n‚ÄúWhat do you mean by that, Steven?‚Äù\n\n\n‚ÄúWhat I hear you saying is‚Ä¶ Am I right?‚Äù\n\n\n\n3.1.10 Equity\nOur lab is committed to continuously building a community and environment where members of all identities and backgrounds can thrive. We seek to promote greater equity in the academy and beyond, and to dismantle systems of oppression in society. We seek to support equity in how we do our work, and increase equity in society as a result of our work.\nIt‚Äôs our job to recognize the privileges and resources that we have access to that are unique and special and to critically use them in a responsible manner to contribute to a broader range for others to have access to.\nWhat this looks like:\n\nCreating standards that promote opportunities for others within and outside the lab\nMentoring students from backgrounds that are underrepresented in academia to increase their opportunities\nPeople have unique needs and goals that may require different forms of support, which require a network of support in and outside of the lab\n\nWhat this sounds like:\n\n‚ÄúHey, did you see this continuing ed opportunity?‚Äù\n\n\n‚ÄúI really think we should take X REU student to increase their odds of being exposed to research at a R1 university.‚Äù\n\n\n\n3.1.11 Diversity & Inclusivity\nBeing a group with a rich and diverse set of identities, perspectives, backgrounds, and experiences is a strength. It enriches our lab culture and our research.\nAcademia has long failed to provide an equitable or inclusive environment, specifically for Black, Indigenous, and people of color (BIPOC), first generation college and low income (FGLI), and LGBTQIA+ communities, who face varying forms of racism, discrimination, and other structural barriers. Within social and biomedical sciences, participant samples are also skewed toward WEIRD (White, Educated, Industrialized, Rich, and Democratic) samples, which limits the generalizability of our work.\nWe are committed to supporting members of underrepresented and marginalized groups in science and to being active allies to members of groups we are not a part of. We further aim to‚Äîwhen possible‚Äîfocus on recruiting representative and inclusive participant samples in our research. All members of the Lab are expected to behave in a manner that promotes an inclusive environment for all."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#presence-in-the-lab",
    "href": "posts/PSY591-Final-Project/Assignment.html#presence-in-the-lab",
    "title": "WORDLE Lab Manual",
    "section": "4.1 Presence in the Lab",
    "text": "4.1 Presence in the Lab\nWe expect lab members to attend (either virtually or in-person) lab-wide meetings and strongly encourage everyone to actively participate in lab activities. Additionally, graduate students and staff are strongly encouraged to attend relevant colloquia.\nThe Lab Manager is available from approximately 9:00 AM to 5:00 PM on weekdays, so being around when they‚Äôre around will help you have easier access to them. Steven also works on weekdays, and is slower to respond evenings/weekends."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#how-long-should-i-work",
    "href": "posts/PSY591-Final-Project/Assignment.html#how-long-should-i-work",
    "title": "WORDLE Lab Manual",
    "section": "4.2 How Long Should I Work?",
    "text": "4.2 How Long Should I Work?\nWe typically expect full-time staff (i.e.¬†Research Directors, Research Coordinators) to work approximately 40 hours a week Monday to Friday. The timeframe that this commitment is completed is largely contingent on the lab member, but it is important to communicate it to the rest of the lab.\nFor example, one member may choose to work 8 to 4, another 10 to 6, or someone else 9 to 5. It is important to note that your schedule may be dependent on the needs of your project."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#time-off",
    "href": "posts/PSY591-Final-Project/Assignment.html#time-off",
    "title": "WORDLE Lab Manual",
    "section": "4.3 Time Off",
    "text": "4.3 Time Off\nWe encourage vacations and days off, just communicate with Steven (or your appropriate supervisor, such as a Research Director) and the team about your plan, including any considerations for nearby deadlines and/or how ongoing tasks will be taken care of in your absence. Post your days off on your google calendar as well as the Lab calendar."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#methods-of-communication",
    "href": "posts/PSY591-Final-Project/Assignment.html#methods-of-communication",
    "title": "WORDLE Lab Manual",
    "section": "5.1 Methods of Communication",
    "text": "5.1 Methods of Communication\nWe frequently use Slack for day-to-day communication. The lab does its best to respect healthy work-life boundaries and uses unscheduled phone calls for emergencies. Timely response to email (and Slack) is expected (ideally within 24 business hours for intra-lab communications), and we do our best to be punctual for lab meetings and events.\n\n5.1.1 Email Best Practices\n\nIt can be helpful to bold important parts of the email, such as the question you are asking or the person to which a certain part is addressed to\nWhen writing times, writing out the full date and time with the time zone (e.g., Will Tuesday March 6, 11 AM EST work for you?) can help making scheduling easier and prevent miscommunication\nWhen setting up a time to meet with someone, suggesting a few times, and then ask them to suggest a time if those don‚Äôt work\n\nIf you are running late for a meeting or event, be sure to notify a fellow Lab member by email or slack. Similarly, if you cannot attend a meeting or event, please inform the planning committee or Lab members ahead of time.\n\n\n5.1.2 Ping Again!\nIf others (anyone, outside of Lab, too) takes longer than 48 hours to respond, follow up (‚ÄúHey Steven, just checking in on this! Thanks‚Äù). It‚Äôs helpful for them and for you and shouldn‚Äôt be perceived as being pushy!\n\n\n5.1.3 Responses\nYou do NOT have to respond to emails or slack messages during the weekend. If you receive them, you are welcome to reply to them at your own discretion. If something is very urgent or requires attention during the weekend, we will let you know."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#general-principles",
    "href": "posts/PSY591-Final-Project/Assignment.html#general-principles",
    "title": "WORDLE Lab Manual",
    "section": "6.1 General Principles",
    "text": "6.1 General Principles\nAuthorship is based on substantial contributions to the research, following guidelines from major journals and professional organizations. We acknowledge that the trajectory of a research project can be difficult to predict, so authorship discussions should happen early and should be revisited if anything changes.\n\n6.1.1 Criteria for Authorship\nTo qualify for authorship, individuals should make substantial contributions in at least two of the following areas:\n\nConception and design - Developing research questions, hypotheses, or study design\nData collection - Recruiting participants, programming experiments, running sessions\nData analysis - Conducting statistical analyses, creating visualizations, interpreting results\nWriting - Drafting or substantially revising the manuscript\nSecuring funding - Writing grants that support the research\n\n\n\n6.1.2 Authorship Order and Expectations\n\nFirst author: Primary contributor who typically led the project, conducted most analyses, and wrote the initial draft\nSecond/middle authors: Made substantial contributions but less extensive than the first author. Order is typically determined by relative contribution\nLast author: Typically the PI (Steven) who provided oversight, funding, and conceptual guidance\nCo-first authors: When two individuals contributed equally, this should be indicated with a footnote\n\n\n\n6.1.3 Special Considerations\n\nUndergraduate RAs: If an RA contributes substantially (e.g., leads data collection, performs analyses, writes methods), they should be considered for authorship\nLab managers/RCs: Contributing to routine lab operations alone does not warrant authorship, but substantial intellectual or analytical contributions do\nExternal collaborators: Early discussions about authorship expectations prevent misunderstandings\n\n\n\n6.1.4 Acknowledgments\nIndividuals who contributed but do not meet authorship criteria should be acknowledged. This includes:\n\nTechnical support (e.g., helping to analyze data on the cluster, prep data, etc.)\nFeedback on ideas or drafts\nFunding sources\nParticipants (collecting data)\nHelpful conversations\n\n\n\n6.1.5 Resolving Disputes\n\nProject leads should be mindful of who has contributed to the project and proactively discuss authorship\nIf disagreements arise, Steven will mediate\nDocumentation of contributions (in project logs, GitHub commits, etc.) can help clarify contributions\n\nWhen in doubt, lab members are encouraged to communicate openly with Steven or other Senior Lab Members."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#documentation",
    "href": "posts/PSY591-Final-Project/Assignment.html#documentation",
    "title": "WORDLE Lab Manual",
    "section": "7.1 Documentation",
    "text": "7.1 Documentation\nSharing knowledge between members of the lab is an important part of our work as academics. The ‚ÄúWiki‚Äù platform on Notion is an excellent way for us to capture our lab‚Äôs diverse skills and knowledge bases in a way that is fairly permanent yet easy to change.\nMost lab protocols are documented in detail in the Wiki. Check out what we have collected so far, and follow the instructions on the first page to add your contributions and help out in improving the site!\nIf you need help finding something and you can‚Äôt find it on the wiki, you can always slack the appropriate slack channels:\n\n#ask_admin: for setting up meetings with Steven or asking questions to the admin team\n#coding: for questions regarding coding\n#stats: for questions about statistics, data analyses and visualizations, etc.\n#random: miscellaneous questions\n\nPart of practicing good science is having good documentation so others may reproduce your work. When documenting processes, try not to reinvent the wheel (i.e, rewriting already effective code). If you run into an issue and are able to solve it on your own, post it on an appropriate slack channel; who knows, one day it may come in handy for someone else."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#open-science-practices",
    "href": "posts/PSY591-Final-Project/Assignment.html#open-science-practices",
    "title": "WORDLE Lab Manual",
    "section": "7.2 Open Science Practices",
    "text": "7.2 Open Science Practices\nTransparency and reproducibility are core values of our lab. We are committed to sharing research materials and adopting open science practices to advance collective knowledge and hold ourselves accountable to the scientific community.\n\n7.2.1 What Should Be Shared\nUnless there are ethical or legal barriers, the following materials should be made publicly available upon publication:\n\nStimuli - Task materials, survey items, images, videos, or audio files used in studies\nData - De-identified raw means and processed data in accessible formats (CSV, JSON, etc.). In our lab that usually means no raw text\nCode - Analysis scripts, data preprocessing pipelines, and task presentation code\nProtocols - Detailed procedures for data collection and analysis\nPre-registrations - Links to pre-registered study plans where appropriate\n\n\n\n7.2.2 Where to Share\n\nOpen Science Framework (OSF) - Primary repository for study materials, data, and preregistrations\nGitHub - Code repositories for analysis pipelines and computational tools\nJournal supplementary materials - When required by the journal\n\n\n\n7.2.3 Timeline for Sharing\n\nPre-registration - Before data collection begins (for confirmatory studies)\nPreprints - Upon submission to a journal (when journal policies allow). Please make sure to share with Steven and the Lab Manager to ensure the website is updated and the preprint get‚Äôs presss!\nFinal materials - Upon publication or acceptance of the manuscript\nCode - Should be version-controlled throughout the project and made public upon publication\n\n\n\n7.2.4 Exceptions and Special Cases\nMaterials may not be shared when:\n\nSharing would violate participant privacy (even after de-identification)\nData contains proprietary information from industry partners\nSharing is prohibited by data use agreements or IRB restrictions\nMaterials are copyrighted by third parties\n\nIn these cases, document the reasons for not sharing and provide as much information as possible (e.g., data dictionaries, summary statistics).\n\n\n7.2.5 Licensing\n\nUse permissive licenses for shared materials produced by the Lab and its members (e.g., CC-BY 4.0 for data, MIT for code)\nClearly indicate the license in README files and repositories\nRespect licenses of third-party materials we build upon (e.g., cite where appropriate)"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#preregistration",
    "href": "posts/PSY591-Final-Project/Assignment.html#preregistration",
    "title": "WORDLE Lab Manual",
    "section": "7.3 Preregistration",
    "text": "7.3 Preregistration\nPreregistration is the practice of documenting research plans before data collection or analysis begins. It helps distinguish confirmatory from exploratory research and increases transparency.\n\n7.3.1 When to Preregister\n\nHigh Encouraged for confirmatory studies - Any study designed to test pre-specified hypotheses should be preregistered\nEncouraged for exploratory studies - Even exploratory work benefits from documenting initial plans\nNot required for secondary data analysis - When analyzing existing datasets, clearly label analyses as exploratory unless there was a pre-existing analysis plan\n\n\n\n7.3.2 What to Include\nA comprehensive preregistration should include:\n\nResearch questions and hypotheses\nStudy design and sampling plan\nVariables and measures\nData collection procedures\nPlanned data exclusions\nAnalysis plan (statistical tests, software, etc.)\nInference criteria (e.g., alpha levels, Bayes factors)\n\nFor more detailed information, please see the Lab Wiki.\n\n\n7.3.3 Where to Preregister\n\nOpen Science Framework (OSF) - Preferred platform; allows embargoed and public registrations\nAsPredicted.org - Simple, quick preregistrations\nClinicalTrials.gov - For clinical trials (when applicable)\n\n\n\n7.3.4 Deviations from Preregistration\nResearch plans often need to change. This is normal and acceptable, but it is importan to document all deviations in the manuscript, distinguish between confirmatory (pre-registered) and exploratory analyses clearly in the manuscript, explain why deviations were necessary, and be transparent about changes strengthens rather than weakens the work.\n\n\n7.3.5 Timeline\nWhen should you Preregister? Typically, preregistration should occur before data collection begins (for new studies) For multi-study papers, one can preregister each study separately/"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#treatment-of-human-subjects",
    "href": "posts/PSY591-Final-Project/Assignment.html#treatment-of-human-subjects",
    "title": "WORDLE Lab Manual",
    "section": "8.1 Treatment of Human Subjects",
    "text": "8.1 Treatment of Human Subjects\nAll research involving human participants must adhere to the highest ethical standards and comply with institutional and federal regulations.\n\n8.1.1 IRB Approval\nAll studies involving human subjects require IRB approval before data collection begins. Lab members must complete required human subjects training (CITI) before conducting research. Additionally, IRB protocols must be kept current; renewals should be submitted at least 30 days before expiration. And we must submit modifications to approved protocols if we modify designs, etc.!\n\n\n8.1.2 Informed Consent\nAll articipants must provide informed consent before participating in any lab study. Consent forms must be written in clear, accessible language (typically 8th-grade reading level) and participants must be informed of their right to withdraw at any time without penalty. For online studies, consent must be documented electronically\n\n\n8.1.3 Data Privacy and Confidentiality\nParticipant data must be de-identified as soon as possible in the analysis processes (e.g., Prolific IDs). Personal identifiers (names, emails, etc.) should be stored separately from research data. Use participant IDs rather than names in all data files and documentation. Data containing identifiable information must be stored on secure, encrypted systems (e.g., the Lab Server). When sharing data publicly, ensure all identifying information has been removed\n\n\n8.1.4 Vulnerable Populations\nSpecial care must be taken when working with vulnerable populations (children, individuals with mental health conditions, etc.):\n\nAdditional protections and oversight may be required\nConsult with the IRB early in the planning process\nHave clear protocols for responding to distress or mandatory reporting situations\n\n\n\n8.1.5 Compensation\nThe Lab deems it important that participants are compensated fairly for their time. That means that payment rates should be consistent with institutional guidelines (typically $10-15/hour). Where possible, partial compensation should be provided if participants withdraw early"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#use-of-artificial-intelligence",
    "href": "posts/PSY591-Final-Project/Assignment.html#use-of-artificial-intelligence",
    "title": "WORDLE Lab Manual",
    "section": "8.2 Use of Artificial Intelligence",
    "text": "8.2 Use of Artificial Intelligence\nAI tools (e.g., ChatGPT, Claude, Copilot) are increasingly integrated into research workflows. We embrace these technologies while maintaining scientific integrity and transparency.\n\n8.2.1 Acceptable Uses\nAI tools may be used for:\n\nCode development and debugging - Using AI assistants to write, troubleshoot, or optimize code\nLiterature review support - Summarizing papers or identifying relevant literature (always verify sources)\nWriting assistance - Improving clarity, grammar, or structure of written work\nBrainstorming - Generating research ideas or hypotheses (which must be critically evaluated by Humans!)\nData preprocessing - Automating routine data cleaning tasks (with verification)\n\n\n\n8.2.2 Required Disclosure\nAI use must be disclosed when:\n\nAI tools generate substantial portions of text in manuscripts or grants\nAI is used in the analysis pipeline (e.g., using LLMs for text classification)\nAI assists in creating figures or visualizations beyond basic formatting\n\nNote. When disclosing use of a LLM, it‚Äôs important to capture 1. the model you used 2. when you used the model (i.e., the data) 3. any sort of customization you used (e.g., change in temperature, etc.)\n\n\n8.2.3 Prohibited Uses\nAI tools should NOT be used for:\n\nFabricating data or results\nGenerating fake citations - Always verify that cited papers exist and are accurately represented\nPeer review - Do not input confidential manuscript content into AI systems\nFinal decision-making without human oversight\n\nWhen in doubt about appropriate AI use, discuss with Steven"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#inclusive-research-practices",
    "href": "posts/PSY591-Final-Project/Assignment.html#inclusive-research-practices",
    "title": "WORDLE Lab Manual",
    "section": "8.3 Inclusive Research Practices",
    "text": "8.3 Inclusive Research Practices\nOur lab values feedback from members of different communities, and we incorporate this feedback into our research workflows. Below are some resources we recommend when preparing your project:\n\nInformation on how the lab attempts to collect participant demographics in an inclusive manner\nInformation on how to construct a citation diversity statement: https://github.com/dalejn/cleanBib"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#working-together-responsibilities-and-respect",
    "href": "posts/PSY591-Final-Project/Assignment.html#working-together-responsibilities-and-respect",
    "title": "WORDLE Lab Manual",
    "section": "8.4 Working Together: Responsibilities and Respect",
    "text": "8.4 Working Together: Responsibilities and Respect\n\n8.4.1 Respect for All Contributors\nResearch is a collaborative effort, and everyone‚Äîfrom administrators to undergraduates‚Äîplays a vital role. All members of the team deserve recognition and appreciation for their contributions, big and small.\nRespect for administrators: Pay it forward. Show gratitude to administrative and support staff (Finance, HR, etc.) by getting them what they need, exactly how they need it, in a timely manner (ideally within 24 hours). Acknowledge their work with explicit thanks.\n\n\n8.4.2 Appropriate Task Delegation\nAll members of the research team should be given tasks that they are approved, trained, and qualified to complete. This ensures things get done right the first time and promotes learning. If you feel like you‚Äôre taking on too much or are unsure how to do a task, speak up!"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#sexual-harassment-and-other-forms-of-harassment",
    "href": "posts/PSY591-Final-Project/Assignment.html#sexual-harassment-and-other-forms-of-harassment",
    "title": "WORDLE Lab Manual",
    "section": "10.1 Sexual Harassment and Other Forms of Harassment",
    "text": "10.1 Sexual Harassment and Other Forms of Harassment\nThe Lab does not tolerate any forms of harassment. With that in mind, it is important to discuss what harassment can look like and outline the available channels for reporting.\nThe following definitions of harassment are borrowed from the Social Affective Neuroscience Society (SANS) code of conduct:\n\n‚ÄúSexual harassment refers to unwelcome sexual advances, requests for sexual favors, and other verbal or physical conduct of a sexual nature. Behavior and language that are welcome and acceptable to one person may be unwelcome and offensive to another. Consequently, individuals must use discretion to ensure that their words and actions communicate respect for others. This is especially important for those in positions of authority since individuals with lower rank or status may be reluctant to express their objections or discomfort regarding unwelcome behavior.‚Äù"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#reporting-violations-of-community-standards",
    "href": "posts/PSY591-Final-Project/Assignment.html#reporting-violations-of-community-standards",
    "title": "WORDLE Lab Manual",
    "section": "10.2 Reporting Violations of Community Standards",
    "text": "10.2 Reporting Violations of Community Standards\nSteven and Senior Lab Members are both available to discuss any concerns that arise with respect to community standards. If you have concerns about interactions with Steven, you can consult a Senior Lab Member, or the Dean of the XXXX School/Program or one of the resources below.\nWhether those concerns involve other Lab members or individuals not directly affiliated with the lab, please know that we are here to listen and provide support to the best of our abilities. Having said that, it is important to note that Steven and Senior Lab Members are mandated reporters of Title IX violations.\n\n10.2.1 Who are Mandated Reporters?\nAll university faculty and staff who regularly work with students in a teaching, advising, coaching, or mentoring capacity are required to report instances of sexual harassment, sexual violence, relationship (dating or domestic) violence, and stalking to the Title IX Office.\nIn other words, Steven, Lab Managers, Post docs, as well as Research Directors and Coordinators are mandated reporters under Title IX.\n\n\n10.2.2 What Does This Mean?\nMandated reporters are required to notify the Title IX Office of violations. These reports are not anonymous, and the University does not guarantee that they will be kept confidential. In rare cases, the University may proceed to a Title IX investigation despite requests for confidentiality."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#general-meetings-one-on-one-or-group",
    "href": "posts/PSY591-Final-Project/Assignment.html#general-meetings-one-on-one-or-group",
    "title": "WORDLE Lab Manual",
    "section": "11.1 General Meetings (One-on-one or Group)",
    "text": "11.1 General Meetings (One-on-one or Group)\nWhen organizing meetings with other lab members or collaborators, make sure to:\n\nSchedule it on your Lab Google calendar\nSend calendar invites to guests\nInclude a zoom link and/or in-person meeting space (we recommend providing both options in case of any last-minute changes)\nCreate an agenda in advance"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#lab-meetings",
    "href": "posts/PSY591-Final-Project/Assignment.html#lab-meetings",
    "title": "WORDLE Lab Manual",
    "section": "11.2 Lab Meetings",
    "text": "11.2 Lab Meetings\nLab meetings are held weekly for an hour and a half. We use a sprint schedule for our meeting schedule. The meeting schedule for the current semester can be found on the Lab Google Calendar.\nLabscrum is an adaptation of scrum‚Äîan implementation of the agile philosophy of project management‚Äîdeveloped for academia by Lisa May. It is a framework for managing research projects and labs that uses empiricism to continuously experiment with and refine how we do research and work together.\nWhile labscrum may help us be more productive, that is not our primary goal. Our core goal is to help each other be well and do science in a sustainable way.\nThe core pillars of labscrum include:\n\nTransparency: To improve the research process, it must be observable\nInspection: Reflection enables the generation of ideas for how to improve the research process\nAdaptation: Ideas are implemented as experiments and the work process is adjusted continuously"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#individual-meetings-with-steven",
    "href": "posts/PSY591-Final-Project/Assignment.html#individual-meetings-with-steven",
    "title": "WORDLE Lab Manual",
    "section": "11.3 Individual Meetings with Steven",
    "text": "11.3 Individual Meetings with Steven\nIf you‚Äôd like to schedule a meeting with Steven, use the #ask_admin slack channel to contact the admin team. The lab manager will schedule these 1:1 meetings each semester, according to lab members‚Äô availability. You can schedule recurring meetings (e.g., every 2 weeks) or request one-time meetings as needed.\nPrepare an agenda in advance for what the meeting will be about. Although there is no set agenda template and the needs of each meeting may vary, a good starting point may be to consider the following sections:\n\nUpdates (stuff you got done since last time)\nTasks and Longer-Term Goals (an ordered priority list for what‚Äôs coming up)\nDiscussion (stuff that needs input)"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#other-intra-lab-meetings",
    "href": "posts/PSY591-Final-Project/Assignment.html#other-intra-lab-meetings",
    "title": "WORDLE Lab Manual",
    "section": "11.4 Other Intra-lab Meetings",
    "text": "11.4 Other Intra-lab Meetings\nIf you‚Äôre requesting a meeting with someone else, you set the agenda. For team meetings, the research director or some other designated person (sometimes a research coordinator) maintains the agenda. It is recommended that you schedule a meeting a few days in advance to accommodate others‚Äô schedules and allow them to prepare."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#funding-policies",
    "href": "posts/PSY591-Final-Project/Assignment.html#funding-policies",
    "title": "WORDLE Lab Manual",
    "section": "12.1 Funding Policies",
    "text": "12.1 Funding Policies\nOur lab is committed to maintaining scientific integrity and independence in all our work. This includes being thoughtful about funding sources and potential conflicts of interest.\n\n12.1.1 Acceptable Funding Sources\nWe actively pursue funding from:\n\nFederal agencies (NIH, NSF, NIDA, NIMH, etc.)\nPrivate foundations with missions aligned with public health and scientific advancement\nUniversity internal grants\nProfessional societies and associations\n\n\n\n12.1.2 Industry Partnerships\nIndustry collaborations can provide valuable resources and real-world applications for our research. However, we maintain the following guardrails when collaborating with folks from industry:\n\nScientific independence - The lab should retain full control over study design, analysis, and publication\nNo publication restrictions - Funding agreements must not restrict our ability to publish results, regardless of outcomes\nTransparency - All industry funding must be disclosed in publications and presentations\nEthical alignment - We will not accept funding from organizations whose core business conflicts with public health (e.g., tobacco companies, organizations promoting misinformation)\n\n\n\n12.1.3 Data Ownership and Sharing\n\nThe lab retains ownership of data generated with lab resources\nIndustry partners may request first access to results before public release, but timelines must be reasonable (typically 30-60 days)\nData sharing agreements must allow for eventual public sharing (after appropriate embargo periods)\n\n\n\n12.1.4 When to Decline Funding\nWe will decline funding that:\n\nRestricts publication rights or requires approval before publication\nPrevents data sharing beyond what is ethically necessary\nCreates significant conflicts of interest that cannot be managed\nComes from organizations whose values fundamentally conflict with our mission\n\n\n\n12.1.5 Discussing Funding Decisions\nIf you have concerns about a potential funding source, please raise them with Steven. These discussions are important and will be taken seriously."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#funding-opportunities",
    "href": "posts/PSY591-Final-Project/Assignment.html#funding-opportunities",
    "title": "WORDLE Lab Manual",
    "section": "12.2 Funding Opportunities",
    "text": "12.2 Funding Opportunities\nThere are multiple funding opportunities available to fund projects for researchers at different stages. For a broad (but by no means comprehensive) list of opportunities, you can take a look at the Funding Opportunities page on the lab wiki. If you see any funding opportunities pop up, feel free to share with the lab via Slack or email.\nThe lab is usually able to cover the cost of lab-wide or full project team events, such as lunches, dinners, and retreats. The lab will usually not cover expenses for individual or small-group activities. If you have any questions about the lab covering the cost of an activity, please check with the admin team, Steven, and/or the Finance Office."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#online",
    "href": "posts/PSY591-Final-Project/Assignment.html#online",
    "title": "WORDLE Lab Manual",
    "section": "13.1 Online",
    "text": "13.1 Online\n\nUse the Lab Wiki to answer most lab-related questions\nThe Wiki is a working document, so if you don‚Äôt see a page on something you think should be on there, feel free to create it!\nUse the search function! You might find something very helpful, or maybe even a similar discussion you can reference to ask someone in the lab\nIf you don‚Äôt find what you‚Äôre looking for, and figure out what you need on your own or with the help of other lab members, consider creating a page on Notion with what you‚Äôve learned"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#asking-for-help---when-and-how",
    "href": "posts/PSY591-Final-Project/Assignment.html#asking-for-help---when-and-how",
    "title": "WORDLE Lab Manual",
    "section": "13.2 Asking for Help - When and How",
    "text": "13.2 Asking for Help - When and How\nAlways ask questions‚Äìthere are no stupid questions! It is recommended that lab members try to address issues on their own first. If you aren‚Äôt able to solve the issue, that‚Äôs okay! Reach out to the relevant person (whether a lab member or university affiliate, e.g., in IT, HR, or stats consulting) and detail the problem as well as the steps you‚Äôve taken to try to solve it. This will help others provide you with relevant, tailored support and cut down on excessive back-and-forth.\nSenior lab members, when helping other Lab members, please do your best to demonstrate how you‚Äôve solved the problem in the past and share any relevant knowledge. This way, the lab member seeking help can learn new skills and be better able to problem-solve independently in the future."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#data-security",
    "href": "posts/PSY591-Final-Project/Assignment.html#data-security",
    "title": "WORDLE Lab Manual",
    "section": "14.1 Data Security",
    "text": "14.1 Data Security\n\n14.1.1 Storage\n\nSensitive data (containing identifiers) must be stored on secure, encrypted university servers\nDe-identified data can be stored on lab-approved cloud services (Box, Dropbox with encryption)\nNever store research data containing identifiers on personal devices without encryption\nUse the lab‚Äôs secure server space for active projects\n\n\n\n14.1.2 Password Management\n\nUse strong, unique passwords for all research accounts\nUse a password manager (e.g., 1Password, LastPass) - the lab can provide licenses\nEnable two-factor authentication (2FA) on all accounts that offer it\nNever share passwords via email or unencrypted channels!\n\n\n\n14.1.3 File Naming and Organization\n\nUse consistent, descriptive file naming conventions\nInclude dates in ISO format (YYYY-MM-DD) when relevant\nMaintain clear folder structures documented in README files\nVersion control code using Git/GitHub\n\n\n\n14.1.4 Access Control\n\nOnly lab members working on a specific project should have access to that project‚Äôs data\nRemove access when lab members leave or transition to different projects unbless otherwise specified\nRegularly audit who has access to sensitive data"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#data-management-plans",
    "href": "posts/PSY591-Final-Project/Assignment.html#data-management-plans",
    "title": "WORDLE Lab Manual",
    "section": "14.2 Data Management Plans",
    "text": "14.2 Data Management Plans\nEvery project should have a data management plan that includes:\n\nWhere data will be stored during and after the project\nWho has access to the data\nPlans for data sharing and archiving\nBackup procedures"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#hipaa-and-phi",
    "href": "posts/PSY591-Final-Project/Assignment.html#hipaa-and-phi",
    "title": "WORDLE Lab Manual",
    "section": "14.3 HIPAA and PHI",
    "text": "14.3 HIPAA and PHI\nIf working with Protected Health Information (PHI):\n\nComplete HIPAA training before accessing any PHI\nFollow all institutional HIPAA compliance procedures\nStore PHI only on HIPAA-compliant systems\nConsult with the IRB and institutional HIPAA office"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#incident-response",
    "href": "posts/PSY591-Final-Project/Assignment.html#incident-response",
    "title": "WORDLE Lab Manual",
    "section": "14.4 Incident Response",
    "text": "14.4 Incident Response\nIf you suspect a data breach or security incident:\n\nImmediately notify Steven and the lab manager\nDocument what happened and what data may be affected\nFollow institutional reporting procedures\nDo not attempt to ‚Äúfix‚Äù the problem before reporting it"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#what-constitutes-a-conflict-of-interest",
    "href": "posts/PSY591-Final-Project/Assignment.html#what-constitutes-a-conflict-of-interest",
    "title": "WORDLE Lab Manual",
    "section": "15.1 What Constitutes a Conflict of Interest",
    "text": "15.1 What Constitutes a Conflict of Interest\nConflicts of interest in research can take many forms, and understanding what constitutes a conflict is the first step in managing them appropriately.\n\n15.1.1 Financial Conflicts\nFinancial conflicts of interest arise when you have monetary interests that could be affected by your research outcomes. These include holding equity or financial interest in a company whose products or services are related to your research, maintaining consulting relationships with companies in your research area, receiving speaking fees or honoraria from organizations that could benefit from your research findings, or earning royalties from products related to your research. Even when these financial relationships don‚Äôt actually influence your work, they can create the appearance of bias and must be disclosed and managed appropriately.\n\n\n15.1.2 Non-Financial Conflicts\nNot all conflicts of interest involve money. Non-financial conflicts can be equally important and sometimes more difficult to identify. These include personal relationships with study participants or collaborators that could bias research outcomes, strongly held beliefs or ideological commitments that could influence research design or interpretation, and situations where your professional advancement depends on achieving specific research outcomes. For example, if your dissertation, tenure case, or promotion relies heavily on finding significant results in a particular direction, this creates a non-financial conflict that should be acknowledged and managed.\n\n\n15.1.3 Examples of Potential Conflicts\nTo make these concepts more concrete, consider the following examples: owning stock in a company that produces mental health assessment tools while conducting research that evaluates similar tools creates a clear financial conflict. Having a close family member as a study participant could bias how you collect or interpret data from that individual. Consulting for a company while publishing research relevant to their products raises questions about whether your findings might be influenced by that relationship. Similarly, serving on the board of an organization that could benefit from your research conclusions‚Äîeven if it‚Äôs a nonprofit or advocacy organization you believe in‚Äîrepresents a potential conflict that should be disclosed."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#disclosure-requirements",
    "href": "posts/PSY591-Final-Project/Assignment.html#disclosure-requirements",
    "title": "WORDLE Lab Manual",
    "section": "15.2 Disclosure Requirements",
    "text": "15.2 Disclosure Requirements\n\n15.2.1 When to Disclose\nTransparency is the cornerstone of managing conflicts of interest. Conflicts must be disclosed in all publications and presentations, ensuring that readers and audiences are aware of any potential biases. They must also be reported on grant applications, disclosed to the university‚Äôs COI office as required by institutional policy, communicated to Steven and relevant lab members who are working on related projects, and included in IRB applications when relevant to human subjects research. The goal is to ensure that all stakeholders‚Äîfrom funding agencies to research participants‚Äîcan evaluate your work with full knowledge of any potential competing interests.\n\n\n15.2.2 How to Disclose\nWhen disclosing conflicts of interest, be as specific as possible about the nature of the relationship rather than using vague language. Include dollar amounts when relevant, as the magnitude of a financial interest can affect how it should be managed. Remember to update your disclosures when circumstances change‚Äîconflicts are not static, and new relationships or changes to existing ones must be reported promptly. When in doubt, err on the side of over-disclosure rather than under-disclosure. It‚Äôs better to disclose something that turns out to be minor than to fail to disclose something that could later be seen as problematic."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#managing-conflicts-of-interest",
    "href": "posts/PSY591-Final-Project/Assignment.html#managing-conflicts-of-interest",
    "title": "WORDLE Lab Manual",
    "section": "15.3 Managing Conflicts of Interest",
    "text": "15.3 Managing Conflicts of Interest\nHaving a conflict of interest is not inherently problematic‚Äîfailing to disclose or manage it is.\n\n15.3.1 Management Strategies\n\nDisclosure - Often sufficient for minor conflicts\nRecusal - Removing yourself from decision-making about affected research\nIndependent oversight - Having others review data analysis or interpretation\nDivestment - Eliminating the conflicting financial interest when needed\nProject reassignment - Moving to a different project without the conflict\n\n\n\n15.3.2 Institutional Review\nSome conflicts require institutional review and management plans:\n\nSignificant financial interests (typically &gt;$5,000 or &gt;5% equity)\nLeadership positions in relevant organizations\nSituations where university policies require review"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#what-to-do-if-you-have-a-conflict",
    "href": "posts/PSY591-Final-Project/Assignment.html#what-to-do-if-you-have-a-conflict",
    "title": "WORDLE Lab Manual",
    "section": "15.3 What to Do If You Have a Conflict",
    "text": "15.3 What to Do If You Have a Conflict\nIf you recognize a potential conflict of interest, follow these steps: First, identify the conflict by recognizing when a potential conflict exists‚Äîbeing proactive is always better than being reactive. Second, disclose the conflict by informing Steven and following institutional disclosure procedures through your university‚Äôs COI office. Third, discuss the situation with Steven to work together to determine if management beyond disclosure is needed. Fourth, document everything by keeping records of your disclosures and any management plans you develop. Finally, monitor the situation over time and reassess as circumstances change, since conflicts can evolve as your career and relationships develop."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#questions-to-ask-yourself",
    "href": "posts/PSY591-Final-Project/Assignment.html#questions-to-ask-yourself",
    "title": "WORDLE Lab Manual",
    "section": "15.4 Questions to Ask Yourself",
    "text": "15.4 Questions to Ask Yourself\nIf you‚Äôre unsure whether something constitutes a conflict of interest, consider these questions: Would a reasonable person question my objectivity if they knew about this relationship? Could this relationship influence my research decisions, even subconsciously? Would I be comfortable if this relationship became public or was reported in the media? Am I avoiding disclosure because it feels uncomfortable or because I‚Äôm worried about how it might be perceived? If you answer ‚Äúyes‚Äù or ‚Äúmaybe‚Äù to any of these questions, it‚Äôs likely worth disclosing.\nWhen in doubt, disclose. It‚Äôs always better to be transparent! And if you have any questions you can always come and talk to Steven or a Senior Lab Member."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#initiating-collaborations",
    "href": "posts/PSY591-Final-Project/Assignment.html#initiating-collaborations",
    "title": "WORDLE Lab Manual",
    "section": "16.1 Initiating Collaborations",
    "text": "16.1 Initiating Collaborations\n\n16.1.1 Internal Collaborations\nSuccessful internal collaborations begin with clear communication. Before committing significant time to a new collaborative project, discuss your project ideas with Steven to ensure alignment with lab priorities and avoid duplication of effort. Be explicit about authorship expectations early in the process‚Äîhaving these conversations upfront prevents misunderstandings later. Document each collaborator‚Äôs roles and responsibilities clearly so everyone knows what‚Äôs expected of them, and use shared project management tools like Notion or GitHub projects to keep everyone on the same page throughout the project lifecycle.\n\n\n16.1.2 External Collaborations\nExternal collaborations can bring new perspectives and resources to our work, but they require additional planning and formal agreements. Steven should be informed of external collaboration discussions early, even in preliminary stages, so he can provide guidance and ensure the collaboration aligns with lab goals and policies. Once you decide to move forward, create a collaboration agreement that outlines project goals and timeline, clarifies each party‚Äôs contributions and responsibilities, establishes authorship expectations, specifies data ownership and sharing arrangements, outlines publication plans including timelines and review processes, and addresses any funding arrangements or cost-sharing agreements. Having these elements in writing protects everyone involved and provides a reference point if questions arise later."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#data-sharing-with-collaborators",
    "href": "posts/PSY591-Final-Project/Assignment.html#data-sharing-with-collaborators",
    "title": "WORDLE Lab Manual",
    "section": "16.2 Data Sharing with Collaborators",
    "text": "16.2 Data Sharing with Collaborators\n\n16.2.1 Before Sharing Data\nData sharing is essential for collaboration, but it must be done responsibly and legally. Before sharing any data with collaborators, ensure you have permission to share by checking your IRB protocols and consent forms to verify that data sharing is allowed. Execute data use agreements when necessary, particularly for sensitive data or when sharing with external collaborators. Share only the minimum data needed for the collaboration‚Äîthere‚Äôs no need to share entire datasets when a subset will suffice. Whenever possible, de-identify data before sharing to protect participant privacy, even when working with trusted collaborators.\n\n\n16.2.2 Data Use Agreements\nFor sensitive data or external collaborations, formal data use agreements provide important protections. These agreements should establish permitted uses of the data so collaborators understand what analyses or applications are acceptable, specify restrictions on further sharing to prevent data from being distributed beyond the agreed-upon parties, outline requirements for data security including storage and transmission protocols, detail plans for data destruction or return once the collaboration is complete, and clarify publication and authorship expectations related to work using the shared data. These agreements may seem formal, but they protect both you and your collaborators and ensure everyone has the same understanding of how data can be used."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#intellectual-property",
    "href": "posts/PSY591-Final-Project/Assignment.html#intellectual-property",
    "title": "WORDLE Lab Manual",
    "section": "16.3 Intellectual Property",
    "text": "16.3 Intellectual Property\n\n16.3.1 General Principles\nIntellectual property in academic research is primarily protected through authorship and open sharing rather than through patents or proprietary restrictions. Ideas developed using lab resources belong to the collaborative team‚Äîno single person can claim exclusive ownership of concepts developed with shared lab infrastructure and support. Individual contributions should be recognized through authorship on publications, presentations, and other scholarly outputs. Novel methods, tools, or software should be shared openly when possible, contributing to the broader scientific community and allowing others to build on our work."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#multi-lab-projects",
    "href": "posts/PSY591-Final-Project/Assignment.html#multi-lab-projects",
    "title": "WORDLE Lab Manual",
    "section": "16.4 Multi-Lab Projects",
    "text": "16.4 Multi-Lab Projects\nLarge collaborative projects involving multiple labs require additional structure and coordination. For these projects, establish a clear governance structure and decision-making processes at the outset so everyone knows who has authority over different aspects of the project. Create detailed project timelines with milestones to keep the work on track and allow for progress monitoring. Hold regular check-in meetings‚Äîmonthly or quarterly depending on the project timeline‚Äîto ensure all teams are aligned and to address any emerging issues. Document all major decisions in shared meeting notes or project management systems so there‚Äôs a record of how and why choices were made. Perhaps most importantly, agree on authorship criteria before work begins, including how author order will be determined and what contributions merit authorship versus acknowledgment."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#when-collaborations-go-wrong",
    "href": "posts/PSY591-Final-Project/Assignment.html#when-collaborations-go-wrong",
    "title": "WORDLE Lab Manual",
    "section": "16.5 When Collaborations Go Wrong",
    "text": "16.5 When Collaborations Go Wrong\nDespite best intentions and careful planning, conflicts can arise in collaborative work. When they do, address issues early before they escalate‚Äîsmall misunderstandings are much easier to resolve than major conflicts. Document concerns in writing, whether through email or shared documents, to create a record and ensure everyone has the same understanding of the issues. If you can‚Äôt resolve a conflict directly with your collaborator, involve Steven in mediation‚Äîhe can provide an outside perspective and help find solutions that work for everyone. For serious disputes, follow institutional procedures for resolving conflicts, which may involve department chairs, ombudspersons, or other university resources. Whatever the outcome, try to learn from the experience to prevent future issues‚Äîreflect on what went wrong and what could be done differently next time."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#best-practices-1",
    "href": "posts/PSY591-Final-Project/Assignment.html#best-practices-1",
    "title": "WORDLE Lab Manual",
    "section": "16.6 Best Practices",
    "text": "16.6 Best Practices\n\nCommunicate frequently - Over-communication is better than under-communication\nBe responsive - Reply to collaborator emails within 48 hours\nMeet deadlines - Or communicate early if you can‚Äôt\nGive credit generously - Acknowledge contributions explicitly\nBe open to feedback - Collaboration involves compromise\nDocument everything - Shared documents, meeting notes, and email trails protect everyone"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#offboarding-procedures",
    "href": "posts/PSY591-Final-Project/Assignment.html#offboarding-procedures",
    "title": "WORDLE Lab Manual",
    "section": "17.1 Offboarding Procedures",
    "text": "17.1 Offboarding Procedures\nWhen transitioning out of the lab:\n\nTransfer project files - Move all project files to appropriate lab members with clear documentation\nDocument ongoing work - Create detailed notes about work in progress, file locations, and next steps\nKnowledge transfer - Schedule meetings with people taking over your projects\nReturn lab equipment - Return all lab property (laptops, keys, equipment, etc.)\nAccess removal - Lab accounts and secure systems access will be removed\nUpdate contact information - Provide forwarding contact information for any follow-up questions\nExit interview - Meet with Steven to discuss your experience and provide feedback\n\nSee the Wiki for detailed offboarding checklists and templates."
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html",
    "href": "posts/LBA-Race-Kickoff/Presentation.html",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "",
    "text": "Background\nResearch Aims\nDesign/Measures\nQuestions for you"
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#acknowledgments-1",
    "href": "posts/PSY591-Final-Project/Assignment.html#acknowledgments-1",
    "title": "WORDLE Lab Manual",
    "section": "17.2 Acknowledgments",
    "text": "17.2 Acknowledgments\nMuch of the content in this lab manual was developed by Steven Mesquiti during his time as a Lab Manager for Dr.¬†Emily Falk at the Communication Neuroscience Lab at the University of Pennsylvania. This document aims to represents a synthesis of best practices, lessons learned, and values cultivated through that experience.\n\n\n\n\n\n\nLiving Document\n\n\n\nThis manual is meant to serve as a working draft and can always be updated! Lab culture evolves, practices improve, and new challenges emerge. We encourage all lab members to suggest changes, additions, or clarifications to keep this document relevant and useful. If you notice something that needs updating or have ideas for new sections, please reach out to Steven or post in the #ask_admin Slack channel."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#funding-lab-opportunities",
    "href": "posts/PSY591-Final-Project/Assignment.html#funding-lab-opportunities",
    "title": "WORDLE Lab Manual",
    "section": "12.2 Funding Lab Opportunities",
    "text": "12.2 Funding Lab Opportunities\nThere are multiple funding opportunities available to fund projects for researchers at different stages. For a broad (but by no means comprehensive) list of opportunities, you can take a look at the Funding Opportunities page on the lab wiki. If you see any funding opportunities pop up, feel free to share with the lab via Slack or email.\nThe lab is usually able to cover the cost of lab-wide or full project team events, such as lunches, dinners, and retreats. The lab will usually not cover expenses for individual or small-group activities. If you have any questions about the lab covering the cost of an activity, please check with the admin team, Steven, and/or the Finance Office."
  },
  {
    "objectID": "posts/PSY591-Final-Project/Assignment.html#best-practices",
    "href": "posts/PSY591-Final-Project/Assignment.html#best-practices",
    "title": "WORDLE Lab Manual",
    "section": "16.6 Best Practices",
    "text": "16.6 Best Practices\nSuccessful collaborations share several common practices. Communicate frequently‚Äîover-communication is better than under-communication, and regular check-ins prevent surprises. Be responsive by replying to collaborator emails within 48 hours, even if just to acknowledge receipt and indicate when you‚Äôll provide a full respons at a later time. Meet deadlines, or if you can‚Äôt, communicate early about the delay so your collaborators can adjust their plans accordingly. Give credit generously by acknowledging contributions explicitly in conversations, presentations, and publications‚Äîrecognition costs nothing and builds goodwill. Be open to feedback, remembering that collaboration involves compromise and that others‚Äô perspectives can strengthen your work. Finally, document everything‚Äîshared documents, meeting notes, and email trails protect everyone by creating a record of decisions, agreements, and contributions throughout the collaboration."
  },
  {
    "objectID": "posts/I-projections/Post.html#first-person-sing.-valence-projections",
    "href": "posts/I-projections/Post.html#first-person-sing.-valence-projections",
    "title": "I-Projection Results",
    "section": "First-person sing. valence projections",
    "text": "First-person sing. valence projections"
  },
  {
    "objectID": "posts/I-projections/Post.html#good",
    "href": "posts/I-projections/Post.html#good",
    "title": "I-Projection Results",
    "section": "Good",
    "text": "Good"
  },
  {
    "objectID": "posts/I-projections/Post.html#able",
    "href": "posts/I-projections/Post.html#able",
    "title": "I-Projection Results",
    "section": "Able",
    "text": "Able"
  }
]