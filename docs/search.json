[
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html",
    "href": "coding/Data-viz-basics/Data-viz.html",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\n\n\n\nDatasets can either be built-in or can be loaded from external sources in R.\nBuilt-in datasets refer to the datasets already provided within R. For the first part, we will be using a dataset called the air quality dataset, which pertains to the daily air quality measurements in New York from May to September 1973. This dataset consists of more than 100 observations for 6 variables\n\nOzone(mean parts per billion)\nSolar.R(Solar Radiation)\nWind(Average wind speed)\nTemp(maximum daily temperature in Fahrenheit)\nMonth(month of observation)\nDay(Day of the month)\n\n\ndata(airquality)\n\nIn case of an External data source (CSV, Excel, text, HTML file etc.), simply set the folder containing the data as the working directory with the setwd() command. Alternatively, you can set the path to the file if you don‚Äôt want to change your directory, but this is not recommended.\n\nsetwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n\nNow, load the file with the help of the read command. In this case, data is in the form of a CSV file named airquality.csv which can be downloaded from here.\nairquality &lt;- read.csv('airquality.csv',header=TRUE, sep=\",\")\nOne small (but important) thing to note is that you can name objects in R using both &lt;- and = which basically tells the computer ‚Äúsave the csv under this name‚Äù. In our case save ‚Äòairquality.csv‚Äô as airquality\nThe above code reads the file airquality.csv into a data frame airquality. Header=TRUE specifies that the data includes a header and sep=‚Äù,‚Äù specifies that the values in data are separated by commas.\n\n\n\nOnce the data has been loaded into the global environment (workspace), we need to explore it to get an idea about its structure and what we have to work with.\nTo do so, we can use several different functions within R\n\nstr displays the internal structure of an R object and gives a quick overview of the rows and columns of the dataset.\n\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nhead(data,n) and tail(data,n) The head outputs the top n elements in the dataset while the tail method outputs the bottom n.¬†The default value for n in R is 10 but you can obviously specify it to be something else if needed :).\n\n\nhead(airquality) \n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\ntail(airquality) \n\n    Ozone Solar.R Wind Temp Month Day\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n150    NA     145 13.2   77     9  27\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\n\n\nsummary(airquality)The summary method displays descriptive statistics for every variable in the dataset, depending upon the type of the variable. We can see at a glance the mean, median, max and the quartile values of the variables, as well as an missing observations which is especially valuable.\n\n\nsummary(airquality)\n\n     Ozone          Solar.R         Wind            Temp          Month     \n Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0   Min.   :5.00  \n 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0   1st Qu.:6.00  \n Median : 31.5   Median :205   Median : 9.70   Median :79.0   Median :7.00  \n Mean   : 42.1   Mean   :186   Mean   : 9.96   Mean   :77.9   Mean   :6.99  \n 3rd Qu.: 63.2   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0   3rd Qu.:8.00  \n Max.   :168.0   Max.   :334   Max.   :20.70   Max.   :97.0   Max.   :9.00  \n NA's   :37      NA's   :7                                                  \n      Day      \n Min.   : 1.0  \n 1st Qu.: 8.0  \n Median :16.0  \n Mean   :15.8  \n 3rd Qu.:23.0  \n Max.   :31.0"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#set-working-directory",
    "href": "coding/Data-viz-basics/Data-viz.html#set-working-directory",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#loading-datasets",
    "href": "coding/Data-viz-basics/Data-viz.html#loading-datasets",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "Datasets can either be built-in or can be loaded from external sources in R.\nBuilt-in datasets refer to the datasets already provided within R. For the first part, we will be using a dataset called the air quality dataset, which pertains to the daily air quality measurements in New York from May to September 1973. This dataset consists of more than 100 observations for 6 variables\n\nOzone(mean parts per billion)\nSolar.R(Solar Radiation)\nWind(Average wind speed)\nTemp(maximum daily temperature in Fahrenheit)\nMonth(month of observation)\nDay(Day of the month)\n\n\ndata(airquality)\n\nIn case of an External data source (CSV, Excel, text, HTML file etc.), simply set the folder containing the data as the working directory with the setwd() command. Alternatively, you can set the path to the file if you don‚Äôt want to change your directory, but this is not recommended.\n\nsetwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n\nNow, load the file with the help of the read command. In this case, data is in the form of a CSV file named airquality.csv which can be downloaded from here.\nairquality &lt;- read.csv('airquality.csv',header=TRUE, sep=\",\")\nOne small (but important) thing to note is that you can name objects in R using both &lt;- and = which basically tells the computer ‚Äúsave the csv under this name‚Äù. In our case save ‚Äòairquality.csv‚Äô as airquality\nThe above code reads the file airquality.csv into a data frame airquality. Header=TRUE specifies that the data includes a header and sep=‚Äù,‚Äù specifies that the values in data are separated by commas."
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#data-exploration",
    "href": "coding/Data-viz-basics/Data-viz.html#data-exploration",
    "title": "üé® R Data Visualization Adventure",
    "section": "",
    "text": "Once the data has been loaded into the global environment (workspace), we need to explore it to get an idea about its structure and what we have to work with.\nTo do so, we can use several different functions within R\n\nstr displays the internal structure of an R object and gives a quick overview of the rows and columns of the dataset.\n\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\n\nhead(data,n) and tail(data,n) The head outputs the top n elements in the dataset while the tail method outputs the bottom n.¬†The default value for n in R is 10 but you can obviously specify it to be something else if needed :).\n\n\nhead(airquality) \n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n\n\ntail(airquality) \n\n    Ozone Solar.R Wind Temp Month Day\n148    14      20 16.6   63     9  25\n149    30     193  6.9   70     9  26\n150    NA     145 13.2   77     9  27\n151    14     191 14.3   75     9  28\n152    18     131  8.0   76     9  29\n153    20     223 11.5   68     9  30\n\n\n\nsummary(airquality)The summary method displays descriptive statistics for every variable in the dataset, depending upon the type of the variable. We can see at a glance the mean, median, max and the quartile values of the variables, as well as an missing observations which is especially valuable.\n\n\nsummary(airquality)\n\n     Ozone          Solar.R         Wind            Temp          Month     \n Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0   Min.   :5.00  \n 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0   1st Qu.:6.00  \n Median : 31.5   Median :205   Median : 9.70   Median :79.0   Median :7.00  \n Mean   : 42.1   Mean   :186   Mean   : 9.96   Mean   :77.9   Mean   :6.99  \n 3rd Qu.: 63.2   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0   3rd Qu.:8.00  \n Max.   :168.0   Max.   :334   Max.   :20.70   Max.   :97.0   Max.   :9.00  \n NA's   :37      NA's   :7                                                  \n      Day      \n Min.   : 1.0  \n 1st Qu.: 8.0  \n Median :16.0  \n Mean   :15.8  \n 3rd Qu.:23.0  \n Max.   :31.0"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#the-plot-function",
    "href": "coding/Data-viz-basics/Data-viz.html#the-plot-function",
    "title": "üé® R Data Visualization Adventure",
    "section": "The Plot function",
    "text": "The Plot function\nThe plot() function is a generic function for plotting of R objects. When we run the code below, we get a scatter/dot plot here wherein each dot represents the value of the Ozone in mean parts per billion.\n\nplot(airquality$Ozone)\n\n\n\n\n\n\n\n\nLet‚Äôs now advance this some and plot a graph between the Ozone and Wind values to study the relationship between the two. The plot shows that Wind and Ozone values have a somewhat negative correlation.\n\nplot(airquality$Ozone, airquality$Wind)\n\n\n\n\n\n\n\n\nWhat happens when we use plot command with the entire dataset without selecting any particular columns?\nWe get a matrix of scatterplots which is a correlation matrix of all the columns. The plot above instantly shows that:\n\nThe level of Ozone and Temperature is correlated positively.\nWind speed is negatively correlated to both Temperature and Ozone level.\n\nWe can quickly discover the relationship between variables by merely looking at the plots drawn between them.\n\nplot(airquality)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#using-arguments-with-the-plot-function",
    "href": "coding/Data-viz-basics/Data-viz.html#using-arguments-with-the-plot-function",
    "title": "üé® R Data Visualization Adventure",
    "section": "Using arguments with the plot() function",
    "text": "Using arguments with the plot() function\nWe can easily style our charts by playing with the arguments of the plot() function.\nThe plot function has an argument called type which can take in values like p: points, l: lines, b: both etc. This decides the shape of the output graph.\n\n# points and lines \n plot(airquality$Ozone, type= \"b\")\n\n\n\n\n\n\n\n\n\n# high density vertical lines.\n plot(airquality$Ozone, type= \"h\")\n\n\n\n\n\n\n\n\nLabels and Titles\nWe can also label the X and the Y axis and give a title to our plot. Additionally, we also have the option of giving color to the plot.\n\nplot(airquality$Ozone, xlab = 'ozone Concentration', ylab = 'No of Instances', main = 'Ozone levels in NY city', col = 'green')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#barplot",
    "href": "coding/Data-viz-basics/Data-viz.html#barplot",
    "title": "üé® R Data Visualization Adventure",
    "section": "Barplot",
    "text": "Barplot\nIn a bar plot, data is represented in the form of rectangular bars and the length of the bar is proportional to the value of the variable or column in the dataset. Both horizontal, as well as a vertical bar chart, can be generated by tweaking the horiz parameter.\n\n# Horizontal bar plot\n barplot(airquality$Ozone, main = 'Ozone Concenteration in air',xlab = 'ozone levels', col= 'green',horiz = TRUE)\n\n\n\n\n\n\n\n\nVertical Barplot\n\n# Horizontal bar plot\n barplot(airquality$Ozone, main = 'Ozone Concenteration in air',xlab = 'ozone levels', col= 'green',horiz = F)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#histogram",
    "href": "coding/Data-viz-basics/Data-viz.html#histogram",
    "title": "üé® R Data Visualization Adventure",
    "section": "Histogram",
    "text": "Histogram\nA histogram is quite similar to a bar chart except that it groups values into continuous ranges. A histogram represents the frequencies of values of a variable bucketed into ranges. We get a histogram of the Solar.R values with hist(airquality$Solar.R).\n\nhist(airquality$Solar.R)\n\n\n\n\n\n\n\n\nBy giving an appropriate value for the color argument (e.g., col='red'), we can obtain a colored histogram as well.\n\nhist(airquality$Solar.R, main = 'Solar Radiation values in air',xlab = 'Solar rad.', col='red')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#boxplot",
    "href": "coding/Data-viz-basics/Data-viz.html#boxplot",
    "title": "üé® R Data Visualization Adventure",
    "section": "Boxplot",
    "text": "Boxplot\nWe have seen how the summary() command in R can display the descriptive statistics for every variable in the dataset. Boxplot does the same albeit graphically in the form of quartiles (e.g.,lowest 25% of the data, the middle 50% of the data, and the highest 25% of the data). It is again very straightforward to plot a boxplot in R.\nMaking a single box plot\n\n#Single box plot\nboxplot(airquality$Solar.R)\n\n\n\n\n\n\n\n\nMaking multiple box plots\n\n# Multiple box plots\nboxplot(airquality[,0:4], main='Multiple Box plots')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#grid-of-charts",
    "href": "coding/Data-viz-basics/Data-viz.html#grid-of-charts",
    "title": "üé® R Data Visualization Adventure",
    "section": "Grid of Charts",
    "text": "Grid of Charts\nThere is a very interesting feature in R which enables us to plot multiple charts at once. This comes in very handy during the EDA since the need to plot multiple graphs one by one is eliminated. For drawing a grid, the first argument should specify certain attributes like the margin of the grid(mar), no of rows and columns(mfrow), whether a border is to be included(bty) and position of the labels(las: 1 for horizontal, las: 0 for vertical).\n\npar(mfrow=c(3,3), mar=c(2,5,2,1), las=1, bty=\"n\")\nplot(airquality$Ozone)\nplot(airquality$Ozone, airquality$Wind)\nplot(airquality$Ozone, type= \"c\")\nplot(airquality$Ozone, type= \"s\")\nplot(airquality$Ozone, type= \"h\")\nbarplot(airquality$Ozone, main = 'Ozone Concenteration in air',xlab = 'ozone levels', col='green',horiz = TRUE)\nhist(airquality$Solar.R)\nboxplot(airquality$Solar.R)\nboxplot(airquality[,0:4], main='Multiple Box plots')"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#ggplot2",
    "href": "coding/Data-viz-basics/Data-viz.html#ggplot2",
    "title": "üé® R Data Visualization Adventure",
    "section": "Ggplot2",
    "text": "Ggplot2\nThe ggplot2 package is one of the most widely used visualization packages in R. It enables the users to create sophisticated visualizations with little code The popularity of ggplot2 has increased tremendously in recent years since it makes it possible to create graphs that contain both univariate and multivariate data in a very simple manner.\nInstall and Load Package and Data\n\n#Installing & Loading the package \n   \n#install.packages(\"ggplot2\") uncomment this to install\nlibrary(ggplot2)\n   \n#Loading the dataset\nattach(mtcars)\n# create factors with value labels \n\n\nScatterplots\ngeom_point() is used to create scatterplots and geom can have many variations like geom_jitter(), geom_count(), etc. Here, we use it to create a scatterplot for weight and mpg of cars. Notice how we specify what variables we want on our X and Y axes.\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg)) + geom_point()\n\n\n\n\n\n\n\n\n\n\nStyling Scatterplots\nWe can also style our scatterplots. For example, we can introduce an aesthetic that colors the points on the graph by some type of factor (e.g.¬†number of cylinders)\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg, color = as.factor(cyl))) + geom_point()\n\n\n\n\n\n\n\n\nThe color parameter is used to differentiate between different factor level of the cyl variable.\nAdditionally, we can introduce things like size\n\nggplot(data = mtcars, mapping = aes(x = wt, y = mpg, size = qsec)) + geom_point()\n\n\n\n\n\n\n\n\nIn the above example, the value of qsec indicates the acceleration which decides the size of the points.\nWe can also use different symbols to specify different things, as well.\n\np  &lt;-  ggplot(mtcars,aes(mpg, wt, shape  =  factor(cyl)))\n  p + geom_point(aes(colour  =  factor(cyl)), size  =  4) + geom_point(colour  =  \"grey90\", size  =  1.5)"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#load-packages",
    "href": "coding/Data-viz-basics/Data-viz.html#load-packages",
    "title": "üé® R Data Visualization Adventure",
    "section": "Load Packages",
    "text": "Load Packages\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix,plotrix, caret, broom, kableExtra, reactable, Hmisc, datarium, car,install = T) \n#use pacman to load packages quickly \n\nOne of the great things about R is its ability to be super flexible. This comes from R‚Äôs ability to use different packages. You can load packages into your current work environment by using the library(PACKAGE) function. It is important to note that in order to library a package you must first have it installed. To install a package you can use the install.packages(\"PACKAGE\") command. You can learn more about the different types of packages hosted on the Comprehensive R Archive Network (CRAN) here! One other important thing is that some packages often have similar commands (e.g., plyr and hmisc both use summarize) that are masked meaning that you will call a function and may not get the function you expect. To get around this you can use PACKAGE::FUNCTION to call package-specific function.\nFor this part of the script, and here forward, we use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion; I‚Äôve had issues with this in the past -_-)."
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#get-our-plot-aesthetics-set-up",
    "href": "coding/Data-viz-basics/Data-viz.html#get-our-plot-aesthetics-set-up",
    "title": "üé® R Data Visualization Adventure",
    "section": "Get our plot aesthetics set-up",
    "text": "Get our plot aesthetics set-up\nThis is a super quick and easy way to style our plots without introducing a vile amount of code lines to each chunk! Let‚Äôs break down what we are working with:\n\ntheme_classic() let‚Äôs us style our plot with a transparent back drop, rather than the grey, and use some other styling features.\ntheme() allows us to specify other parameters which are discussed below\nlegend.position we can specify where we want our graph‚Äôs legend to be. We can set it to: left, right, bottom, or top\ntext let‚Äôs us style our text. We can specify things like the size, color, adjustment, margins, etc.\naxis.text allows us to style the axis text similar to above\naxis.line permits for the styling of the axis lines (e.g., color, etc.)\naxis.ticks.x or y allows us to style the x and y axes\n\n\nplot_aes = theme_classic() +\n  theme(text = element_text(size = 16, family = \"Futura Medium\")) + \n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)) + \n  theme(axis.text=element_text(size=16),\n        axis.title=element_text(size=20,face=\"bold\"))+\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 20)) +\n  theme(axis.text=element_text(size = 14),\n        axis.title=element_text(size = 20,face=\"bold\"))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#load-up-the-data",
    "href": "coding/Data-viz-basics/Data-viz.html#load-up-the-data",
    "title": "üé® R Data Visualization Adventure",
    "section": "Load up the data",
    "text": "Load up the data\nSince we are using existing datasets in R and on github, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts, you‚Äôll see people use nomenclature similar to: df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over my files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to read in different files you can take a peek at this site.\n\n# Load the data\ndata(\"genderweight\", package = \"datarium\")\ngenderweight &lt;- as.data.frame(genderweight)\n\n# Show a sample of the data by group"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#basic-density",
    "href": "coding/Data-viz-basics/Data-viz.html#basic-density",
    "title": "üé® R Data Visualization Adventure",
    "section": "Basic Density",
    "text": "Basic Density\nTo check the distribution of the data we can use density plots in the ggplot within tidyverse to visualize this. In the first part of the code, we tell ggplot to get our data from the genderweight dataset and use the weight variable. Using geom_denisty we then tell it we want to color our plot dodgerblue (my favorite R palette :) ), as well as fill it with that color using color=\"dodgerblue4\", fill=\"dodgerblue3\". We then add our plot_aes object (usually doesn‚Äôt matter too much where we add this). Next we add a vertical line to our denisty plot using geom_vline, tell it that we want it to be at the mean value of the weight variable (which must be a continuous variable) and then style it color=\"dodgerblue3\", linetype=\"dashed\", size=1, telling R we want a blue, dashed line.\nLastly, we annotate the graph using annotate_figure. We feed in the object we want to annotate first (in this case p) and then tell R what we want for the top and bottom annotations!\n\np &lt;- ggplot(genderweight, aes(x=weight)) + \n  geom_density(color=\"dodgerblue4\", fill=\"dodgerblue3\", alpha=0.2) + plot_aes +\n  geom_vline(aes(xintercept=mean(weight)),\n            color=\"dodgerblue3\", linetype=\"dashed\", size=1) \nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Vertical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#styling-basic-density",
    "href": "coding/Data-viz-basics/Data-viz.html#styling-basic-density",
    "title": "üé® R Data Visualization Adventure",
    "section": "Styling Basic Density",
    "text": "Styling Basic Density\nWe can also have the densities by gender. We do so by adding color=group, fill=group to our code, letting R know to color things by our grouping variable (gender, called group in the dataset).\n\np&lt;-ggplot(genderweight, aes(x=weight, color=group, fill=group, alpha=0.1)) +\n  geom_density()+geom_vline(aes(xintercept=mean(weight)),\n            color=\"blue\", linetype=\"dashed\", size=1) + plot_aes \n\nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Verical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#overlay-two-different-geom-wrappers",
    "href": "coding/Data-viz-basics/Data-viz.html#overlay-two-different-geom-wrappers",
    "title": "üé® R Data Visualization Adventure",
    "section": "Overlay two different geom wrappers",
    "text": "Overlay two different geom wrappers\nWith ggplot2, we can also overlay different types of geom wrappers.\nFor example, we can overlay scatter plots (using geom_jitter) and boxplots (using geom_boxplot). Notice how we introduce grouping variables, plot title, and axis labels\n\n # Create a box plot with jittered data points\nggplot(genderweight, aes(x = group, y = weight,color = group)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, size = 2,alpha=0.2) +\n  # Add axis labels\n  xlab(\"Groups\") +\n  ylab(\"Weight\") +\n  plot_aes +\n  # Add plot title\n  ggtitle(\"Weight by Groups\") + theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#load-in-the-data",
    "href": "coding/Data-viz-basics/Data-viz.html#load-in-the-data",
    "title": "üé® R Data Visualization Adventure",
    "section": "Load in the Data",
    "text": "Load in the Data\nFirst, we load in the raw data from github. Second, we subset the data for a range of dates we are interested in (March 2019-March 2021). We then filter out word counts that are too noisy. Since LIWC is a bag of words program, we want to make sure our observations aren‚Äôt too noisy or large. For example, in the sentence ‚ÄúI love you‚Äù pronouns take up about 66% of the words the sentence and doing any type of analyses with that would not be very informative. We can get more into LIWC at a later time\nOne we have formatted our data appropriately, we can move to tidy it up. That is, we get the 4 variables of interest and use dplyr to group them by the month of their observation and then summarize them by getting the means and standard errors for each variable within each month\n\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Big_CEO.csv\") #read in the data from github \ndf &lt;- df %&gt;% filter(WC&lt;=5400)   %&gt;% \n  filter(WC&gt;=25)\n\ndf$month_year &lt;- format(as.Date(df$Date), \"%Y-%m\") ###extracting month and year to build fiscal quarter graphs, need a new variable bc if not it'll give us issues\n\ndf2 &lt;- df %&gt;%#converting our dates to quarterly dates \n  group_by(month_year) %&gt;% ###grouping by the Top100 tag and date \n  summarise_at(vars(\"Date\",\"WC\",\"Analytic\",\"cogproc\",'we','i'),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n\ndf2 &lt;- df2[\"2019-01\"&lt;= df2$month_year & df2$month_year &lt;= \"2021-03\",] #covid dates \n\nLet‚Äôs take a look at our data structure so you get an idea of what we‚Äôre working with using head().\n\nhead(df2)\n\n# A tibble: 6 √ó 13\n  month_year Date_mean  WC_mean Analytic_mean cogproc_mean we_mean i_mean\n  &lt;chr&gt;      &lt;date&gt;       &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 2019-01    2019-01-25   1774.          39.2         11.5    5.76   1.77\n2 2019-02    2019-02-15   1711.          40.8         11.4    5.69   1.66\n3 2019-03    2019-03-11   1427.          40.8         11.3    5.71   1.60\n4 2019-04    2019-04-23   1691.          39.3         11.4    5.67   1.77\n5 2019-05    2019-05-08   1527.          40.3         11.4    5.58   1.64\n6 2019-06    2019-06-13   1763.          40.8         11.4    5.85   1.65\n# ‚Ñπ 6 more variables: Date_std.error &lt;dbl&gt;, WC_std.error &lt;dbl&gt;,\n#   Analytic_std.error &lt;dbl&gt;, cogproc_std.error &lt;dbl&gt;, we_std.error &lt;dbl&gt;,\n#   i_std.error &lt;dbl&gt;"
  },
  {
    "objectID": "coding/Data-viz-basics/Data-viz.html#build-our-graphs",
    "href": "coding/Data-viz-basics/Data-viz.html#build-our-graphs",
    "title": "üé® R Data Visualization Adventure",
    "section": "Build our Graphs",
    "text": "Build our Graphs\nNow, we‚Äôll run our code. Here‚Äôs a general explanation of what each line does using our first example:\nAnalytic &lt;- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) + creates a ggplot object and names it Analytic. We have Date_mean as our X axis and Analytic_mean as our y variable\ngeom_line(colour = \"dodgerblue3\") + writes our line graph using the parameters we specified above\nscale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") + Tells the graph we want 3 month partitions on our X-axis using the format \"%Y-%m\"\ngeom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) + Graphs the standard error around our linegraph\nggtitle(\"Analytic Thinking\") + titles our plot\nlabs(x = \"Month\", y = 'Standardized score') + adds our x and y axis labels\nplot_aes + here‚Äôs our plot aes object\ngeom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) + specifying we want a vertical line at this specific date\ngeom_rect(data = df2, #summer surge give us a rectangle using the data\naes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), let the lower bound on the x plane be this date\nxmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"), let the upper bound on the x plane be this date\nymin = -Inf, let the lower bound on the y plane be this value\nymax = Inf), let the upper bound on the x plane be this value\nfill = \"gray\", color it grey\nalpha = 0.009) + let it be pretty transparent\ngeom_rect(data = df2, #winter surge give us a rectangle using the data\naes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), let the lower bound on the x plane be this date\nxmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"), let the upper bound on the x plane be this date\nymin = -Inf, let the lower bound on the y plane be this value\nymax = Inf), let the upper bound on the x plane be this value\nfill = \"gray\", color it grey\nalpha = 0.009)let it be pretty transparent\nAnalytic &lt;- Analytic + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"), save a new object called Analytic and write text at this specific date on the x plane\ny=43,label=\"Summer 2020 surge\", size = 3) + specifying what our label is, what the y plane value is and font size\nannotate(geom=\"text\",x=as.Date(\"2020-12-03\"), his specific date on the x plane\ny=43,label=\"Winter 2020 surge\", size = 3) specifying what our label is, what the y plane value is and font size\nWe can use the {r fig.height=6, fig.width=8} in the block of code to specify the dimensions of our figures. This is super helpful when building stuff for presentations, project, etc. I forgot to mention earlier that in the set-up chunk we used fig.path = \"figs/data-viz/\" to specify where our figures to output to within our working directroy. So, if you go to wherever your directorty is you should be able to find them in that file path :).\n\nAnalytic Thinking\n\nAnalytic &lt;- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) +\n  ggtitle(\"Analytic Thinking\") +\n  labs(x = \"Month\", y = 'Standardized score') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nAnalytic &lt;- Analytic + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=43,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=43,label=\"Winter 2020 surge\", size = 3)\nAnalytic\n\n\n\n\n\n\n\n\n\n\nCogproc\n\nCogproc &lt;- ggplot(data=df2, aes(x=Date_mean, y=cogproc_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=cogproc_mean-cogproc_std.error, ymax=cogproc_mean+cogproc_std.error), alpha=0.2) +\n  ggtitle(\"Cognitive Processing\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nCogproc &lt;- Cogproc + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=12.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=12.5,label=\"Winter 2020 surge\", size = 3)\nCogproc\n\n\n\n\n\n\n\n\n\n\nI-words\n\ni &lt;- ggplot(data=df2, aes(x=Date_mean, y=i_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=i_mean-i_std.error, ymax=i_mean+i_std.error), alpha=0.2) +\n  ggtitle(\"I-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\ni &lt;- i + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=1.95,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=1.95,label=\"Winter 2020 surge\", size = 3)\ni\n\n\n\n\n\n\n\n\n\n\nWe-words\n\nwe &lt;- ggplot(data=df2, aes(x=Date_mean, y=we_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=we_mean-we_std.error, ymax=we_mean+we_std.error), alpha=0.2) +\n  ggtitle(\"We-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nwe &lt;- we + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=6.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=6.5,label=\"Winter 2020 surge\", size = 3)\nwe\n\n\n\n\n\n\n\n\n\n\nTie them all together\n\ngraphs &lt;- ggpubr::ggarrange(Analytic,Cogproc,i,we,ncol=2, nrow=2, common.legend = TRUE, legend = \"bottom\")\nannotate_figure(graphs,\n                top = text_grob(\"CEOs' Language Change\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Note. Vertical Line Represents the onset of the pandemic. \\n\\ Horizontal shading represents Standard Error. Vertical bars represent virus surges.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 16))"
  },
  {
    "objectID": "coding/t-test/T-tests.html",
    "href": "coding/t-test/T-tests.html",
    "title": "T-tests",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Data-viz-basics\")\n\n\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD.\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, caret, broom, kableExtra, reactable, Hmisc, datarium, car,install = T) \n#use pacman to load packages quickly \n\nOne of the great things about R is its ability to be super flexible. This comes from R‚Äôs ability to use different packages. You can load packages into your current work environment by using the library(PACKAGE) function. It is important to note that in order to library a package you must first have it installed. To install a package you can use the install.packages(\"PACKAGE\") command. You can learn more about the different types of packages hosted on the Comprehensive R Archive Network (CRAN) here! One other important thing is that some packages often have similar commands (e.g., plyr and hmisc both use summarize) that are masked meaning that you will call a function and may not get the function you expect. To get around this you can use PACKAGE::FUNCTION to call package-specific function.\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion).\n\n\n\nThis is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())\n\n\n\n\nUsing stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\nmystats &lt;- function(x, na.omit=FALSE){\n  if (na.omit)\n    x &lt;- x[!is.na(x)]\n  m &lt;- mean(x)\n  n &lt;- length(x)\n  s &lt;- sd(x)\n  skew &lt;- sum((x-m)^3/s^3)/n\n  kurt &lt;- sum((x-m)^4/s^4)/n - 3\n  return(c(n=n, mean=m, stdev=s, skew=skew, kurtosis=kurt))\n}\n\n\n\n\nSince we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\n\n# Load the data\ndata(\"genderweight\", package = \"datarium\")\ngenderweight &lt;- as.data.frame(genderweight)\n\n# Show a sample of the data by group"
  },
  {
    "objectID": "coding/t-test/T-tests.html#set-working-directory",
    "href": "coding/t-test/T-tests.html#set-working-directory",
    "title": "T-tests",
    "section": "",
    "text": "Change to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD."
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-packages",
    "href": "coding/t-test/T-tests.html#load-packages",
    "title": "T-tests",
    "section": "",
    "text": "if (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, caret, broom, kableExtra, reactable, Hmisc, datarium, car,install = T) \n#use pacman to load packages quickly \n\nOne of the great things about R is its ability to be super flexible. This comes from R‚Äôs ability to use different packages. You can load packages into your current work environment by using the library(PACKAGE) function. It is important to note that in order to library a package you must first have it installed. To install a package you can use the install.packages(\"PACKAGE\") command. You can learn more about the different types of packages hosted on the Comprehensive R Archive Network (CRAN) here! One other important thing is that some packages often have similar commands (e.g., plyr and hmisc both use summarize) that are masked meaning that you will call a function and may not get the function you expect. To get around this you can use PACKAGE::FUNCTION to call package-specific function.\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion)."
  },
  {
    "objectID": "coding/t-test/T-tests.html#get-our-plot-aesthetics-set-up",
    "href": "coding/t-test/T-tests.html#get-our-plot-aesthetics-set-up",
    "title": "T-tests",
    "section": "",
    "text": "This is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "coding/t-test/T-tests.html#build-relevant-functions",
    "href": "coding/t-test/T-tests.html#build-relevant-functions",
    "title": "T-tests",
    "section": "",
    "text": "Using stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\nmystats &lt;- function(x, na.omit=FALSE){\n  if (na.omit)\n    x &lt;- x[!is.na(x)]\n  m &lt;- mean(x)\n  n &lt;- length(x)\n  s &lt;- sd(x)\n  skew &lt;- sum((x-m)^3/s^3)/n\n  kurt &lt;- sum((x-m)^4/s^4)/n - 3\n  return(c(n=n, mean=m, stdev=s, skew=skew, kurtosis=kurt))\n}"
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-data",
    "href": "coding/t-test/T-tests.html#load-data",
    "title": "T-tests",
    "section": "",
    "text": "Since we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\n\n# Load the data\ndata(\"genderweight\", package = \"datarium\")\ngenderweight &lt;- as.data.frame(genderweight)\n\n# Show a sample of the data by group"
  },
  {
    "objectID": "coding/t-test/T-tests.html#statistical-assumptions",
    "href": "coding/t-test/T-tests.html#statistical-assumptions",
    "title": "T-tests",
    "section": "Statistical Assumptions",
    "text": "Statistical Assumptions\nAssumption are also important. That is, data need to possess certain qualities for us to be able to use this type of test. For a t-test these are:\n\nThe DV data are continuous (not ordinal or nominal).\nThe sample data have been randomly sampled from a population.\nThere is homogeneity of variance (i.e., the variability of the data in each group is similar).\nThe distribution is approximately normal.\n\nClick through the tabs to see how to check each assumption.\n\nContinuous\nWe can check this by looking at the structure of our data using the class function (for one variable) or str function (for all the variables in our dataset). We can see that weight is numeric and therefore continuous! Therefore, we can move forward with our analyses.\n\nclass(genderweight$weight)\n\n[1] \"numeric\"\n\n\n\nstr(genderweight)\n\n'data.frame':   40 obs. of  3 variables:\n $ id    : Factor w/ 40 levels \"1\",\"2\",\"3\",\"4\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ group : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ weight: num  61.6 64.6 66.2 59.3 64.9 ...\n\n\n\n\nRandomly Sampled\nThis is something you do when you design the study‚Äìwe can‚Äôt do anything in R to check this.\n\n\nHomogeneity of Variance\nWe need to make sure the variability of the data in each group is similar. We can use something called Levene‚Äôs Test for equality of error variances to do this. If we violate this assumption (p &lt;. 05 in our test) we will have to use a Welch‚Äôs T-test. We violate this assumption so if we were actually doing a meaningful project we would need to use a different statistical test. For the sake of brevity we‚Äôll pretend we are ok for now.\n\nleveneTest(genderweight$weight ~ genderweight$group)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)  \ngroup  1    6.12  0.018 *\n      38                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nThe distribution is approximately normal.\nTo check the distribution of the data we can use density plots in the ggplot within tidyverse to visualize this. It‚Äôs also important to get some statistics behind this, and to do that we can look at skewness and kurtosis via the mystats function that we wrote earlier. You can also use psych::describe to get similar information. For skewness and kurtosis, we want values of skewness fall between ‚àí 3 and + 3, and kurtosis is appropriate from a range of ‚àí 10 to + 10\n\n# Basic density\np &lt;- ggplot(genderweight, aes(x=weight)) + \n  geom_density(color=\"dodgerblue4\", fill=\"dodgerblue3\", alpha=0.2) + plot_aes +\n  geom_vline(aes(xintercept=mean(weight)),\n            color=\"dodgerblue3\", linetype=\"dashed\", size=1) \nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Verical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))\n\n\n\n\n\n\n\n\nWe can also have the densities by gender. Looks like we should have some interesting results!\n\np&lt;-ggplot(genderweight, aes(x=weight, color=group, fill=group, alpha=0.1)) +\n  geom_density()+geom_vline(aes(xintercept=mean(weight)),\n            color=\"blue\", linetype=\"dashed\", size=1) + plot_aes \n\nannotate_figure(p,\n                top = text_grob(\"Density Plots for both genders\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Verical line represents mean value.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 12))\n\n\n\n\n\n\n\n\n\nmystats(genderweight$weight)\n\n       n     mean    stdev     skew kurtosis \n 40.0000  74.6624  11.7924   0.1486  -1.7419"
  },
  {
    "objectID": "coding/t-test/T-tests.html#sample-size-means-and-standard-deviations",
    "href": "coding/t-test/T-tests.html#sample-size-means-and-standard-deviations",
    "title": "T-tests",
    "section": "Sample Size, Means, and Standard Deviations",
    "text": "Sample Size, Means, and Standard Deviations\n\ngenderweight %&gt;%\n  group_by(group) %&gt;%\n  get_summary_stats(weight, type = \"mean_sd\") %&gt;% \n  reactable::reactable(striped = TRUE)"
  },
  {
    "objectID": "coding/t-test/T-tests.html#build-a-simple-graph-to-visualize",
    "href": "coding/t-test/T-tests.html#build-a-simple-graph-to-visualize",
    "title": "T-tests",
    "section": "Build a simple graph to visualize",
    "text": "Build a simple graph to visualize\nWe also visualize our data with a box plot, while overlaying the scatter plots!\n\n # Create a box plot with jittered data points\nggplot(genderweight, aes(x = group, y = weight,color = group)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, size = 2,alpha=0.2) +\n  # Add axis labels\n  xlab(\"Groups\") +\n  ylab(\"Weight\") +\n  plot_aes +\n  # Add plot title\n  ggtitle(\"Weight by Groups\") + theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "coding/t-test/T-tests.html#independent-t-test",
    "href": "coding/t-test/T-tests.html#independent-t-test",
    "title": "T-tests",
    "section": "Independent T-test",
    "text": "Independent T-test\nWe can see that we have a significant effect of gender on weight. That is, when conducting our independent t-test we observed a large effect [t(26.87) =-20.79, p &lt; .001, d = -6.575], such that Men (M = 85.826, SD = 4.354) possessed significantly greater body weight compared to their female counterparts (M = 63.499, SD = 2.028).\n\nstat.test &lt;- genderweight %&gt;% \n  t_test(weight ~ group) %&gt;%\n  add_significance()\nstat.test\n\n# A tibble: 1 √ó 9\n  .y.    group1 group2    n1    n2 statistic    df        p p.signif\n  &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;   \n1 weight F      M         20    20     -20.8  26.9 4.30e-18 ****"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cohens-d",
    "href": "coding/t-test/T-tests.html#cohens-d",
    "title": "T-tests",
    "section": "Cohen‚Äôs D",
    "text": "Cohen‚Äôs D\nFrom inspecting our output, we can see we have a large effect of gender\n\ngenderweight %&gt;%  cohens_d(weight ~ group, var.equal = TRUE)\n\n# A tibble: 1 √ó 7\n  .y.    group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 weight F      M        -6.57    20    20 large"
  },
  {
    "objectID": "coding/t-test/T-tests.html#visualize-our-results-using-gg-plot-with-stats",
    "href": "coding/t-test/T-tests.html#visualize-our-results-using-gg-plot-with-stats",
    "title": "T-tests",
    "section": "Visualize our results using GG-Plot with stats",
    "text": "Visualize our results using GG-Plot with stats\n\nplot &lt;- ggplot(genderweight, aes(x = group, y = weight, color = group)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, size = 2) +\n  # Add axis labels\n  xlab(\"Gender\") +\n  ylab(\"Weight\") +\n  plot_aes +\n  # Add plot title\n  ggtitle(\"Weight by Gender\") + theme(plot.title = element_text(hjust = 0.5))\n\nstat.test &lt;- stat.test %&gt;% add_xy_position(x = \"group\")\nplot &lt;- plot +  stat_pvalue_manual(stat.test, tip.length = 0) +\n  labs(subtitle = get_test_label(stat.test, detailed = TRUE)) \n\nannotate_figure(plot,\n                bottom = text_grob(\"D = -6.575\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 16))"
  },
  {
    "objectID": "coding/t-test/T-tests.html#abstract",
    "href": "coding/t-test/T-tests.html#abstract",
    "title": "T-tests",
    "section": "Abstract",
    "text": "Abstract\nThe COVID-19 pandemic sent shockwaves across the fabric of our society. Examining the impact of the pandemic on business leadership is particularly important to understanding how this event affected their decision-making. The present study documents the psychological effects of the COVID-19 pandemic on chief executive officers (CEOs). This was accomplished by analyzing CEOs‚Äô language from quarterly earnings calls (N = 19,536) for a year before and after lockdown. CEOs had large shifts in language in the months immediately following the start of the pandemic lockdowns. Analytic thinking plummeted after the world went into lockdown, with CEOs‚Äô language becoming less technical and more personal and intuitive. In parallel, CEOs‚Äô language showed signs of increased cognitive load, as they were processing the effect of the pandemic on their business practices. Business leaders‚Äô use of collective-focused language (we-usage) dropped substantially after the pandemic began, perhaps suggesting CEOs felt disconnected from their companies. Self-focused (I-usage) language increased, showing the increased preoccupation of business leaders. The size of the observed shifts in language during the pandemic also dwarfed responses to other events that occurred dating back to 2010, with the effect lasting around seven months."
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-necessary-packages-and-set-working-directory",
    "href": "coding/t-test/T-tests.html#load-necessary-packages-and-set-working-directory",
    "title": "T-tests",
    "section": "Load necessary packages and set Working Directory",
    "text": "Load necessary packages and set Working Directory\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse,zoo,lubridate,plotrix,ggpubr, caret, broom, kableExtra, reactable, effsize, install = T)"
  },
  {
    "objectID": "coding/t-test/T-tests.html#define-aesthetics",
    "href": "coding/t-test/T-tests.html#define-aesthetics",
    "title": "T-tests",
    "section": "Define aesthetics",
    "text": "Define aesthetics\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() +\n  theme(text = element_text(size = 16, family = \"Futura Medium\")) + \n  theme(axis.text.x=element_text(angle=45, hjust=1)) +\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16)) + \n  theme(axis.text=element_text(size=16),\n        axis.title=element_text(size=20,face=\"bold\"))+\n  theme(plot.title.position = 'plot', \n        plot.title = element_text(hjust = 0.5, face = \"bold\", size = 20)) +\n  theme(axis.text=element_text(size = 14),\n        axis.title=element_text(size = 20,face=\"bold\"))"
  },
  {
    "objectID": "coding/t-test/T-tests.html#write-our-table-funcions",
    "href": "coding/t-test/T-tests.html#write-our-table-funcions",
    "title": "T-tests",
    "section": "Write our Table Funcions",
    "text": "Write our Table Funcions\n\nbaseline_ttest &lt;- function(ttest_list) {\n  # Extract relevant information from each test and store in a data frame\n  ttest_df &lt;- data.frame(\n    Group1 = seq(0,0,1),\n    Group2 = seq(1,24,1),\n    t = sapply(ttest_list, function(x) x$statistic),\n    df = sapply(ttest_list, function(x) x$parameter),\n    p_value = sapply(ttest_list, function(x) x$p.value)\n  )\n  \n  # Format p-values as scientific notation\n  ttest_df$p_value &lt;- format(ttest_df$p_value, scientific = T)\n  \n  # Rename columns\n  colnames(ttest_df) &lt;- c(\"t\", \"t + 1 \", \"t-value\", \"Degrees of Freedom\", \"p-value\")\n  \n  # Create table using kableExtra\n  kable(ttest_df, caption = \"Summary of Welch's t-Tests\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\npost_pandemic_summary &lt;- function(ttest_list) {\n  # Extract relevant information from each test and store in a data frame\n  ttest_df &lt;- data.frame(\n    Group1 = seq(12,23,1),\n    Group2 = seq(13,24,1),\n    t = sapply(ttest_list, function(x) x$statistic),\n    df = sapply(ttest_list, function(x) x$parameter),\n    p_value = sapply(ttest_list, function(x) x$p.value)\n  )\n  \n  # Format p-values as scientific notation\n  ttest_df$p_value &lt;- format(ttest_df$p_value, scientific = T)\n  \n  # Rename columns\n  colnames(ttest_df) &lt;- c(\"t\", \"t + 1 \", \"t-value\", \"Degrees of Freedom\", \"p-value\")\n  \n  # Create table using kableExtra\n  kable(ttest_df, caption = \"Summary of Welch's t-Tests\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\n\n\nbaseline_cohen_d &lt;- function(cohen_d_list) {\n  # Extract relevant information from each test and store in a data frame\n  cohen_d_df &lt;- data.frame(\n    Group1 = seq(0,0,1),\n    Group2 = seq(1,24,1),\n    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)\n  )\n  \n  # Rename columns\n  colnames(cohen_d_df) &lt;- c(\"t\", \"t + 1\", \"Cohen's d\")\n  \n  # Create table using kableExtra\n  kable(cohen_d_df, caption = \"Summary of Cohen's D\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\npost_cohen_d &lt;- function(cohen_d_list) {\n  # Extract relevant information from each test and store in a data frame\n  cohen_d_df &lt;- data.frame(\n    Group1 = seq(12,23,1),\n    Group2 = seq(13,24,1),\n    Cohen_d = sapply(cohen_d_list, function(x) x$estimate)\n  )\n  \n  # Rename columns\n  colnames(cohen_d_df) &lt;- c(\"t\", \"t+1\", \"Cohen's d\")\n  \n  # Create table using kableExtra\n  kable(cohen_d_df, caption = \"Summary of Cohen's D\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\nbaseline_mean_diff &lt;- function(mean_diff_list) {\n  # Extract relevant information from each mean difference calculation and store in a data frame\n  mean_diff_df &lt;- data.frame(\n    Group1 = seq(0,0,1),\n    Group2 = seq(1,24,1),\n    mean_diff = mean_diff_list\n  )\n  \n  # Rename columns\n  colnames(mean_diff_df) &lt;- c(\"t\", \"t+1\", \"Mean Difference\")\n  \n  # Create table using kableExtra\n  kable(mean_diff_df, caption = \"Summary of Mean Differences\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}\n\n\npost_mean_diff &lt;- function(mean_diff_list) {\n  # Extract relevant information from each mean difference calculation and store in a data frame\n  mean_diff_df &lt;- data.frame(\n    Group1 = seq(12,23,1),\n    Group2 = seq(13,24,1),\n    mean_diff = mean_diff_list\n  )\n  \n  # Rename columns\n  colnames(mean_diff_df) &lt;- c(\"t\", \"t+1\", \"Mean Difference\")\n  \n  # Create table using kableExtra\n  kable(mean_diff_df, caption = \"Summary of Mean Differences\", booktabs = TRUE) %&gt;%\n   kableExtra::kable_styling()\n}"
  },
  {
    "objectID": "coding/t-test/T-tests.html#load-in-the-data",
    "href": "coding/t-test/T-tests.html#load-in-the-data",
    "title": "T-tests",
    "section": "Load in the Data",
    "text": "Load in the Data\n\ndata  &lt;-  read_csv(\"https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Big_CEO.csv\") #read in the data from github \n\ndata &lt;- data[\"2019-03-01\"&lt;= data$Date & data$Date &lt;= \"2021-04-01\",] #subsetting covid dates \n\ndata &lt;- data %&gt;% filter(WC&lt;=5400) %&gt;% #filter out based on our exclusion criteria\n  filter(WC&gt;=25)\n\ndata$month_year &lt;- format(as.Date(data$Date), \"%Y-%m\") #reformat \n\ndata_tidy &lt;- data %&gt;% dplyr::select(Date, Speaker, Analytic, cogproc,allnone,we,i,emo_anx) %&gt;%\n  mutate(Date = lubridate::ymd(Date),\n         time_month = as.numeric(Date - ymd(\"2019-03-01\")) / 30, #centering at start of march\n         time_month_quad = time_month * time_month) #making our quadratic term\n\ndata_tidy$Date_off &lt;- floor(data_tidy$time_month) #rounding off dates to whole months using ceiling function (0 = 2019-03, 24 = 2021-04)\ndata_tidy$Date_covid &lt;- as.factor(data_tidy$Date_off) #factorize"
  },
  {
    "objectID": "coding/t-test/T-tests.html#create-tidy-data-for-graphs",
    "href": "coding/t-test/T-tests.html#create-tidy-data-for-graphs",
    "title": "T-tests",
    "section": "Create Tidy Data for Graphs",
    "text": "Create Tidy Data for Graphs\n\ndf &lt;- read_csv(\"https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Big_CEO.csv\")#put code here to read in Big CEO data\ndf &lt;- df %&gt;% filter(WC&lt;=5400)   %&gt;% \n  filter(WC&gt;=25)\n\ndf$month_year &lt;- format(as.Date(df$Date), \"%Y-%m\") ###extracting month and year to build fiscal quarter graphs, need a new variable bc if not it'll give us issues\n\ndf2 &lt;- df %&gt;%#converting our dates to quarterly dates \n  group_by(month_year) %&gt;% ###grouping by the Top100 tag and date \n  summarise_at(vars(\"Date\",\"WC\",\"Analytic\",\"cogproc\",'we','i'),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n\ndf2 &lt;- df2[\"2019-01\"&lt;= df2$month_year & df2$month_year &lt;= \"2021-03\",] #covid dates"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thinking",
    "href": "coding/t-test/T-tests.html#analytic-thinking",
    "title": "T-tests",
    "section": "Analytic Thinking",
    "text": "Analytic Thinking\n\nanalytic_my.t = function(fac1, fac2){\n  t.test(data_tidy$Analytic[data_tidy$Date_covid==fac1], \n         data_tidy$Analytic[data_tidy$Date_covid==fac2])\n} #writing our t-test function to compare t to t[i] \n\nanalytic_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$Analytic[data_tidy$Date_covid==fac1], \n          data_tidy$Analytic[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\nanalytic_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$Analytic[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$Analytic[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cognitive-processing",
    "href": "coding/t-test/T-tests.html#cognitive-processing",
    "title": "T-tests",
    "section": "Cognitive Processing",
    "text": "Cognitive Processing\n\ncogproc_my.t = function(fac1, fac2){\n  t.test(data_tidy$cogproc[data_tidy$Date_covid==fac1], \n         data_tidy$cogproc[data_tidy$Date_covid==fac2])\n} #writing our t-test function to compare t to t[i] \n\n\ncogproc_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$cogproc[data_tidy$Date_covid==fac1], \n          data_tidy$cogproc[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\ncogproc_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$cogproc[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$cogproc[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words",
    "href": "coding/t-test/T-tests.html#i-words",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\ni_my.t = function(fac1, fac2){\n  t.test(data_tidy$i[data_tidy$Date_covid==fac1], \n         data_tidy$i[data_tidy$Date_covid==fac2])\n} #writing our t-test function to compare t to t + 1 \n\ni_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$i[data_tidy$Date_covid==fac1], \n          data_tidy$i[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\n\ni_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$i[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$i[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words",
    "href": "coding/t-test/T-tests.html#we-words",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nwe_my.t = function(fac1, fac2){\n  t.test(data_tidy$we[data_tidy$Date_covid==fac1], \n         data_tidy$we[data_tidy$Date_covid==fac2])\n} \n\nwe_my.d = function(fac1, fac2){\n  cohen.d(data_tidy$we[data_tidy$Date_covid==fac1], \n          data_tidy$we[data_tidy$Date_covid==fac2])\n} #function for cohen's d\n\nwe_mean &lt;-  function(fac1, fac2){\n  mean(data_tidy$we[data_tidy$Date_covid==fac1])- \n    mean(data_tidy$we[data_tidy$Date_covid==fac2])\n} #function to do mean differences"
  },
  {
    "objectID": "coding/t-test/T-tests.html#tidy-data",
    "href": "coding/t-test/T-tests.html#tidy-data",
    "title": "T-tests",
    "section": "Tidy data",
    "text": "Tidy data\nData transformations\n\nNone\n\nExclusions\n\nExcluded texts that were shorter than ** 25 words ** and greater than ** 5,400 words **!"
  },
  {
    "objectID": "coding/t-test/T-tests.html#range-of-dates",
    "href": "coding/t-test/T-tests.html#range-of-dates",
    "title": "T-tests",
    "section": "Range of Dates",
    "text": "Range of Dates\n\nrange(data$Date)\n\n[1] \"2019-03-01\" \"2021-04-01\""
  },
  {
    "objectID": "coding/t-test/T-tests.html#number-of-speakers",
    "href": "coding/t-test/T-tests.html#number-of-speakers",
    "title": "T-tests",
    "section": "Number of Speakers",
    "text": "Number of Speakers\n\nspeakers &lt;- data %&gt;%\n  select(Speaker) %&gt;%\n  unique() %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  reactable::reactable(striped = TRUE)\nspeakers"
  },
  {
    "objectID": "coding/t-test/T-tests.html#number-of-transcripts",
    "href": "coding/t-test/T-tests.html#number-of-transcripts",
    "title": "T-tests",
    "section": "Number of Transcripts",
    "text": "Number of Transcripts\n\ntranscripts &lt;- data %&gt;%\n  select(1) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  reactable::reactable(striped = TRUE)\ntranscripts"
  },
  {
    "objectID": "coding/t-test/T-tests.html#mean-word-count",
    "href": "coding/t-test/T-tests.html#mean-word-count",
    "title": "T-tests",
    "section": "Mean Word Count",
    "text": "Mean Word Count\n\nword_count &lt;- data %&gt;%\n  select(WC) %&gt;%\n  dplyr::summarize(mean = mean(WC)) %&gt;%\n  reactable::reactable(striped = TRUE)\nword_count"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thinking-1",
    "href": "coding/t-test/T-tests.html#analytic-thinking-1",
    "title": "T-tests",
    "section": "Analytic Thinking",
    "text": "Analytic Thinking\n\nT-test\n\nanalytic_ttest&lt;- mapply(analytic_my.t,seq(12,23,1), seq(13,24,1),SIMPLIFY=F) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1\npost_pandemic_summary(analytic_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n5.0849\n525.79\n5.124e-07\n\n\n13\n14\n-2.5948\n373.06\n9.839e-03\n\n\n14\n15\n-1.6726\n252.04\n9.565e-02\n\n\n15\n16\n1.9242\n377.62\n5.508e-02\n\n\n16\n17\n-2.2122\n200.57\n2.808e-02\n\n\n17\n18\n-1.6872\n218.93\n9.298e-02\n\n\n18\n19\n0.6199\n262.61\n5.358e-01\n\n\n19\n20\n0.8738\n128.22\n3.839e-01\n\n\n20\n21\n-1.5398\n230.76\n1.250e-01\n\n\n21\n22\n1.9533\n94.32\n5.374e-02\n\n\n22\n23\n-1.1498\n55.55\n2.552e-01\n\n\n23\n24\n-1.7179\n2141.37\n8.596e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nanalytic_d &lt;- mapply(analytic_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) \npost_cohen_d(analytic_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n0.3275\n\n\n13\n14\n-0.1598\n\n\n14\n15\n-0.1320\n\n\n15\n16\n0.1936\n\n\n16\n17\n-0.1617\n\n\n17\n18\n-0.1481\n\n\n18\n19\n0.0710\n\n\n19\n20\n0.0899\n\n\n20\n21\n-0.1246\n\n\n21\n22\n0.2682\n\n\n22\n23\n-0.1598\n\n\n23\n24\n-0.0739\n\n\n\n\n\n\n\n\n\nMean Differences\n\nanalytic_meandiff &lt;- mapply(analytic_mean, seq(12,23,1), seq(13,24,1)) #across all of the months comparing to time zero\npost_mean_diff(analytic_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n4.7346\n\n\n13\n14\n-2.1905\n\n\n14\n15\n-1.8443\n\n\n15\n16\n2.7483\n\n\n16\n17\n-2.2318\n\n\n17\n18\n-2.1013\n\n\n18\n19\n1.1589\n\n\n19\n20\n1.2765\n\n\n20\n21\n-1.7791\n\n\n21\n22\n4.0651\n\n\n22\n23\n-2.0756\n\n\n23\n24\n-0.9941"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cogproc",
    "href": "coding/t-test/T-tests.html#cogproc",
    "title": "T-tests",
    "section": "Cogproc",
    "text": "Cogproc\n\nT-test\n\ncogproc_ttest &lt;-mapply(cogproc_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\npost_pandemic_summary(cogproc_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n-4.3161\n534.57\n1.893e-05\n\n\n13\n14\n1.4046\n366.54\n1.610e-01\n\n\n14\n15\n4.0193\n257.87\n7.665e-05\n\n\n15\n16\n-3.1317\n367.30\n1.877e-03\n\n\n16\n17\n0.9868\n199.24\n3.249e-01\n\n\n17\n18\n4.1804\n223.61\n4.178e-05\n\n\n18\n19\n-1.1984\n285.88\n2.318e-01\n\n\n19\n20\n-1.4930\n133.62\n1.378e-01\n\n\n20\n21\n3.2109\n234.85\n1.508e-03\n\n\n21\n22\n-1.7045\n87.35\n9.183e-02\n\n\n22\n23\n0.9968\n55.38\n3.232e-01\n\n\n23\n24\n-0.9994\n2145.13\n3.177e-01\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ncogproc_d &lt;-mapply(cogproc_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)\npost_cohen_d(cogproc_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n-0.2755\n\n\n13\n14\n0.0887\n\n\n14\n15\n0.3007\n\n\n15\n16\n-0.3205\n\n\n16\n17\n0.0733\n\n\n17\n18\n0.3436\n\n\n18\n19\n-0.1329\n\n\n19\n20\n-0.1294\n\n\n20\n21\n0.2477\n\n\n21\n22\n-0.2453\n\n\n22\n23\n0.1405\n\n\n23\n24\n-0.0430\n\n\n\n\n\n\n\n\n\nMean Differences\n\ncogproc_meandiff &lt;- mapply(cogproc_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2019]across all of the months\npost_mean_diff(cogproc_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n-0.6107\n\n\n13\n14\n0.1785\n\n\n14\n15\n0.6095\n\n\n15\n16\n-0.6540\n\n\n16\n17\n0.1560\n\n\n17\n18\n0.7442\n\n\n18\n19\n-0.2962\n\n\n19\n20\n-0.2746\n\n\n20\n21\n0.5305\n\n\n21\n22\n-0.5358\n\n\n22\n23\n0.2776\n\n\n23\n24\n-0.0887"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words-1",
    "href": "coding/t-test/T-tests.html#i-words-1",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\nT-test\n\ni_ttest &lt;- mapply(i_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first paratheses) to t[i] (second parentheses) increasing by 1\npost_pandemic_summary(i_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n-5.1026\n477.85\n4.842e-07\n\n\n13\n14\n2.9683\n362.97\n3.194e-03\n\n\n14\n15\n2.7352\n261.20\n6.661e-03\n\n\n15\n16\n-3.5895\n336.98\n3.805e-04\n\n\n16\n17\n1.7614\n191.52\n7.976e-02\n\n\n17\n18\n3.4394\n240.73\n6.870e-04\n\n\n18\n19\n-2.6019\n255.11\n9.813e-03\n\n\n19\n20\n0.4503\n134.91\n6.532e-01\n\n\n20\n21\n1.5059\n248.77\n1.334e-01\n\n\n21\n22\n2.0159\n84.28\n4.700e-02\n\n\n22\n23\n-3.8068\n57.56\n3.437e-04\n\n\n23\n24\n4.4095\n2135.84\n1.088e-05\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ni_d &lt;- mapply(i_my.d,seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)\npost_cohen_d(i_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n-0.3468\n\n\n13\n14\n0.1902\n\n\n14\n15\n0.1991\n\n\n15\n16\n-0.3758\n\n\n16\n17\n0.1452\n\n\n17\n18\n0.2370\n\n\n18\n19\n-0.3007\n\n\n19\n20\n0.0378\n\n\n20\n21\n0.1020\n\n\n21\n22\n0.2972\n\n\n22\n23\n-0.4622\n\n\n23\n24\n0.1900\n\n\n\n\n\n\n\n\n\nMean Differences\n\ni_meandiff &lt;- mapply(i_mean,seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months\npost_mean_diff(i_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n-0.2878\n\n\n13\n14\n0.1551\n\n\n14\n15\n0.1625\n\n\n15\n16\n-0.3242\n\n\n16\n17\n0.1289\n\n\n17\n18\n0.2083\n\n\n18\n19\n-0.2364\n\n\n19\n20\n0.0329\n\n\n20\n21\n0.0886\n\n\n21\n22\n0.2293\n\n\n22\n23\n-0.3912\n\n\n23\n24\n0.1657"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words-1",
    "href": "coding/t-test/T-tests.html#we-words-1",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nT-test\n\nwe_ttest &lt;- mapply(we_my.t, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\npost_pandemic_summary(we_ttest)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n12\n13\n4.1038\n527.08\n4.709e-05\n\n\n13\n14\n0.9117\n378.82\n3.625e-01\n\n\n14\n15\n-3.3226\n253.14\n1.023e-03\n\n\n15\n16\n2.4647\n373.96\n1.416e-02\n\n\n16\n17\n-0.3375\n197.52\n7.361e-01\n\n\n17\n18\n-4.2759\n229.50\n2.794e-05\n\n\n18\n19\n2.5510\n262.60\n1.131e-02\n\n\n19\n20\n-0.1422\n131.79\n8.871e-01\n\n\n20\n21\n-1.9395\n238.21\n5.362e-02\n\n\n21\n22\n-0.2952\n84.06\n7.685e-01\n\n\n22\n23\n0.8557\n55.76\n3.958e-01\n\n\n23\n24\n-0.3495\n2137.77\n7.267e-01\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nwe_d &lt;- mapply(we_my.d, seq(12,23,1), seq(13,24,1),SIMPLIFY=FALSE)\npost_cohen_d(we_d)\n\n\nSummary of Cohen's D\n\n\nt\nt+1\nCohen's d\n\n\n\n\n12\n13\n0.2639\n\n\n13\n14\n0.0550\n\n\n14\n15\n-0.2595\n\n\n15\n16\n0.2501\n\n\n16\n17\n-0.0256\n\n\n17\n18\n-0.3276\n\n\n18\n19\n0.2920\n\n\n19\n20\n-0.0130\n\n\n20\n21\n-0.1444\n\n\n21\n22\n-0.0436\n\n\n22\n23\n0.1170\n\n\n23\n24\n-0.0151\n\n\n\n\n\n\n\n\n\nMean Differences\n\nwe_meandiff &lt;- mapply(we_mean, seq(12,23,1), seq(13,24,1)) # comparing time zero [3/2020]across all of the months\npost_mean_diff(we_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n12\n13\n0.3778\n\n\n13\n14\n0.0763\n\n\n14\n15\n-0.3676\n\n\n15\n16\n0.3649\n\n\n16\n17\n-0.0365\n\n\n17\n18\n-0.4711\n\n\n18\n19\n0.4169\n\n\n19\n20\n-0.0183\n\n\n20\n21\n-0.2042\n\n\n21\n22\n-0.0609\n\n\n22\n23\n0.1583\n\n\n23\n24\n-0.0210"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thining",
    "href": "coding/t-test/T-tests.html#analytic-thining",
    "title": "T-tests",
    "section": "Analytic Thining",
    "text": "Analytic Thining\n\nT-test\n\nanalytic_ttest_baseline &lt;-mapply(analytic_my.t,0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parantheses) to t[i] (second parentheses)increasing by 1\nbaseline_ttest(analytic_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n1.5025\n1161.5\n1.332e-01\n\n\n0\n2\n0.6860\n1036.8\n4.929e-01\n\n\n0\n3\n0.2508\n245.1\n8.022e-01\n\n\n0\n4\n2.6728\n1120.1\n7.631e-03\n\n\n0\n5\n0.4785\n1004.8\n6.324e-01\n\n\n0\n6\n1.0343\n280.4\n3.019e-01\n\n\n0\n7\n2.6675\n1049.9\n7.760e-03\n\n\n0\n8\n1.4046\n993.4\n1.605e-01\n\n\n0\n9\n1.0147\n328.1\n3.110e-01\n\n\n0\n10\n1.5505\n286.2\n1.221e-01\n\n\n0\n11\n1.9738\n1061.6\n4.867e-02\n\n\n0\n12\n1.3054\n1272.1\n1.920e-01\n\n\n0\n13\n5.7770\n623.9\n1.201e-08\n\n\n0\n14\n5.1516\n929.5\n3.153e-07\n\n\n0\n15\n1.4219\n370.2\n1.559e-01\n\n\n0\n16\n3.9258\n316.9\n1.061e-04\n\n\n0\n17\n3.2572\n918.1\n1.166e-03\n\n\n0\n18\n0.1171\n302.2\n9.068e-01\n\n\n0\n19\n0.8463\n164.4\n3.986e-01\n\n\n0\n20\n3.7364\n920.4\n1.981e-04\n\n\n0\n21\n0.6393\n331.8\n5.231e-01\n\n\n0\n22\n2.6168\n63.2\n1.109e-02\n\n\n0\n23\n3.7687\n1112.0\n1.727e-04\n\n\n0\n24\n2.4326\n1125.2\n1.515e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nanalytic_D_baseline &lt;- mapply(analytic_my.d,0, seq(1,24,1),SIMPLIFY=FALSE) \nbaseline_cohen_d(analytic_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n0.0880\n\n\n0\n2\n0.0330\n\n\n0\n3\n0.0206\n\n\n0\n4\n0.1587\n\n\n0\n5\n0.0235\n\n\n0\n6\n0.0867\n\n\n0\n7\n0.1621\n\n\n0\n8\n0.0687\n\n\n0\n9\n0.0806\n\n\n0\n10\n0.1283\n\n\n0\n11\n0.1024\n\n\n0\n12\n0.0694\n\n\n0\n13\n0.3954\n\n\n0\n14\n0.2534\n\n\n0\n15\n0.1138\n\n\n0\n16\n0.3057\n\n\n0\n17\n0.1588\n\n\n0\n18\n0.0102\n\n\n0\n19\n0.0861\n\n\n0\n20\n0.1803\n\n\n0\n21\n0.0530\n\n\n0\n22\n0.3237\n\n\n0\n23\n0.2019\n\n\n0\n24\n0.1263\n\n\n\n\n\n\n\n\n\nMean Differences\n\nanalytic_mean_baseline &lt;- mapply(analytic_mean, 0, seq(1,24,1)) #across all of the months comparing to time zero\nbaseline_mean_diff(analytic_mean_baseline)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n1.3114\n\n\n0\n2\n0.4935\n\n\n0\n3\n0.3040\n\n\n0\n4\n2.3251\n\n\n0\n5\n0.3412\n\n\n0\n6\n1.3028\n\n\n0\n7\n2.3954\n\n\n0\n8\n0.9976\n\n\n0\n9\n1.1987\n\n\n0\n10\n1.9189\n\n\n0\n11\n1.4369\n\n\n0\n12\n1.0438\n\n\n0\n13\n5.7785\n\n\n0\n14\n3.5880\n\n\n0\n15\n1.7437\n\n\n0\n16\n4.4920\n\n\n0\n17\n2.2602\n\n\n0\n18\n0.1590\n\n\n0\n19\n1.3178\n\n\n0\n20\n2.5943\n\n\n0\n21\n0.8152\n\n\n0\n22\n4.8803\n\n\n0\n23\n2.8046\n\n\n0\n24\n1.8106"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cogproc-1",
    "href": "coding/t-test/T-tests.html#cogproc-1",
    "title": "T-tests",
    "section": "Cogproc",
    "text": "Cogproc\n\nT-test\n\ncogproc_ttest_baseline &lt;- mapply(cogproc_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\nbaseline_ttest(cogproc_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n-0.5097\n1156.51\n6.103e-01\n\n\n0\n2\n-0.7179\n1035.97\n4.730e-01\n\n\n0\n3\n-0.2391\n218.72\n8.112e-01\n\n\n0\n4\n-1.8417\n1119.70\n6.579e-02\n\n\n0\n5\n-0.3764\n1051.94\n7.067e-01\n\n\n0\n6\n0.2442\n282.79\n8.072e-01\n\n\n0\n7\n-1.7142\n1029.21\n8.680e-02\n\n\n0\n8\n-0.9538\n1076.64\n3.404e-01\n\n\n0\n9\n1.0446\n320.31\n2.970e-01\n\n\n0\n10\n-0.8169\n255.26\n4.148e-01\n\n\n0\n11\n-0.7245\n1147.57\n4.689e-01\n\n\n0\n12\n-2.0280\n1307.90\n4.276e-02\n\n\n0\n13\n-5.7012\n609.25\n1.855e-08\n\n\n0\n14\n-6.5911\n924.04\n7.329e-11\n\n\n0\n15\n-0.3856\n395.99\n7.000e-01\n\n\n0\n16\n-4.0812\n298.22\n5.758e-05\n\n\n0\n17\n-5.4650\n949.00\n5.916e-08\n\n\n0\n18\n0.9265\n310.67\n3.549e-01\n\n\n0\n19\n-0.5797\n184.74\n5.628e-01\n\n\n0\n20\n-3.7994\n936.81\n1.544e-04\n\n\n0\n21\n0.7639\n341.61\n4.455e-01\n\n\n0\n22\n-1.3820\n61.97\n1.719e-01\n\n\n0\n23\n-1.0691\n1140.02\n2.853e-01\n\n\n0\n24\n-1.8593\n1172.33\n6.323e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ncogproc_D_baseline &lt;- mapply(cogproc_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)\nbaseline_cohen_d(cogproc_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n-0.0299\n\n\n0\n2\n-0.0345\n\n\n0\n3\n-0.0213\n\n\n0\n4\n-0.1094\n\n\n0\n5\n-0.0180\n\n\n0\n6\n0.0204\n\n\n0\n7\n-0.1048\n\n\n0\n8\n-0.0446\n\n\n0\n9\n0.0841\n\n\n0\n10\n-0.0732\n\n\n0\n11\n-0.0364\n\n\n0\n12\n-0.1070\n\n\n0\n13\n-0.3939\n\n\n0\n14\n-0.3256\n\n\n0\n15\n-0.0298\n\n\n0\n16\n-0.3292\n\n\n0\n17\n-0.2601\n\n\n0\n18\n0.0789\n\n\n0\n19\n-0.0527\n\n\n0\n20\n-0.1809\n\n\n0\n21\n0.0622\n\n\n0\n22\n-0.1778\n\n\n0\n23\n-0.0568\n\n\n0\n24\n-0.0951\n\n\n\n\n\n\n\n\n\nMean Differences\n\ncogproc_mean_baseline &lt;- mapply(cogproc_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months\nbaseline_mean_diff(cogproc_meandiff)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n-0.6107\n\n\n0\n2\n0.1785\n\n\n0\n3\n0.6095\n\n\n0\n4\n-0.6540\n\n\n0\n5\n0.1560\n\n\n0\n6\n0.7442\n\n\n0\n7\n-0.2962\n\n\n0\n8\n-0.2746\n\n\n0\n9\n0.5305\n\n\n0\n10\n-0.5358\n\n\n0\n11\n0.2776\n\n\n0\n12\n-0.0887\n\n\n0\n13\n-0.6107\n\n\n0\n14\n0.1785\n\n\n0\n15\n0.6095\n\n\n0\n16\n-0.6540\n\n\n0\n17\n0.1560\n\n\n0\n18\n0.7442\n\n\n0\n19\n-0.2962\n\n\n0\n20\n-0.2746\n\n\n0\n21\n0.5305\n\n\n0\n22\n-0.5358\n\n\n0\n23\n0.2776\n\n\n0\n24\n-0.0887"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words-2",
    "href": "coding/t-test/T-tests.html#i-words-2",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\nT-test\n\ni_ttest_baseline &lt;- mapply(i_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first paratheseses) to t[i] (second parentheses) increasing by 1\nbaseline_ttest(i_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n-3.3450\n1143.82\n8.495e-04\n\n\n0\n2\n-1.1963\n1155.18\n2.318e-01\n\n\n0\n3\n-0.1911\n213.55\n8.486e-01\n\n\n0\n4\n-4.1439\n1114.31\n3.672e-05\n\n\n0\n5\n-0.6477\n1056.56\n5.173e-01\n\n\n0\n6\n-1.6111\n278.03\n1.083e-01\n\n\n0\n7\n-3.3533\n1035.23\n8.274e-04\n\n\n0\n8\n-2.0582\n1066.96\n3.981e-02\n\n\n0\n9\n-1.4168\n265.19\n1.577e-01\n\n\n0\n10\n-2.7747\n284.30\n5.891e-03\n\n\n0\n11\n-1.9849\n1154.30\n4.739e-02\n\n\n0\n12\n-0.3320\n1263.50\n7.399e-01\n\n\n0\n13\n-5.0280\n571.49\n6.644e-07\n\n\n0\n14\n-3.7093\n958.88\n2.198e-04\n\n\n0\n15\n0.2214\n390.58\n8.249e-01\n\n\n0\n16\n-3.9255\n253.44\n1.116e-04\n\n\n0\n17\n-4.4733\n1005.42\n8.580e-06\n\n\n0\n18\n0.4135\n350.62\n6.795e-01\n\n\n0\n19\n-2.6460\n180.60\n8.864e-03\n\n\n0\n20\n-4.3779\n986.11\n1.326e-05\n\n\n0\n21\n-1.3222\n371.13\n1.869e-01\n\n\n0\n22\n1.3508\n63.34\n1.816e-01\n\n\n0\n23\n-5.6223\n1250.84\n2.322e-08\n\n\n0\n24\n-1.8931\n1254.80\n5.858e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\ni_D_baseline &lt;- mapply(i_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)\nbaseline_cohen_d(i_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n-0.1966\n\n\n0\n2\n-0.0544\n\n\n0\n3\n-0.0174\n\n\n0\n4\n-0.2467\n\n\n0\n5\n-0.0310\n\n\n0\n6\n-0.1358\n\n\n0\n7\n-0.2047\n\n\n0\n8\n-0.0967\n\n\n0\n9\n-0.1296\n\n\n0\n10\n-0.2305\n\n\n0\n11\n-0.0996\n\n\n0\n12\n-0.0177\n\n\n0\n13\n-0.3562\n\n\n0\n14\n-0.1786\n\n\n0\n15\n0.0172\n\n\n0\n16\n-0.3537\n\n\n0\n17\n-0.2047\n\n\n0\n18\n0.0327\n\n\n0\n19\n-0.2453\n\n\n0\n20\n-0.2011\n\n\n0\n21\n-0.1028\n\n\n0\n22\n0.1664\n\n\n0\n23\n-0.2904\n\n\n0\n24\n-0.0945\n\n\n\n\n\n\n\n\n\nMean Differences\n\ni_mean_baseline &lt;- mapply(i_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months\nbaseline_mean_diff(i_mean_baseline)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n-0.1748\n\n\n0\n2\n-0.0504\n\n\n0\n3\n-0.0149\n\n\n0\n4\n-0.2082\n\n\n0\n5\n-0.0266\n\n\n0\n6\n-0.1159\n\n\n0\n7\n-0.1744\n\n\n0\n8\n-0.0846\n\n\n0\n9\n-0.1162\n\n\n0\n10\n-0.1958\n\n\n0\n11\n-0.0843\n\n\n0\n12\n-0.0150\n\n\n0\n13\n-0.3028\n\n\n0\n14\n-0.1477\n\n\n0\n15\n0.0147\n\n\n0\n16\n-0.3094\n\n\n0\n17\n-0.1805\n\n\n0\n18\n0.0278\n\n\n0\n19\n-0.2086\n\n\n0\n20\n-0.1757\n\n\n0\n21\n-0.0871\n\n\n0\n22\n0.1422\n\n\n0\n23\n-0.2490\n\n\n0\n24\n-0.0833"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words-2",
    "href": "coding/t-test/T-tests.html#we-words-2",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nT-test\n\nwe_ttest_baseline &lt;- mapply(we_my.t, 0, seq(1,24,1),SIMPLIFY=FALSE) #compare t (first parathese) to t[i] (second parantheses) increasing by 1\nbaseline_ttest(we_ttest_baseline)\n\n\nSummary of Welch's t-Tests\n\n\nt\nt + 1\nt-value\nDegrees of Freedom\np-value\n\n\n\n\n0\n1\n0.5718\n1161.88\n5.676e-01\n\n\n0\n2\n1.5919\n1008.45\n1.117e-01\n\n\n0\n3\n-1.0685\n214.75\n2.865e-01\n\n\n0\n4\n0.6154\n1116.23\n5.384e-01\n\n\n0\n5\n0.9396\n979.10\n3.476e-01\n\n\n0\n6\n-1.1796\n280.32\n2.392e-01\n\n\n0\n7\n-0.2036\n1067.88\n8.387e-01\n\n\n0\n8\n0.6497\n972.54\n5.160e-01\n\n\n0\n9\n-0.6307\n351.29\n5.286e-01\n\n\n0\n10\n-0.9677\n309.04\n3.340e-01\n\n\n0\n11\n-0.9268\n1073.79\n3.543e-01\n\n\n0\n12\n-0.4002\n1197.17\n6.891e-01\n\n\n0\n13\n3.3604\n676.59\n8.220e-04\n\n\n0\n14\n5.6601\n890.34\n2.040e-08\n\n\n0\n15\n0.4232\n395.82\n6.724e-01\n\n\n0\n16\n3.3898\n317.82\n7.876e-04\n\n\n0\n17\n5.1356\n889.20\n3.457e-07\n\n\n0\n18\n-0.7164\n361.98\n4.742e-01\n\n\n0\n19\n2.3094\n191.38\n2.199e-02\n\n\n0\n20\n4.1802\n873.54\n3.205e-05\n\n\n0\n21\n0.8667\n390.06\n3.866e-01\n\n\n0\n22\n0.2288\n64.77\n8.198e-01\n\n\n0\n23\n2.5427\n1081.13\n1.114e-02\n\n\n0\n24\n2.2873\n1080.95\n2.237e-02\n\n\n\n\n\n\n\n\n\nCohen‚Äôs D\n\nwe_D_baseline &lt;- mapply(we_my.d, 0, seq(1,24,1),SIMPLIFY=FALSE)\nbaseline_cohen_d(we_D_baseline)\n\n\nSummary of Cohen's D\n\n\nt\nt + 1\nCohen's d\n\n\n\n\n0\n1\n0.0334\n\n\n0\n2\n0.0778\n\n\n0\n3\n-0.0967\n\n\n0\n4\n0.0362\n\n\n0\n5\n0.0469\n\n\n0\n6\n-0.0989\n\n\n0\n7\n-0.0123\n\n\n0\n8\n0.0322\n\n\n0\n9\n-0.0483\n\n\n0\n10\n-0.0764\n\n\n0\n11\n-0.0479\n\n\n0\n12\n-0.0216\n\n\n0\n13\n0.2229\n\n\n0\n14\n0.2874\n\n\n0\n15\n0.0327\n\n\n0\n16\n0.2636\n\n\n0\n17\n0.2567\n\n\n0\n18\n-0.0557\n\n\n0\n19\n0.2040\n\n\n0\n20\n0.2103\n\n\n0\n21\n0.0657\n\n\n0\n22\n0.0271\n\n\n0\n23\n0.1374\n\n\n0\n24\n0.1205\n\n\n\n\n\n\n\n\n\nMean Differences\n\nwe_mean_baseline &lt;- mapply(we_mean, 0, seq(1,24,1)) # comparing time zero [3/2020]across all of the months\nbaseline_mean_diff(we_mean_baseline)\n\n\nSummary of Mean Differences\n\n\nt\nt+1\nMean Difference\n\n\n\n\n0\n1\n0.0531\n\n\n0\n2\n0.1227\n\n\n0\n3\n-0.1575\n\n\n0\n4\n0.0545\n\n\n0\n5\n0.0718\n\n\n0\n6\n-0.1605\n\n\n0\n7\n-0.0191\n\n\n0\n8\n0.0495\n\n\n0\n9\n-0.0766\n\n\n0\n10\n-0.1218\n\n\n0\n11\n-0.0731\n\n\n0\n12\n-0.0335\n\n\n0\n13\n0.3443\n\n\n0\n14\n0.4207\n\n\n0\n15\n0.0531\n\n\n0\n16\n0.4180\n\n\n0\n17\n0.3815\n\n\n0\n18\n-0.0896\n\n\n0\n19\n0.3273\n\n\n0\n20\n0.3090\n\n\n0\n21\n0.1048\n\n\n0\n22\n0.0439\n\n\n0\n23\n0.2022\n\n\n0\n24\n0.1813"
  },
  {
    "objectID": "coding/t-test/T-tests.html#analytic-thinking-2",
    "href": "coding/t-test/T-tests.html#analytic-thinking-2",
    "title": "T-tests",
    "section": "Analytic Thinking",
    "text": "Analytic Thinking\n\nAnalytic &lt;- ggplot(data=df2, aes(x=Date_mean, y=Analytic_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=Analytic_mean-Analytic_std.error, ymax=Analytic_mean+Analytic_std.error), alpha=0.2) +\n  ggtitle(\"Analytic Thinking\") +\n  labs(x = \"Month\", y = 'Standardized score') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nAnalytic &lt;- Analytic + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=43,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=43,label=\"Winter 2020 surge\", size = 3)\nAnalytic"
  },
  {
    "objectID": "coding/t-test/T-tests.html#cogproc-2",
    "href": "coding/t-test/T-tests.html#cogproc-2",
    "title": "T-tests",
    "section": "Cogproc",
    "text": "Cogproc\n\nCogproc &lt;- ggplot(data=df2, aes(x=Date_mean, y=cogproc_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=cogproc_mean-cogproc_std.error, ymax=cogproc_mean+cogproc_std.error), alpha=0.2) +\n  ggtitle(\"Cognitive Processing\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nCogproc &lt;- Cogproc + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=12.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=12.5,label=\"Winter 2020 surge\", size = 3)\nCogproc"
  },
  {
    "objectID": "coding/t-test/T-tests.html#i-words-3",
    "href": "coding/t-test/T-tests.html#i-words-3",
    "title": "T-tests",
    "section": "I-words",
    "text": "I-words\n\ni &lt;- ggplot(data=df2, aes(x=Date_mean, y=i_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=i_mean-i_std.error, ymax=i_mean+i_std.error), alpha=0.2) +\n  ggtitle(\"I-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\ni &lt;- i + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=1.95,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=1.95,label=\"Winter 2020 surge\", size = 3)\ni"
  },
  {
    "objectID": "coding/t-test/T-tests.html#we-words-3",
    "href": "coding/t-test/T-tests.html#we-words-3",
    "title": "T-tests",
    "section": "We-words",
    "text": "We-words\n\nwe &lt;- ggplot(data=df2, aes(x=Date_mean, y=we_mean, group=1)) +\n  geom_line(colour = \"dodgerblue3\") +\n  scale_x_date(date_breaks = \"3 month\", date_labels = \"%Y-%m\") +\n  geom_ribbon(aes(ymin=we_mean-we_std.error, ymax=we_mean+we_std.error), alpha=0.2) +\n  ggtitle(\"We-usage\") +\n  labs(x = \"Month\", y = '% Total Words') +\n  plot_aes + #here's our plot aes object\n  geom_vline(xintercept = as.numeric(as.Date(\"2020-03-01\")), linetype = 1) +\n  geom_rect(data = df2, #summer surge\n            aes(xmin = as.Date(\"2020-06-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2020-07-20\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009) +\n  geom_rect(data = df2, #winter surge\n            aes(xmin = as.Date(\"2020-11-15\", \"%Y-%m-%d\"), \n                xmax = as.Date(\"2021-01-01\",  \"%Y-%m-%d\"),\n                ymin = -Inf, \n                ymax = Inf),\n            fill = \"gray\", \n            alpha = 0.009)\nwe &lt;- we + annotate(geom=\"text\",x=as.Date(\"2020-07-01\"),\n                                y=6.5,label=\"Summer 2020 surge\", size = 3) + \n  annotate(geom=\"text\",x=as.Date(\"2020-12-03\"),\n           y=6.5,label=\"Winter 2020 surge\", size = 3)\nwe"
  },
  {
    "objectID": "coding/t-test/T-tests.html#tie-them-all-together",
    "href": "coding/t-test/T-tests.html#tie-them-all-together",
    "title": "T-tests",
    "section": "Tie them all together",
    "text": "Tie them all together\n\ngraphs &lt;- ggpubr::ggarrange(Analytic,Cogproc,i,we,ncol=2, nrow=2, common.legend = TRUE, legend = \"bottom\")\nannotate_figure(graphs,\n                top = text_grob(\"CEOs' Language Change\",  color = \"black\", face = \"bold\", size = 20),\n                bottom = text_grob(\"Note. Vertical Line Represents the onset of the pandemic. \\n\\ Horizontal shading represents Standard Error. Vertical bars represent virus surges.\"\n                                   , color = \"Black\",\n                                   hjust = 1.1, x = 1, face = \"italic\", size = 16))"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "CV",
    "section": "",
    "text": "Department of Psychology\nPrinceton University\nsm9518@princeton.edu\n\n\n\nPrinceton University (2024-Present)\nPh.D.¬†in Psychology\nAdvisor: Dr.¬†Erik Nook\nTexas State University (2019-2022)\nM.A.¬†in Psychological Research\nAdvisor: Dr.¬†Randall Osborne\nSouthwestern University (2015-2019)\nB.A. in Psychology (Minor: Spanish)\n\n\n\n\n* Denotes Shared First Authorship\nMesquiti, S., Cosme, D., Nook, E.C., Falk E.B., Burns S. (Under Revision). Predicting Psychological and Subjective Well-being through Language-based Assessments of Well-being.\nMesquiti, S.,* Seraj, S.,* Weyland, A. H., Ashokkumar, A., Boyd, R. L., Mihalcea, R., & Pennebaker, J. W. (2025). Analysis of social media language reveals the psychological interaction of three successive upheavals. Scientific Reports, 15(1), 5740. DOI.\nKang, Y., Mesquiti, S., Baik, E.S., & Falk, E. B. (2024). Empathy and Helping: The Role of Affect in Response to Others‚Äô Suffering. Scientific Reports, 15, 3256 DOI.\nMesquiti, S., Seraj, S. (2023). The Psychological Impacts of the COVID-19 Pandemic on Business Leadership. PLOS One. DOI\n\n\n\n\n\nKang, Y., Mesquiti, S. & Falk, E. B. (in prep). The Effect of Compassion Training on Emotional Support Giving.\nMesquiti, S. & Nook, E.C. (in prep.) The Implication of Artificial Intelligence on Mental Health.\nMesquiti, S., Stade E. C., Hull, T.D., Nook, E. C. (in prep.) Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset.\n\n\n\n\n\n\n2025: Princeton University Data Driven Social Science Graduate Fellowship ($1,200)\n2025: Princeton University Data Driven Social Science: Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments ($4,283)\n2025: New Jersey Health Foundation: Investigating Racial Equity of Psycholinguistic Measures of Mental Health (PI: Erik C. Nook; $34,867; # PC 42-25)\n2022: Northwestern University Kellogg School of Management Behavioral Research Fellowship (Finalist [Declined]; $35,000)\n2020: Texas State University Thesis Research Support Fellowship ($1,070)\n\n\n\n\n\n\n2025: Princeton University Data Driven Social Science Graduate Fellowship\n2024-25: Princeton University William G. Bowen Merit Fellowship\n2020-21: Texas State University Graduate College of Liberal Arts Scholarship\n2021: Masters in Psychological Research Graduate Competitive Scholarship\n2019: Texas State University Graduate College of Education Scholarship\n2015: Southwestern University Mood Scholar\n\n\n\n\n\n\n2025: Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset. Social Psychology Seminar, Princeton University, Princeton, NJ.\n2022: Within-and between-person effects of autonomous motivation on goal pursuit. Duckworth Lab, Princeton University.\n2021: The whole truth and nothing but the truth: an analysis of the variability of prosocial lying in the Netherlands and the United States. Texas State University Psychology Department Brown Bag.\n2021: The truth about prosocial lying: Cross-cultural differences in prosocial lying. Southwestern Psychological Association (Virtual).\n\n\n\n\n\n* Denotes Undergraduate Mentored\n\nMesquiti, S. C., Nook, E. (2024, March). Investigating Racial Sensitivity in Language-Based Assessments of Mental Health. Poster presented at the annual meeting for the Society for Affective Science.\nMesquiti, S. C., Cosme, D., Nook, E., Falk E., Burns S. (2025, February). Predicting Well-being with Language-based Assessments. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Burns S., Cosme, D., Falk E. (2024, February). Enhancing Psychological Well-being Prediction Through Natural Language Analysis. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Mobasser, A., Falk, E., Pfeifer, J. H., & Cosme, D (2023, February). Within-and between-person effects of autonomous motivation on goal pursuit. Poster submitted to the annual meeting for the Society for Personality and Social Psychology.\nSpehar, A.,* Muzekari, B., Butler, T. B., Mesquiti, S.C., Cosme, D., Falk, E. (2022, August). Relating Collective and Self-Focus Language in Social Support with Self-Reported Depression. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa. Github\nAndrews, M. E., Cooper, N., Paul, A., Johnson, D., Muzekari, B., Mesquiti, S.C., Torres, O., Resnick, A., Scholz, C., Mattan, B., Barnet, I., Henriksen, L., Strasser, A., & Falk, E. B. (2023, May). Compounding effect of microaggressions and exposure to tobacco advertising on stress and smoking. Annual conference of the International Communication Association, Toronto, Ontario, Canada.\nSantana, C.,* Mesquiti, S.C., Carreras-Tartak, J., Kang, Y., Falk, E. (2022, August). Relating Collective and Self-Focus Language in Social Support with Self-Reported Depression. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nWoolfolk, Z*., Cosme, D., Butler, T., Carreras-Tartak, J., Mesquiti, S.C., Kang, Y., Falk, E. (2022, August). Racial Homophily in Emotion Sharing Network. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nMesquiti, S.C. (2022, April). Exploring Cognitive Language of CEOs during the COVID-19 Pandemic Over Time and its Connection to Decision Processing. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Tsai, J. L., Marion, J., Olivarez, O., Blackburn, K., Durland, M. (2022, February). The linguistic lifespan of a CEO: Exploring the way cognitive language unfolds over time and its connection to decision processing. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nHaskard-Zolnierek, K., Mesquiti, S.C., & Snyder, M. (2022, February). Associations between patient satisfaction and physician and nurse word use. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C., Kok, R., Clegg, J. M., Warnell, K. R. (2021, April). The truth behind prosocial lies: A linguistic analysis of lying to be polite. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Domer, K., Warnell, K. R., Clegg, J. M. (2021, February). More than a disappointing gift: Examining variability in when and how we prosocially lie. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nDomer, K., Mesquiti, S.C., Warnell, K. R., Clegg, J. M. (2021, February). More than a social norm: Examining who we lie to and what we lie about. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C. (2019, April). Internship at the Central Texas Treatment Center. Poster presented at Southwestern University Creative Works Symposium, Georgetown, Tx.\n\n\n\n\n\n\nEmi Yun | Thesis Student | Princeton University (2024-Present)\nArden Spehar | Summer MindCORE REU Student | University of Pennsylvania (2022)\nCarlos Santana | Summer MindCORE REU Student | University of Pennsylvania (2022)\n\n\n\n\n\n\nEmily Falk\nProfessor of Communication, Psychology, Marketing, and OID (Operations, Informatics, and Decisions), Vice Dean of the Annenberg School for Communication, University of Pennsylvania\nefalk@falklab.org\nJames Pennebaker\nProfessor Emeritus of Psychology, The University of Texas at Austin\npennebaker@utexas.edu\nErik Nook\nAssistant Professor of Psychology, Princeton University\nenook@princeton.edu"
  },
  {
    "objectID": "CV.html#education",
    "href": "CV.html#education",
    "title": "CV",
    "section": "",
    "text": "Princeton University (2024-Present)\nPh.D.¬†in Psychology\nAdvisor: Dr.¬†Erik Nook\nTexas State University (2019-2022)\nM.A.¬†in Psychological Research\nAdvisor: Dr.¬†Randall Osborne\nSouthwestern University (2015-2019)\nB.A. in Psychology (Minor: Spanish)"
  },
  {
    "objectID": "CV.html#publications",
    "href": "CV.html#publications",
    "title": "CV",
    "section": "",
    "text": "* Denotes Shared First Authorship\nMesquiti, S., Cosme, D., Nook, E.C., Falk E.B., Burns S. (Under Revision). Predicting Psychological and Subjective Well-being through Language-based Assessments of Well-being.\nMesquiti, S.,* Seraj, S.,* Weyland, A. H., Ashokkumar, A., Boyd, R. L., Mihalcea, R., & Pennebaker, J. W. (2025). Analysis of social media language reveals the psychological interaction of three successive upheavals. Scientific Reports, 15(1), 5740. DOI.\nKang, Y., Mesquiti, S., Baik, E.S., & Falk, E. B. (2024). Empathy and Helping: The Role of Affect in Response to Others‚Äô Suffering. Scientific Reports, 15, 3256 DOI.\nMesquiti, S., Seraj, S. (2023). The Psychological Impacts of the COVID-19 Pandemic on Business Leadership. PLOS One. DOI"
  },
  {
    "objectID": "CV.html#works-in-progress",
    "href": "CV.html#works-in-progress",
    "title": "CV",
    "section": "",
    "text": "Kang, Y., Mesquiti, S. & Falk, E. B. (in prep). The Effect of Compassion Training on Emotional Support Giving.\nMesquiti, S. & Nook, E.C. (in prep.) The Implication of Artificial Intelligence on Mental Health.\nMesquiti, S., Stade E. C., Hull, T.D., Nook, E. C. (in prep.) Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset."
  },
  {
    "objectID": "CV.html#funding",
    "href": "CV.html#funding",
    "title": "CV",
    "section": "",
    "text": "2025: Princeton University Data Driven Social Science Graduate Fellowship ($1,200)\n2025: Princeton University Data Driven Social Science: Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments ($4,283)\n2025: New Jersey Health Foundation: Investigating Racial Equity of Psycholinguistic Measures of Mental Health (PI: Erik C. Nook; $34,867; # PC 42-25)\n2022: Northwestern University Kellogg School of Management Behavioral Research Fellowship (Finalist [Declined]; $35,000)\n2020: Texas State University Thesis Research Support Fellowship ($1,070)"
  },
  {
    "objectID": "CV.html#fellowships",
    "href": "CV.html#fellowships",
    "title": "CV",
    "section": "",
    "text": "2025: Princeton University Data Driven Social Science Graduate Fellowship\n2024-25: Princeton University William G. Bowen Merit Fellowship\n2020-21: Texas State University Graduate College of Liberal Arts Scholarship\n2021: Masters in Psychological Research Graduate Competitive Scholarship\n2019: Texas State University Graduate College of Education Scholarship\n2015: Southwestern University Mood Scholar"
  },
  {
    "objectID": "CV.html#invited-talks",
    "href": "CV.html#invited-talks",
    "title": "CV",
    "section": "",
    "text": "2025: Tracking Shifts in the Latent Meaning of ‚ÄúI‚Äù and Its Connection to Mental Health Outcomes in a Large Therapy Dataset. Social Psychology Seminar, Princeton University, Princeton, NJ.\n2022: Within-and between-person effects of autonomous motivation on goal pursuit. Duckworth Lab, Princeton University.\n2021: The whole truth and nothing but the truth: an analysis of the variability of prosocial lying in the Netherlands and the United States. Texas State University Psychology Department Brown Bag.\n2021: The truth about prosocial lying: Cross-cultural differences in prosocial lying. Southwestern Psychological Association (Virtual)."
  },
  {
    "objectID": "CV.html#conference-presentations",
    "href": "CV.html#conference-presentations",
    "title": "CV",
    "section": "",
    "text": "* Denotes Undergraduate Mentored\n\nMesquiti, S. C., Nook, E. (2024, March). Investigating Racial Sensitivity in Language-Based Assessments of Mental Health. Poster presented at the annual meeting for the Society for Affective Science.\nMesquiti, S. C., Cosme, D., Nook, E., Falk E., Burns S. (2025, February). Predicting Well-being with Language-based Assessments. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Burns S., Cosme, D., Falk E. (2024, February). Enhancing Psychological Well-being Prediction Through Natural Language Analysis. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nMesquiti, S. C., Mobasser, A., Falk, E., Pfeifer, J. H., & Cosme, D (2023, February). Within-and between-person effects of autonomous motivation on goal pursuit. Poster submitted to the annual meeting for the Society for Personality and Social Psychology.\nSpehar, A.,* Muzekari, B., Butler, T. B., Mesquiti, S.C., Cosme, D., Falk, E. (2022, August). Relating Collective and Self-Focus Language in Social Support with Self-Reported Depression. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa. Github\nAndrews, M. E., Cooper, N., Paul, A., Johnson, D., Muzekari, B., Mesquiti, S.C., Torres, O., Resnick, A., Scholz, C., Mattan, B., Barnet, I., Henriksen, L., Strasser, A., & Falk, E. B. (2023, May). Compounding effect of microaggressions and exposure to tobacco advertising on stress and smoking. Annual conference of the International Communication Association, Toronto, Ontario, Canada.\nSantana, C.,* Mesquiti, S.C., Carreras-Tartak, J., Kang, Y., Falk, E. (2022, August). Relating Collective and Self-Focus Language in Social Support with Self-Reported Depression. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nWoolfolk, Z*., Cosme, D., Butler, T., Carreras-Tartak, J., Mesquiti, S.C., Kang, Y., Falk, E. (2022, August). Racial Homophily in Emotion Sharing Network. Poster presented at the annual University of Pennsylvania MindCORE student show case. Philadelphia, Pa.\nMesquiti, S.C. (2022, April). Exploring Cognitive Language of CEOs during the COVID-19 Pandemic Over Time and its Connection to Decision Processing. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Tsai, J. L., Marion, J., Olivarez, O., Blackburn, K., Durland, M. (2022, February). The linguistic lifespan of a CEO: Exploring the way cognitive language unfolds over time and its connection to decision processing. Poster presented at the annual meeting for the Society for Personality and Social Psychology.\nHaskard-Zolnierek, K., Mesquiti, S.C., & Snyder, M. (2022, February). Associations between patient satisfaction and physician and nurse word use. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C., Kok, R., Clegg, J. M., Warnell, K. R. (2021, April). The truth behind prosocial lies: A linguistic analysis of lying to be polite. Poster presented at the annual Texas State University Psychology Department Showcase. San Marcos, Tx.\nMesquiti, S.C., Domer, K., Warnell, K. R., Clegg, J. M. (2021, February). More than a disappointing gift: Examining variability in when and how we prosocially lie. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nDomer, K., Mesquiti, S.C., Warnell, K. R., Clegg, J. M. (2021, February). More than a social norm: Examining who we lie to and what we lie about. Poster presented at the annual meeting for the Society for Personality and Social Psychology (virtual).\nMesquiti, S.C. (2019, April). Internship at the Central Texas Treatment Center. Poster presented at Southwestern University Creative Works Symposium, Georgetown, Tx."
  },
  {
    "objectID": "CV.html#mentorship",
    "href": "CV.html#mentorship",
    "title": "CV",
    "section": "",
    "text": "Emi Yun | Thesis Student | Princeton University (2024-Present)\nArden Spehar | Summer MindCORE REU Student | University of Pennsylvania (2022)\nCarlos Santana | Summer MindCORE REU Student | University of Pennsylvania (2022)"
  },
  {
    "objectID": "CV.html#references",
    "href": "CV.html#references",
    "title": "CV",
    "section": "",
    "text": "Emily Falk\nProfessor of Communication, Psychology, Marketing, and OID (Operations, Informatics, and Decisions), Vice Dean of the Annenberg School for Communication, University of Pennsylvania\nefalk@falklab.org\nJames Pennebaker\nProfessor Emeritus of Psychology, The University of Texas at Austin\npennebaker@utexas.edu\nErik Nook\nAssistant Professor of Psychology, Princeton University\nenook@princeton.edu"
  },
  {
    "objectID": "coding.html",
    "href": "coding.html",
    "title": "Coding Help",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSimple Correlation examples for students\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nT-tests\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nRegression\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nüé® R Data Visualization Adventure\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications and Preprints",
    "section": "",
    "text": "Publications and Preprints\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nEmpathy and helping: the role of affect in response to others‚Äô suffering\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of social media language reveals the psychological interaction of three successive upheavals\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Psychological and Subjective Well-being through Language-based Assessment\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe psychological impacts of the COVID-19 pandemic on business leadership\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/the-psychological-impacts-of-the-covid-19-pandemic-on-business-leadership/index.html",
    "href": "publications/the-psychological-impacts-of-the-covid-19-pandemic-on-business-leadership/index.html",
    "title": "The psychological impacts of the COVID-19 pandemic on business leadership",
    "section": "",
    "text": "Abstract\nThe COVID-19 pandemic had a profound impact on business leadership, specifically on chief executive officers (CEOs). To document the psychological impacts of the pandemic on corporate leadership, this study analyzed the language of CEOs during company quarterly earnings calls (N=19,536) one year before and after the onset of the pandemic. Following the start of lockdowns, CEOs exhibited significant language shifts. Analytic thinking declined, and their language became less technical and more personal and intuitive. CEOs also showed signs of increased cognitive load as they grappled with the pandemic‚Äôs impact on their business practices. The study observed a substantial decrease in collective-focused language (we-usage) among CEOs, indicative of disconnection from their companies. Concurrently, there was an increase in self-focused (I-usage) language, suggesting heightened preoccupation among business leaders. The observed language changes reflect the unique effect of the pandemic on CEOs, which had some notable differences compared to the general population. This study sheds light on how the COVID-19 pandemic influenced business leaders‚Äô psychological states and decision-making strategies‚Äîprocesses that have a substantial impact on a company‚Äôs performance. The findings underscore the importance of language data in understanding large-scale societal events.\nCitation: Mesquiti, S., & Seraj, S. (2023). The psychological impacts of the COVID-19 pandemic on business leadership. PLoS ONE, 18(10), e0290621. https://doi.org/10.1371/journal.pone.0290621"
  },
  {
    "objectID": "publications/analysis-of-social-media-language-reveals-the-psychological-interaction-of-three-successive-upheavals/index.html",
    "href": "publications/analysis-of-social-media-language-reveals-the-psychological-interaction-of-three-successive-upheavals/index.html",
    "title": "Analysis of social media language reveals the psychological interaction of three successive upheavals",
    "section": "",
    "text": "Abstract\nUsing social media data, the present study documents how three successive upheavals: the COVID pandemic, the Black Lives Matter (BLM) protests of 2020, and the US Supreme Court decision to overturn Roe v. Wade interacted to impact the cognitive, emotional, and social styles of people in the US. Text analyses were conducted on 45,225,895 Reddit comments from 2,451,289 users and 889,402 news headlines from four news sources. Results revealed significant shifts in language related to self-focus (e.g., first-person singular pronouns), collective-focus (e.g., first-person plural pronouns), negative emotion (anxiety and anger words), and engagement (e.g., discussion of upheaval-related topics) after each event. Language analyses captured how social justice-related upheavals (BLM, Roe v. Wade) may have affected people in different ways emotionally than those that affected them personally (COVID). The onset of COVID was related to people becoming increasingly anxious and people turned inward to focus on their personal situations. However, BLM and the overturning of Roe v. Wade aroused anger and action, as people may have looked beyond themselves to address these issues. Analysis of upheaval-related discussions captured the public‚Äôs sustained interest in BLM and COVID, whereas interest in Roe v. Wade declined relatively quickly. Shifts in discussions also showed how events interacted as people focused on only one national event at a time, with interest in other events dampening when a new event occurred. The findings underscore the dynamic nature of culturally shared events that are apparent in everyday online language use.\nCitation: Mesquiti, S., Seraj, S., Weyland, A. H., Ashokkumar, A., Boyd, R. L., Mihalcea, R., & Pennebaker, J. W. (2025). Analysis of social media language reveals the psychological interaction of three successive upheavals. Scientific Reports, 15, 5740. https://doi.org/10.1038/s41598-025-89165-z"
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html",
    "href": "posts/LLM on HPC/LLM-to-HPC.html",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "This guide walks you through configuring your environment, authenticating access, and downloading the LLaMA 3 models from Hugging Face on the Adroit HPC system. It assumes you have basic familiarity with command line, Python, and HPC usage.\nThis process is infinitely easier if you have connected VSCode to the adroit cluster. Here is some info on that. IT also runs help sessions on this, which are very useful.\n\n\nBy default, Hugging Face stores downloaded models in your home directory (e.g., /home/username/.cache/huggingface), which may have limited storage on HPC systems. Redirect the cache to your scratch directory for ample space.\nYou can do that by using the checkquota command in the terminal to find out how much space you have in your scratch directory.\n\nSet the environment variable HF_HOME to your scratch directory:\n\nOn the Adroit login node, run this command to append the export statement to your .bashrc:\necho \"export HF_HOME=/scratch/network/$USER/.cache/huggingface/\" &gt;&gt; $HOME/.bashrc\n\nReload your shell configuration:\n\nsource ~/.bashrc\n\nVerify the variable is set:\n\necho $HF_HOME\nThe excpected output should be:\n/scratch/network/&lt;YourNetID&gt;/.cache/huggingface/\n\n\n\nMeta requires users to accept a license and gain explicit access to the LLaMA 3 models on Hugging Face. So, this means you‚Äôll need sign up for a Hugging Face account and request access to the LLaMA 3 models.\n\nGo to the LLaMA 3 model page on Hugging Face: https://huggingface.co/meta-llama/Llama-3.1-8B (or whatever model you want access to)\nLog in or create a Hugging Face account if you haven‚Äôt already.\nAccept the model license terms: Click the ‚ÄúAccess repository‚Äù button and agree to the license to request access.\nWait for access to be granted. This should be relatively quick, but may take a few minutes to a few hours depending on demand.\n\n\n\n\nOnce access is granted, authenticate your HPC environment to allow downloading protected models.\n\nLog in to Hugging Face CLI:\n\nOn the Adroit login node, run the following command:\nhuggingface-cli login\n\nEnter your Hugging Face token:\n\nYou will be prompted to enter your Hugging Face access token. You can find this token in your Hugging Face account settings under ‚ÄúAccess Tokens‚Äù. Copy and paste it into the terminal when prompted. Make sure not to share this with anyone since this is a personal access token that allows downloading models.\n\n\n\nNow that you have authenticated, you can download the LLaMA 3 model to your scratch directory.\n\nCreate a Python script download_llama3.py with this content:\n\nMake sure to replace meta-llama/Llama-3.1-8B with the specific model you want\nto download if different. Also, make sure you have transformers installed in your Python environment.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"meta-llama/Llama-3.1-8B\"\ncache_path = \"/scratch/network/sm9518/.cache/huggingface\"  # replace with your actual NetID\n\n# Download model and tokenizer to cache\nAutoTokenizer.from_pretrained(model_id, cache_dir=cache_path)\nAutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_path)\n\nprint(f\"{model_id} Downloaded Successfully! to {cache_path}\")\n\nRun the script on the login node:\n\npython download_llama3.py\n\nThis will download all necessary model files into your scratch cache directory set by HF_HOME.\n\n\n\n\nNow that you have downloaded the LLaMA 3 model to your scratch directory, you can run inference on an HPC compute node.\n\n\nSave the following code to /scratch/network/$USER/python_test/run_test_llama.py. This script loads the model and runs a short text generation example using the transformers pipeline API.\nfrom transformers import pipeline\nimport torch\nimport os\n\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif not torch.cuda.is_available():\n    raise ValueError(\n        \"CUDA is not available. Make sure you are running this on a GPU node. \"\n        \"For example, run with Slurm requesting GPU:\\n\\n\"\n        \"\\tsalloc -t 0:10:00 --ntasks=1 --gres=gpu:1 python run_test_llama.py\"\n    )\n\nmodel_path = \"/scratch/network/$USER/.cache/huggingface/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n\nprint(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n\npipe = pipeline(\"text-generation\", model=model_path, tokenizer=model_path)\n\nprompt = \"You are an expert psychologist. Tell me something interesting about psychology regarding Erik Nook's research:\"\noutput = pipe(prompt, max_new_tokens=50)\n\nprint(\"\\nModel output:\\n\", output)\n\nMake sure to replace $USER in the path with your actual NetID or use a variable if running programmatically.\n\n\n\n\nThis Slurm batch script requests one A100 GPU on the Adroit cluster and runs the above Python test script.\n#!/bin/bash\n#SBATCH --job-name=llama3-textgen           # Job name\n#SBATCH --nodes=1                           # Use one node\n#SBATCH --ntasks=1                          # One task\n#SBATCH --cpus-per-task=1                   # One CPU core\n#SBATCH --mem=36G                          # Memory request # can be ess\n#SBATCH --gres=gpu:1                        # Request 1 GPU\n#SBATCH --time=00:10:00                     # Max runtime (adjust as needed)\n#SBATCH --constraint=a100                   # Use A100 GPU\n#SBATCH --nodelist=adroit-h11g1             # Run on node with free GPUs\n#SBATCH --mail-type=ALL                     # Email on start, end, fail\n#SBATCH --mail-user=sm9518@princeton.edu   # Your email\n\nmodule purge\nmodule load anaconda3/2024.6\nmodule load cudatoolkit/11.8 \n\nsource activate talkspaceMADS\n\ncd /scratch/network/$USER/python_test\n\necho \"Job started at $(date)\"\n\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\npython run_test_llama.py\n\necho \"Job completed at $(date)\"\n\nüí° Check GPU Node Usage Before Selecting a Node or GPU.\nBefore submitting your job or manually specifying a GPU node (e.g., with #SBATCH --nodelist=adroit-h11g1).\nIt‚Äôs a good idea to check which nodes and GPUs have free memory or are under low load. Otherwise, your job might be assigned to a GPU that is already fully used, causing CUDA out-of-memory errors.\n\nOn Adroit, you can use commands like these from the login node to check GPU availability:\n# Show GPU status and free GPUs per node \nshownodes -p gpu\nIf you don‚Äôt specify a node, Slurm will pick one for you, but it might not always be the best choice if GPUs on that node are busy."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-1-configure-the-hugging-face-cache-directory",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-1-configure-the-hugging-face-cache-directory",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "By default, Hugging Face stores downloaded models in your home directory (e.g., /home/username/.cache/huggingface), which may have limited storage on HPC systems. Redirect the cache to your scratch directory for ample space.\nYou can do that by using the checkquota command in the terminal to find out how much space you have in your scratch directory.\n\nSet the environment variable HF_HOME to your scratch directory:\n\nOn the Adroit login node, run this command to append the export statement to your .bashrc:\necho \"export HF_HOME=/scratch/network/$USER/.cache/huggingface/\" &gt;&gt; $HOME/.bashrc\n\nReload your shell configuration:\n\nsource ~/.bashrc\n\nVerify the variable is set:\n\necho $HF_HOME\nThe excpected output should be:\n/scratch/network/&lt;YourNetID&gt;/.cache/huggingface/"
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-2-get-authentication-access-from-meta-required-for-llama-models",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-2-get-authentication-access-from-meta-required-for-llama-models",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Meta requires users to accept a license and gain explicit access to the LLaMA 3 models on Hugging Face. So, this means you‚Äôll need sign up for a Hugging Face account and request access to the LLaMA 3 models.\n\nGo to the LLaMA 3 model page on Hugging Face: https://huggingface.co/meta-llama/Llama-3.1-8B (or whatever model you want access to)\nLog in or create a Hugging Face account if you haven‚Äôt already.\nAccept the model license terms: Click the ‚ÄúAccess repository‚Äù button and agree to the license to request access.\nWait for access to be granted. This should be relatively quick, but may take a few minutes to a few hours depending on demand."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-3-log-in-to-hugging-face-cli-on-hpc",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-3-log-in-to-hugging-face-cli-on-hpc",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Once access is granted, authenticate your HPC environment to allow downloading protected models.\n\nLog in to Hugging Face CLI:\n\nOn the Adroit login node, run the following command:\nhuggingface-cli login\n\nEnter your Hugging Face token:\n\nYou will be prompted to enter your Hugging Face access token. You can find this token in your Hugging Face account settings under ‚ÄúAccess Tokens‚Äù. Copy and paste it into the terminal when prompted. Make sure not to share this with anyone since this is a personal access token that allows downloading models."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-4-download-the-llama-3-model-on-the-login-node",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-4-download-the-llama-3-model-on-the-login-node",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Now that you have authenticated, you can download the LLaMA 3 model to your scratch directory.\n\nCreate a Python script download_llama3.py with this content:\n\nMake sure to replace meta-llama/Llama-3.1-8B with the specific model you want\nto download if different. Also, make sure you have transformers installed in your Python environment.\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"meta-llama/Llama-3.1-8B\"\ncache_path = \"/scratch/network/sm9518/.cache/huggingface\"  # replace with your actual NetID\n\n# Download model and tokenizer to cache\nAutoTokenizer.from_pretrained(model_id, cache_dir=cache_path)\nAutoModelForCausalLM.from_pretrained(model_id, cache_dir=cache_path)\n\nprint(f\"{model_id} Downloaded Successfully! to {cache_path}\")\n\nRun the script on the login node:\n\npython download_llama3.py\n\nThis will download all necessary model files into your scratch cache directory set by HF_HOME."
  },
  {
    "objectID": "posts/LLM on HPC/LLM-to-HPC.html#step-5-test-the-downloaded-model",
    "href": "posts/LLM on HPC/LLM-to-HPC.html#step-5-test-the-downloaded-model",
    "title": "LLM to HPC Tutorial",
    "section": "",
    "text": "Now that you have downloaded the LLaMA 3 model to your scratch directory, you can run inference on an HPC compute node.\n\n\nSave the following code to /scratch/network/$USER/python_test/run_test_llama.py. This script loads the model and runs a short text generation example using the transformers pipeline API.\nfrom transformers import pipeline\nimport torch\nimport os\n\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif not torch.cuda.is_available():\n    raise ValueError(\n        \"CUDA is not available. Make sure you are running this on a GPU node. \"\n        \"For example, run with Slurm requesting GPU:\\n\\n\"\n        \"\\tsalloc -t 0:10:00 --ntasks=1 --gres=gpu:1 python run_test_llama.py\"\n    )\n\nmodel_path = \"/scratch/network/$USER/.cache/huggingface/models--meta-llama--Llama-3.1-8B/snapshots/d04e592bb4f6aa9cfee91e2e20afa771667e1d4b\"\n\nprint(\"CUDA_VISIBLE_DEVICES =\", os.environ.get(\"CUDA_VISIBLE_DEVICES\"))\n\npipe = pipeline(\"text-generation\", model=model_path, tokenizer=model_path)\n\nprompt = \"You are an expert psychologist. Tell me something interesting about psychology regarding Erik Nook's research:\"\noutput = pipe(prompt, max_new_tokens=50)\n\nprint(\"\\nModel output:\\n\", output)\n\nMake sure to replace $USER in the path with your actual NetID or use a variable if running programmatically.\n\n\n\n\nThis Slurm batch script requests one A100 GPU on the Adroit cluster and runs the above Python test script.\n#!/bin/bash\n#SBATCH --job-name=llama3-textgen           # Job name\n#SBATCH --nodes=1                           # Use one node\n#SBATCH --ntasks=1                          # One task\n#SBATCH --cpus-per-task=1                   # One CPU core\n#SBATCH --mem=36G                          # Memory request # can be ess\n#SBATCH --gres=gpu:1                        # Request 1 GPU\n#SBATCH --time=00:10:00                     # Max runtime (adjust as needed)\n#SBATCH --constraint=a100                   # Use A100 GPU\n#SBATCH --nodelist=adroit-h11g1             # Run on node with free GPUs\n#SBATCH --mail-type=ALL                     # Email on start, end, fail\n#SBATCH --mail-user=sm9518@princeton.edu   # Your email\n\nmodule purge\nmodule load anaconda3/2024.6\nmodule load cudatoolkit/11.8 \n\nsource activate talkspaceMADS\n\ncd /scratch/network/$USER/python_test\n\necho \"Job started at $(date)\"\n\nexport PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n\npython run_test_llama.py\n\necho \"Job completed at $(date)\"\n\nüí° Check GPU Node Usage Before Selecting a Node or GPU.\nBefore submitting your job or manually specifying a GPU node (e.g., with #SBATCH --nodelist=adroit-h11g1).\nIt‚Äôs a good idea to check which nodes and GPUs have free memory or are under low load. Otherwise, your job might be assigned to a GPU that is already fully used, causing CUDA out-of-memory errors.\n\nOn Adroit, you can use commands like these from the login node to check GPU availability:\n# Show GPU status and free GPUs per node \nshownodes -p gpu\nIf you don‚Äôt specify a node, Slurm will pick one for you, but it might not always be the best choice if GPUs on that node are busy."
  },
  {
    "objectID": "posts/Federal research funding cuts will stall scientific progress, hurt Texas students/Federal research funding cuts will stall scientific progress, hurt Texas students.html",
    "href": "posts/Federal research funding cuts will stall scientific progress, hurt Texas students/Federal research funding cuts will stall scientific progress, hurt Texas students.html",
    "title": "Federal research funding cuts will stall scientific progress, hurt Texas students",
    "section": "",
    "text": "This piece was published as an Op-ed in the San Antonio Express News\nScientific progress in the United States faces a series of threats. These disruptions have slashed federal research funding; upended the scientific process; dismantled diversity, equity and inclusion ‚Äî DEI ‚Äî initiatives; and endangered the careers of future and current scientists.\nIn particular, the federal government has frozen billions in funding for the National Institutes of Health and the National Science Foundation ‚Äî both of which provide substantial medical and economic benefits to San Antonio and Texas.\nLast year, the NIH awarded nearly $2 billion in funding to Texan institutes, which supported more than 30,000 Texan jobs and generated more than $6 billion in economic activity.\nAs a scientist, I am deeply concerned about these funding cuts. I strongly believe in the direct and indirect opportunities that science provides to improve the lives of our community members.\nI am a Mexican American who was born and raised on San Antonio‚Äôs West Side, where programs like the Pre-Freshman Engineering Program, or PREP, gave me early exposure to careers in science and laid the foundation for my academic journey.\nToday, I am fortunate enough to be a psychology doctoral student at Princeton University, where I am conducting research showing how to identify individuals struggling with poor mental health by examining patterns in their language use.\nPrograms like PREP provided my first introduction to research, which was further developed in labs at Texas State University, the University of Texas at Austin and the University of Pennsylvania ‚Äî all institutions supported by grants from the NIH and NSF.\nPrograms like PREP, often labeled as ‚ÄúDEI initiatives,‚Äù are not about exclusion. They are about giving everyone a shot at the American Dream. They ensure young people without built-in academic connections or financial resources have opportunities to pursue careers in science.\nWithout these programs, countless students ‚Äî regardless of race, gender or political affiliation ‚Äî will lose their chance to break into fields that shape our nation‚Äôs future.\nStripping funding from anything perceived as ‚ÄúDEI-adjacent‚Äù harms us all. It reduces available jobs in the state, freezes life-changing research and shuts the door on talented, hardworking students.\nTo make matters worse, recent government budget cuts have slashed funding for universities, including critical ‚Äúindirect costs‚Äù that fund lab space, technical support, and salaries for scientists and staff.\nUniversities across the country, including the University of Texas at San Antonio, have already begun reducing graduate admissions. This disruption will shrink the pipeline of trained scientists, limit opportunities for young San Antonians, and jeopardize scientific progress in Texas and beyond.\nDue to decades of federally funded research, we have breakthroughs like revolutionary cancer treatments and lifesaving vaccines. Similar advancements have been made in our ability to detect mental health issues with language and provide more personalized support ‚Äî progress that can save countless lives.\nThese advancements depend on continued support from institutions like the NIH and NSF, which drive our economy, research and the training of future scientists.\nPrograms that provide early exposure to science ‚Äî especially for underrepresented students ‚Äî ensure that the next generation is prepared to tackle issues like the mental health epidemic.\nCutting funding for research and training will stall progress and limit opportunities for young minds. Now more than ever, we must fight to protect science and its future to build a healthier, more resilient society for everyone.\n\nAbout the Author\nSteven Mesquiti was born and raised in San Antonio‚Äôs West Side. He completed his undergraduate studies at Southwestern University, has a master‚Äôs degree from Texas State University and is a doctoral student at Princeton University."
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#depression-related-open-ended-questions",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#depression-related-open-ended-questions",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Depression-related Open-ended Questions",
    "text": "Depression-related Open-ended Questions\n\n\nAt initial time point, participants will provide written responses to 9 open-ended questions about mood, motivation, sleep, energy, self-perception, and life satisfaction (‚âà150 words total; see below for full list of questions).\nAdapted from Hur et al.¬†(2024)"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#main-outcomes",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#main-outcomes",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Main Outcomes",
    "text": "Main Outcomes\n\nGAD-7 (anxiety; measured at both time points)\nPHQ-8 (depression; measured at both time points)"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#demographics",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#demographics",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Demographics",
    "text": "Demographics\n\nAge & ResidenceGenderRaceSES\n\n\n\nAge: ___\nState of residence: ___"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#other-measures",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#other-measures",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Other Measures‚Ä¶",
    "text": "Other Measures‚Ä¶\n\nInitial Timepoint\n\nBilingualism (using LEAP-Q)\n\nBoth Timepoints\n\nSatisfaction with Life Short Scale\n\nUCLA Loneliness Scale\n\nFollow-up Only\n\nSocial Media Use\n\nSocial Media‚Äìderived Language-based Assessments"
  },
  {
    "objectID": "posts/I-projections/Post.html#under-the-hood",
    "href": "posts/I-projections/Post.html#under-the-hood",
    "title": "I-Projection Results",
    "section": "Under the hood",
    "text": "Under the hood\nWhat this analysis does\n\nLoads a RoBERTa (a LLM) to embed participant text into high-dimensional vectors.\nDefines psychological axes (valence, ability) using controlled ‚ÄúI am‚Äù / ‚ÄúI feel‚Äù statements; each axis is the normalized difference between mean embeddings of positive vs.¬†negative statements.\nThis allows us to capture the psychological dimensions of interest in the embedding space (i.e., good-bad and able-unable).\nProjects target words (e.g., I, me, my) onto these axes by computing the dot product between each word‚Äôs embedding and the axis vector, capturing how strongly that word aligns with a psychological dimension.\nAggregates these projections at the text level to create text-level measures of valence and ability projections for each participant, which we can then use for down-stream analyses."
  },
  {
    "objectID": "posts/I-projections/Post.html#first_person_sing_valence_combined",
    "href": "posts/I-projections/Post.html#first_person_sing_valence_combined",
    "title": "I-Projection Results",
    "section": "first_person_sing_valence_combined",
    "text": "first_person_sing_valence_combined"
  },
  {
    "objectID": "posts/I-projections/Post.html#first_person_sing_ability_combined",
    "href": "posts/I-projections/Post.html#first_person_sing_ability_combined",
    "title": "I-Projection Results",
    "section": "first_person_sing_ability_combined",
    "text": "first_person_sing_ability_combined"
  },
  {
    "objectID": "posts/I-projections/Post.html#valence-projection",
    "href": "posts/I-projections/Post.html#valence-projection",
    "title": "I-Projection Results",
    "section": "Valence Projection",
    "text": "Valence Projection\n\n\nNon-disaggregatedDisaggregated"
  },
  {
    "objectID": "posts/I-projections/Post.html#ability-projection",
    "href": "posts/I-projections/Post.html#ability-projection",
    "title": "I-Projection Results",
    "section": "Ability Projection",
    "text": "Ability Projection\n\n\nNon-disaggregatedDisaggregated"
  },
  {
    "objectID": "posts/I-projections/Post.html#words",
    "href": "posts/I-projections/Post.html#words",
    "title": "I-Projection Results",
    "section": "% words",
    "text": "% words\nControlling for freq. of words used to construct our axes."
  },
  {
    "objectID": "posts/I-projections/Post.html#valence-projections",
    "href": "posts/I-projections/Post.html#valence-projections",
    "title": "I-Projection Results",
    "section": "Valence Projections",
    "text": "Valence Projections\n\n\nNon-disaggregated (Valence)Disaggregated (valence)"
  },
  {
    "objectID": "posts/I-projections/Post.html#ability-projections",
    "href": "posts/I-projections/Post.html#ability-projections",
    "title": "I-Projection Results",
    "section": "Ability Projections",
    "text": "Ability Projections\n\n\nNon-disaggregated (Ability)Disaggregated (ability)"
  },
  {
    "objectID": "posts/I-projections/Post.html#sentiment-and-total-wc",
    "href": "posts/I-projections/Post.html#sentiment-and-total-wc",
    "title": "I-Projection Results",
    "section": "Sentiment and Total WC",
    "text": "Sentiment and Total WC\nControlling for sentiment scores, emotion words (LIWC), and total word count."
  },
  {
    "objectID": "posts/I-projections/Post.html#valence-projections-1",
    "href": "posts/I-projections/Post.html#valence-projections-1",
    "title": "I-Projection Results",
    "section": "Valence Projections",
    "text": "Valence Projections\n\n\nNon-disaggregated (valence)Disaggregated (valence)"
  },
  {
    "objectID": "posts/I-projections/Post.html#ability-projections-1",
    "href": "posts/I-projections/Post.html#ability-projections-1",
    "title": "I-Projection Results",
    "section": "Ability projections",
    "text": "Ability projections\n\n\nNon-disaggregated (ability)Disaggregated (ability)"
  },
  {
    "objectID": "posts/I-projections/Post.html#good-bad",
    "href": "posts/I-projections/Post.html#good-bad",
    "title": "I-Projection Results",
    "section": "Good-bad",
    "text": "Good-bad"
  },
  {
    "objectID": "posts/I-projections/Post.html#able-unable",
    "href": "posts/I-projections/Post.html#able-unable",
    "title": "I-Projection Results",
    "section": "Able-Unable",
    "text": "Able-Unable"
  },
  {
    "objectID": "publications/empathy-and-helping-the-role-of-affect-in-response-to-others-suffering/index.html",
    "href": "publications/empathy-and-helping-the-role-of-affect-in-response-to-others-suffering/index.html",
    "title": "Empathy and helping: the role of affect in response to others‚Äô suffering",
    "section": "",
    "text": "Abstract\nDecades of research hold that empathy is a multifaceted construct. A related challenge in empathy research is to describe how each subcomponent of empathy uniquely contributes to social outcomes. Here, we examined distinct mechanisms through which different components of empathy‚ÄîEmpathic Concern, Perspective Taking, and Personal Distress‚Äîmay relate to prosociality. Participants (N = 77) watched a prerecorded video of a person sharing an emotional real-life story and provided verbal support in response. The listeners then reported how positive and negative they felt while listening to the story. We found that individuals with greater tendencies to experience Empathic Concern and Perspective Taking felt more positive (e.g., connected, compassionate), whereas those with higher Personal Distress felt more negative (e.g., nervous, anxious) in response to another‚Äôs suffering. We also observed indirect relationships between Empathic Concern / Perspective Taking and the tendency to help others through positive affective responses to the other‚Äôs suffering. These findings build upon the growing literature that distinguishes different components of empathy and their mechanisms that relate to divergent behavioral consequences. Results also highlight the role of positive affect that may motivate prosociality in the face of others‚Äô suffering.\nCitation: Kang, Y., Mesquiti, S., Baik, E. S., & Falk, E. B. (2025). Empathy and helping: the role of affect in response to others‚Äô suffering. Scientific Reports, 15(1), 3256. https://doi.org/10.1038/s41598-025-87221-2"
  },
  {
    "objectID": "publications/predicting-psychological-and-subjective-well-being-through-language-based-assessment/index.html",
    "href": "publications/predicting-psychological-and-subjective-well-being-through-language-based-assessment/index.html",
    "title": "Predicting Psychological and Subjective Well-being through Language-based Assessment",
    "section": "",
    "text": "Abstract\nWell-being is often defined in terms of a person‚Äôs comfort, happiness, functioning and flourishing. Scholars distinguish subjective well-being (i.e., perceiving one‚Äôs life as pleasant) from psychological well-being (i.e., perceiving one‚Äôs life as meaningful). Advances in natural language processing have yielded automated assessments of psychological states and traits from language alone, including subjective well-being. However, the strength of these tools for assessing psychological well-being remains unstudied. Across three studies (one preregistered), we examined the strength of language-based assessments of self-reported subjective and psychological well-being components. Participants gave verbal or written responses to queries regarding their satisfaction with life and autonomy, along with questionnaire measures of subjective and psychological well-being. We then tested the strength of contextual word embeddings generated from AI-based transformers applied to verbal responses in predicting self-reported satisfaction with life and psychological well-being. Predictions generated from word embeddings of open-ended assessments correlated significantly with questionnaire measures of corresponding well-being constructs (rs = .16 &lt; r &lt; .63) and they also generalized across well-being components (rs = .15 &lt; r &lt; .50). However, the strength of these relations was lower than previous studies (rs = .72 &lt; r &lt; .85), and sense of autonomy was consistently less predictable than satisfaction with life. These findings demonstrate that although linguistic measures can significantly correlate with one‚Äôs sense of autonomy, it appears to be more challenging to assess than other forms of well-being.\nCitation: Mesquiti, S., Cosme, D., Nook, E. C., Falk, E. B., & Burns, S. (2025). Predicting psychological and subjective well-being through language-based assessment. Preprint."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Steven Mesquiti",
    "section": "",
    "text": "My name is Steven Mesquiti! I‚Äôm a second-year PhD Student in the Department of Psychology at Princeton University advised by Erik Nook.\nI study how we can use Natural Language Processing techniques and Artificial Intelligence to better understand people‚Äôs mental health.\nBefore Princeton, I worked as a Lab Manager for Emily Falk in the Communication Neuroscience Lab at the University of Pennsylvania. Along the way, I have worked with excessively generous scientists like Jamie Pennebaker to answer questions at the nexus of Computer Science, Computational Linguistics, and Psychology."
  },
  {
    "objectID": "index.html#research-interests",
    "href": "index.html#research-interests",
    "title": "Steven Mesquiti",
    "section": "üß† Research Interests",
    "text": "üß† Research Interests\n\nNatural Language Processing\nMental Health\nLanguage-based Assessments of Mental Health\nArtificial Intelligence"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Steven Mesquiti",
    "section": "üéì Education",
    "text": "üéì Education\n\n\n\n\n\n\n\n\n\nDegree\nField\nYear(s)\nInstitution\n\n\n\n\nPh.D.\nPsychology\n2024‚ÄìPresent\nPrinceton University\n\n\nM.A.\nPsychological Research\n2019‚Äì2022\nTexas State University\n\n\nB.A.\nPsychology (Minor: Spanish)\n2015‚Äì2019\nSouthwestern University"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Steven Mesquiti",
    "section": "",
    "text": "My name is Steven Mesquiti! I‚Äôm a second-year PhD Student in the Department of Psychology at Princeton University advised by Erik Nook.\nI study how we can use Natural Language Processing techniques and Artificial Intelligence to better understand people‚Äôs mental health.\nBefore Princeton, I worked as a Lab Manager for Emily Falk in the Communication Neuroscience Lab at the University of Pennsylvania. Along the way, I have worked with excessively generous scientists like Jamie Pennebaker to answer questions at the nexus of Computer Science, Computational Linguistics, and Psychology."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "Steven Mesquiti",
    "section": "üß† Research Interests",
    "text": "üß† Research Interests\n\nNatural Language Processing\nMental Health\nLanguage-based Assessments of Mental Health\nArtificial Intelligence"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Steven Mesquiti",
    "section": "üéì Education",
    "text": "üéì Education\n\n\n\n\n\n\n\n\n\nDegree\nField\nYear(s)\nInstitution\n\n\n\n\nPh.D.\nPsychology\n2024‚ÄìPresent\nPrinceton University\n\n\nM.A.\nPsychological Research\n2019‚Äì2022\nTexas State University\n\n\nB.A.\nPsychology (Minor: Spanish)\n2015‚Äì2019\nSouthwestern University"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nLLM to HPC Tutorial\n\n\n\n\n\n\nCoding\n\n\nTutorial\n\n\n\n\n\n\n\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nData-Driven Approaches to Building Equitable Language-Based Mental Health Assessments\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\n\n\n\n\n\n\nFederal research funding cuts will stall scientific progress, hurt Texas students\n\n\n\n\n\n\nOpinion\n\n\nScience Policy\n\n\n\n\n\n\n\n\n\nMar 17, 2025\n\n\nSteven Mesquiti\n\n\n\n\n\n\n\n\n\n\n\n\nI-Projection Results\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nSteven\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "coding/correlation/Correlations.html",
    "href": "coding/correlation/Correlations.html",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/correlation\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD.\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, zoo, rlang,caret, broom, kableExtra, reactable, Hmisc, datarium, car,corrplot, plotrix, install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion).\n\n\n\nThis is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())\n\n\n\n\nUsing stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n#summary stats function \n\nmystats_df &lt;- function(df, na.omit=FALSE) {\n  if (na.omit) {\n    df &lt;- df[complete.cases(df), ]\n  }\n  \n  stats_df &lt;- data.frame(\n    n = rep(NA, ncol(df)),\n    mean = rep(NA, ncol(df)),\n    stdev = rep(NA, ncol(df)),\n    skew = rep(NA, ncol(df)),\n    kurtosis = rep(NA, ncol(df))\n  )\n  \n  for (i in seq_along(df)) {\n    x &lt;- df[[i]]\n    m &lt;- mean(x)\n    n &lt;- length(x)\n    s &lt;- sd(x)\n    skew &lt;- sum((x-m)^3/s^3)/n\n    kurt &lt;- sum((x-m)^4/s^4)/n - 3\n    stats_df[i, ] &lt;- c(n, m, s, skew, kurt)\n  }\n  \n  row.names(stats_df) &lt;- colnames(df)\n  return(stats_df)\n}\n\n\n# correlation table function \n\napply_if &lt;- function(mat, p, f) {\n  # Fill NA with FALSE\n  p[is.na(p)] &lt;- FALSE\n  mat[p] &lt;- f(mat[p])\n  mat\n}\n\ncorr_table &lt;- function(mat, corrtype = \"pearson\") {\n  matCorr &lt;- mat\n  if (class(matCorr) != \"rcorr\") {\n    matCorr &lt;- rcorr(mat, type = corrtype)\n  }\n  \n  # Remove upper diagonal\n  matCorr$r[upper.tri(matCorr$r)] &lt;- NA\n  matCorr$P[upper.tri(matCorr$P)] &lt;- NA\n\n  # Add one star for each p &lt; 0.05, 0.01, 0.001\n  stars &lt;- apply_if(round(matCorr$r, 2), matCorr$P &lt; 0.05, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.01, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.001, function(x) paste0(x, \"*\"))\n  \n  # Put - on diagonal and blank on upper diagonal\n  stars[upper.tri(stars, diag = T)] &lt;- \"-\"\n  stars[upper.tri(stars, diag = F)] &lt;- \"\"\n  n &lt;- length(stars[1,])\n  colnames(stars) &lt;- 1:n\n  # Remove _ and convert to title case\n  row.names(stars) &lt;- tools::toTitleCase(sapply(row.names(stars), gsub, pattern=\"_\", replacement = \" \"))\n  # Add index number to row names\n  row.names(stars) &lt;- paste(paste0(1:n,\".\"), row.names(stars))\n  kable(stars) %&gt;% \n    kableExtra::kable_styling()\n}\n\n\n\n\nSince we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the mtcars dataset which is built into R and we are going to call it df.\n\n# Load the data\ndata(\"mtcars\")"
  },
  {
    "objectID": "coding/correlation/Correlations.html#set-working-directory",
    "href": "coding/correlation/Correlations.html#set-working-directory",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/correlation\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD."
  },
  {
    "objectID": "coding/correlation/Correlations.html#load-packages",
    "href": "coding/correlation/Correlations.html#load-packages",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "if (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, rstatix, zoo, rlang,caret, broom, kableExtra, reactable, Hmisc, datarium, car,corrplot, plotrix, install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion)."
  },
  {
    "objectID": "coding/correlation/Correlations.html#get-our-plot-aesthetics-set-up",
    "href": "coding/correlation/Correlations.html#get-our-plot-aesthetics-set-up",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "This is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "coding/correlation/Correlations.html#build-relevant-functions",
    "href": "coding/correlation/Correlations.html#build-relevant-functions",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "Using stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n#summary stats function \n\nmystats_df &lt;- function(df, na.omit=FALSE) {\n  if (na.omit) {\n    df &lt;- df[complete.cases(df), ]\n  }\n  \n  stats_df &lt;- data.frame(\n    n = rep(NA, ncol(df)),\n    mean = rep(NA, ncol(df)),\n    stdev = rep(NA, ncol(df)),\n    skew = rep(NA, ncol(df)),\n    kurtosis = rep(NA, ncol(df))\n  )\n  \n  for (i in seq_along(df)) {\n    x &lt;- df[[i]]\n    m &lt;- mean(x)\n    n &lt;- length(x)\n    s &lt;- sd(x)\n    skew &lt;- sum((x-m)^3/s^3)/n\n    kurt &lt;- sum((x-m)^4/s^4)/n - 3\n    stats_df[i, ] &lt;- c(n, m, s, skew, kurt)\n  }\n  \n  row.names(stats_df) &lt;- colnames(df)\n  return(stats_df)\n}\n\n\n# correlation table function \n\napply_if &lt;- function(mat, p, f) {\n  # Fill NA with FALSE\n  p[is.na(p)] &lt;- FALSE\n  mat[p] &lt;- f(mat[p])\n  mat\n}\n\ncorr_table &lt;- function(mat, corrtype = \"pearson\") {\n  matCorr &lt;- mat\n  if (class(matCorr) != \"rcorr\") {\n    matCorr &lt;- rcorr(mat, type = corrtype)\n  }\n  \n  # Remove upper diagonal\n  matCorr$r[upper.tri(matCorr$r)] &lt;- NA\n  matCorr$P[upper.tri(matCorr$P)] &lt;- NA\n\n  # Add one star for each p &lt; 0.05, 0.01, 0.001\n  stars &lt;- apply_if(round(matCorr$r, 2), matCorr$P &lt; 0.05, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.01, function(x) paste0(x, \"*\"))\n  stars &lt;- apply_if(stars, matCorr$P &lt; 0.001, function(x) paste0(x, \"*\"))\n  \n  # Put - on diagonal and blank on upper diagonal\n  stars[upper.tri(stars, diag = T)] &lt;- \"-\"\n  stars[upper.tri(stars, diag = F)] &lt;- \"\"\n  n &lt;- length(stars[1,])\n  colnames(stars) &lt;- 1:n\n  # Remove _ and convert to title case\n  row.names(stars) &lt;- tools::toTitleCase(sapply(row.names(stars), gsub, pattern=\"_\", replacement = \" \"))\n  # Add index number to row names\n  row.names(stars) &lt;- paste(paste0(1:n,\".\"), row.names(stars))\n  kable(stars) %&gt;% \n    kableExtra::kable_styling()\n}"
  },
  {
    "objectID": "coding/correlation/Correlations.html#load-data",
    "href": "coding/correlation/Correlations.html#load-data",
    "title": "Simple Correlation examples for students",
    "section": "",
    "text": "Since we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the mtcars dataset which is built into R and we are going to call it df.\n\n# Load the data\ndata(\"mtcars\")"
  },
  {
    "objectID": "coding/correlation/Correlations.html#some-other-things",
    "href": "coding/correlation/Correlations.html#some-other-things",
    "title": "Simple Correlation examples for students",
    "section": "Some other things",
    "text": "Some other things\n\nSimple Correlation tests can only be calculated between continuous variables. However, there are other types of correlations tests that can be used to deal with different data types (that‚Äôs outside the scope of this tutorial).\nSpurious correlations exist (i.e., correlations != causation)! Just because something appears to be related, due to it‚Äôs correlation coefficient, doesn‚Äôt mean there‚Äôs actually a relationship there. For example, consumption of ice cream and boating accidents are often related. However, does eating more ice cream reallyyyy lead to people having boat accidents? Think about it. Also, if we are going to infer causation we have to manipulate variables experimentally. We often do not do that in studies where we use correlational analyses.\nYou should become familiar with how to interpret correlation coefficients (esp within your specific field). That is, what is a small, medium, and large correlation? At what point is a correlation coefficient too large (e.g., are you measuring the same construct)? Here‚Äôs an article that may help!"
  },
  {
    "objectID": "coding/correlation/Correlations.html#statistical-assumptions",
    "href": "coding/correlation/Correlations.html#statistical-assumptions",
    "title": "Simple Correlation examples for students",
    "section": "Statistical Assumptions",
    "text": "Statistical Assumptions\nAssumption are also important. That is, data need to possess certain qualities for us to be able to use this type of test. For a t-test these are:\n\nThe data are continuous (not ordinal or nominal).\nData from both variables follow normal distributions.\nYour data have no outliers.\nYour data is from a random or representative sample.\nYou expect a linear relationship between the two variables.\n\nClick through the tabs to see how to check each assumption.\n\nContinuous\nWe can check this by looking at the structure of our data using the str function (for all the variables in our dataset). We can see what variables R is treating as continuous and move forward with our analyses!\n\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\nRandomly Sampled\nThis is something you do when you design the study‚Äìwe can‚Äôt do anything in R to check this.\n\n\nNo outliers\nWe can use the QQ-plot to inspect for outliers, which is in the ggpubr package. To do this we are going to utilize R‚Äôs ability to write functions and for loops. First, we grab all of the names of the variables we want to get qq-plots for using vars &lt;- colnames(mtcars) and save them as a list in R. This will allow us to specify what variables we want to graph. Depending on what datset we are working with, that can be as few as 2 or as many as the entire dataset! Second, we write our qqplot_all function which allows us to write the same graph as many times as we want without having to write out graphing code every. single. time. This is especially useful when graphs don‚Äôt need unique customizations. Next, we write out for loop which allows us to use the qqplot_all function for each of the 11 graphs and save them as 11 unique objects named qq_var[i]. We then arrange all 11 using ggarrange so we can take a look. Lastly, we use Markdown‚Äôs customizability to specify how large (or small) we want out figure to be. Here we go with a 10 x 10 figure. We can see that the variable don‚Äôt have any observations &gt; abs(3) and therefore no outliers.\n\nvars &lt;- colnames(mtcars)\n\nqqplot_all &lt;- function(data) {\n  vars &lt;- names(data)\n  n_vars &lt;- length(vars)\n  \n  for(i in 1:n_vars) {\n    qqplot &lt;- ggqqplot(data[[i]], ylab = vars[i],color = \"dodgerblue\") + plot_aes\n    assign(paste0(\"qq_\", vars[i]), qqplot, envir = .GlobalEnv)\n  }\n}\n\nqqplot_all(mtcars) # create QQ plots for all variables\n\nggarrange(qq_am,qq_carb,qq_cyl,qq_disp,qq_drat,qq_gear,qq_hp,qq_mpg,qq_qsec,qq_vs,qq_wt, common.legend = T, legend = 'right')\n\n\n\n\n\n\n\n\n\n\nNormal Distribution\nTo check the distribution of the data we can use density plots in the ggplot within tidyverse to visualize this. It‚Äôs also important to get some statistics behind this, and to do that we can look at skewness and kurtosis via the mystats function that we wrote earlier. You can also use psych::describe to get similar information. For skewness and kurtosis, we want values of skewness fall between ‚àí 3 and + 3, and kurtosis is appropriate from a range of ‚àí 10 to + 10.\nFor this example we are also going to visualize all of the variables in the dataset! To do this we are going to agian utilize R‚Äôs ability to write functions and for loops. First, we grab all of the names of the variables we want to get density plots for using vars &lt;- colnames(mtcars) and save them as a list in R. Second, we write our density function which allows us to write the same graph as many times as we want without having to write out graphing code every. single. time. Next, we write out for loop which allows us to use the density function for each of the 11 graphs and save them as 11 unique objects named d[i]. We then arrange all 11 using ggarrange so we can take a look. Lastly, we use Markdown‚Äôs customizability to specify how large (or small) we want out figure to be.\n\n#names &lt;- select(data,14:130) #get names of certain variables \n#names &lt;- colnames(df) #if you wanna graph ALL the variables\n\nvars &lt;- colnames(mtcars)\n\n#loop to create density plots \n\ndensity &lt;- function(data, x, y){ #create graphing function \nggplot(data = data) +\n  geom_density(aes_string(x = vars[i]),\n               adjust = 1.5, \n               alpha = 0.5, fill = \"dodgerblue\") + plot_aes\n}\n\nfor(i in 1:11) { #loop use graphing function 11 times\n  nam &lt;- paste(\"d\", i, sep = \"\")\n  assign(nam, density(mtcars,vars[i]))\n}\nggarrange(d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11, common.legend = T, legend = 'right')\n\n\n\n\n\n\n\n\n\n\nYou expect a linear relationship between the two variables.\nTo check this assumption you can plot a scatter and plot between two variables and plot a line of best fit using ggplot. Since we have a bunch of variables in our dataset that we might be interested in, we are only ging to do a few for simplicity‚Äôs sake (weight and MPG).\n\nggplot(data = mtcars, aes(x = mpg, y = wt, color = cyl)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  scale_color_gradient(low = \"blue\", high = \"red\") +\n  labs(x = \"Miles/(US) gallon\", y = \"Weight (1000 lbs)\") +\n  plot_aes"
  },
  {
    "objectID": "coding/correlation/Correlations.html#sample-size-means-standard-deviations-etc.",
    "href": "coding/correlation/Correlations.html#sample-size-means-standard-deviations-etc.",
    "title": "Simple Correlation examples for students",
    "section": "Sample Size, Means, Standard Deviations, etc.!",
    "text": "Sample Size, Means, Standard Deviations, etc.!\n\nmtcars %&gt;%\n  get_summary_stats(type = \"full\") %&gt;% \n  reactable::reactable(striped = TRUE)"
  },
  {
    "objectID": "coding/correlation/Correlations.html#get-summary-stats-for-entire-dataframe",
    "href": "coding/correlation/Correlations.html#get-summary-stats-for-entire-dataframe",
    "title": "Simple Correlation examples for students",
    "section": "Get summary stats for entire dataframe",
    "text": "Get summary stats for entire dataframe\nAlso can grab things like skewness and kurtosis!\n\nmystats_df(mtcars)\n\n      n     mean    stdev    skew kurtosis\nmpg  32  20.0906   6.0269  0.6107 -0.37277\ncyl  32   6.1875   1.7859 -0.1746 -1.76212\ndisp 32 230.7219 123.9387  0.3817 -1.20721\nhp   32 146.6875  68.5629  0.7260 -0.13555\ndrat 32   3.5966   0.5347  0.2659 -0.71470\nwt   32   3.2172   0.9785  0.4231 -0.02271\nqsec 32  17.8487   1.7869  0.3690  0.33511\nvs   32   0.4375   0.5040  0.2403 -2.00194\nam   32   0.4062   0.4990  0.3640 -1.92474\ngear 32   3.6875   0.7378  0.5289 -1.06975\ncarb 32   2.8125   1.6152  1.0509  1.25704"
  },
  {
    "objectID": "coding/correlation/Correlations.html#simple-example",
    "href": "coding/correlation/Correlations.html#simple-example",
    "title": "Simple Correlation examples for students",
    "section": "Simple example",
    "text": "Simple example\nNow, we‚Äôll conduct our correlation analyses. For simplicity, we‚Äôll again start with just two variables (weight and mpg) and then scale up.\nFrom inspecting our output we can see there is a significant, large, negative correlation between mpg and car weight [r = -0.8677, p &lt; .001].\n\nres &lt;- cor.test(mtcars$wt, mtcars$mpg, \n                    method = \"pearson\")\nres\n\n\n    Pearson's product-moment correlation\n\ndata:  mtcars$wt and mtcars$mpg\nt = -9.6, df = 30, p-value = 1e-10\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.9338 -0.7441\nsample estimates:\n    cor \n-0.8677"
  },
  {
    "objectID": "coding/correlation/Correlations.html#scaling",
    "href": "coding/correlation/Correlations.html#scaling",
    "title": "Simple Correlation examples for students",
    "section": "Scaling!",
    "text": "Scaling!\nNow that we‚Äôve seen how to conduct a correlation between two variables, let‚Äôs scale it. R is flexible and can let us run a correlation between every variable in the dataset or just a select few. Since mtcars is a nice dataset to work with, we‚Äôll do the entire dataset using corr &lt;- rcorr(as.matrix(mtcars[1:11])) saving our output as an object. However, when inspect our object we get the r-values (correlation coefficients) and p-values in two separate matricies. Having these in two separate matricies is ok. However, it doesn‚Äôt do much for us in terms of trying to make sense of our data. So, let‚Äôs visualize it.\n\ncorr &lt;- rcorr(as.matrix(mtcars[1:11]))\ncorr\n\n       mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\nmpg   1.00 -0.85 -0.85 -0.78  0.68 -0.87  0.42  0.66  0.60  0.48 -0.55\ncyl  -0.85  1.00  0.90  0.83 -0.70  0.78 -0.59 -0.81 -0.52 -0.49  0.53\ndisp -0.85  0.90  1.00  0.79 -0.71  0.89 -0.43 -0.71 -0.59 -0.56  0.39\nhp   -0.78  0.83  0.79  1.00 -0.45  0.66 -0.71 -0.72 -0.24 -0.13  0.75\ndrat  0.68 -0.70 -0.71 -0.45  1.00 -0.71  0.09  0.44  0.71  0.70 -0.09\nwt   -0.87  0.78  0.89  0.66 -0.71  1.00 -0.17 -0.55 -0.69 -0.58  0.43\nqsec  0.42 -0.59 -0.43 -0.71  0.09 -0.17  1.00  0.74 -0.23 -0.21 -0.66\nvs    0.66 -0.81 -0.71 -0.72  0.44 -0.55  0.74  1.00  0.17  0.21 -0.57\nam    0.60 -0.52 -0.59 -0.24  0.71 -0.69 -0.23  0.17  1.00  0.79  0.06\ngear  0.48 -0.49 -0.56 -0.13  0.70 -0.58 -0.21  0.21  0.79  1.00  0.27\ncarb -0.55  0.53  0.39  0.75 -0.09  0.43 -0.66 -0.57  0.06  0.27  1.00\n\nn= 32 \n\n\nP\n     mpg    cyl    disp   hp     drat   wt     qsec   vs     am     gear  \nmpg         0.0000 0.0000 0.0000 0.0000 0.0000 0.0171 0.0000 0.0003 0.0054\ncyl  0.0000        0.0000 0.0000 0.0000 0.0000 0.0004 0.0000 0.0022 0.0042\ndisp 0.0000 0.0000        0.0000 0.0000 0.0000 0.0131 0.0000 0.0004 0.0010\nhp   0.0000 0.0000 0.0000        0.0100 0.0000 0.0000 0.0000 0.1798 0.4930\ndrat 0.0000 0.0000 0.0000 0.0100        0.0000 0.6196 0.0117 0.0000 0.0000\nwt   0.0000 0.0000 0.0000 0.0000 0.0000        0.3389 0.0010 0.0000 0.0005\nqsec 0.0171 0.0004 0.0131 0.0000 0.6196 0.3389        0.0000 0.2057 0.2425\nvs   0.0000 0.0000 0.0000 0.0000 0.0117 0.0010 0.0000        0.3570 0.2579\nam   0.0003 0.0022 0.0004 0.1798 0.0000 0.0000 0.2057 0.3570        0.0000\ngear 0.0054 0.0042 0.0010 0.4930 0.0000 0.0005 0.2425 0.2579 0.0000       \ncarb 0.0011 0.0019 0.0253 0.0000 0.6212 0.0146 0.0000 0.0007 0.7545 0.1290\n     carb  \nmpg  0.0011\ncyl  0.0019\ndisp 0.0253\nhp   0.0000\ndrat 0.6212\nwt   0.0146\nqsec 0.0000\nvs   0.0007\nam   0.7545\ngear 0.1290\ncarb"
  },
  {
    "objectID": "coding/correlation/Correlations.html#visualize-using-corrplot",
    "href": "coding/correlation/Correlations.html#visualize-using-corrplot",
    "title": "Simple Correlation examples for students",
    "section": "Visualize using corrplot",
    "text": "Visualize using corrplot\nWe can use the corrplot package to visualize our matrix. This is a great package for using shapes and colors to indicate the strength and direction of variable relationships.\nNow we‚Äôll breakdown what each part of this function does:\n\ncorr$r Allows us to feed in the correlation coefficients from our matrix\ntype=\"upper\" Specifies we want the upper diagonal for our matrix\nbg = \"white\" Set the background to white\nmethod = \"number\" Lets us use numbers as a way to visualize the results. We can also use things liek squares, circles, etc.\nnumber.cex = 15/ncol(corr)Allows for us to tinker with the size of the numbers to make them fit.\ninsig = \"blank\" Let the correlations that are p &gt; .05 be blank (or whatever threshold you specify).\n\n\n#Visualize \ncorrplot(corr$r, type=\"upper\",bg = \"white\", method = \"number\",number.cex= 15/ncol(corr),insig = \"blank\")"
  },
  {
    "objectID": "coding/correlation/Correlations.html#build-a-table",
    "href": "coding/correlation/Correlations.html#build-a-table",
    "title": "Simple Correlation examples for students",
    "section": "Build a table",
    "text": "Build a table\nWhile building a data visualiaton is cool, sometimes we want to build a professional table. We can do this using a function and kable styling to make it very pretty and presentable :). We do some by feeding our correlation objcet into our corr_table function. This not only gives us the correlation coefficients, but also the p-value thresholds they are are significant at.\n\ncorr_table(corr)\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n\n\n\n\n1. Mpg\n-\n\n\n\n\n\n\n\n\n\n\n\n\n2. Cyl\n-0.85***\n-\n\n\n\n\n\n\n\n\n\n\n\n3. Disp\n-0.85***\n0.9***\n-\n\n\n\n\n\n\n\n\n\n\n4. Hp\n-0.78***\n0.83***\n0.79***\n-\n\n\n\n\n\n\n\n\n\n5. Drat\n0.68***\n-0.7***\n-0.71***\n-0.45**\n-\n\n\n\n\n\n\n\n\n6. Wt\n-0.87***\n0.78***\n0.89***\n0.66***\n-0.71***\n-\n\n\n\n\n\n\n\n7. Qsec\n0.42*\n-0.59***\n-0.43*\n-0.71***\n0.09\n-0.17\n-\n\n\n\n\n\n\n8. Vs\n0.66***\n-0.81***\n-0.71***\n-0.72***\n0.44*\n-0.55***\n0.74***\n-\n\n\n\n\n\n9. Am\n0.6***\n-0.52**\n-0.59***\n-0.24\n0.71***\n-0.69***\n-0.23\n0.17\n-\n\n\n\n\n10. Gear\n0.48**\n-0.49**\n-0.56***\n-0.13\n0.7***\n-0.58***\n-0.21\n0.21\n0.79***\n-\n\n\n\n11. Carb\n-0.55**\n0.53**\n0.39*\n0.75***\n-0.09\n0.43*\n-0.66***\n-0.57***\n0.06\n0.27\n-"
  },
  {
    "objectID": "coding/correlation/Correlations.html#load-the-data",
    "href": "coding/correlation/Correlations.html#load-the-data",
    "title": "Simple Correlation examples for students",
    "section": "Load the data",
    "text": "Load the data\n\ninaug &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Inaug_ALL_VARS.csv') #read in the data\n\ninaug &lt;- inaug %&gt;% mutate(we_i_ratio = we/i) \n\ntidy_df_Inaug&lt;- inaug %&gt;%\n group_by(year) %&gt;% ###grouping by the year \n   summarise_at(vars(\"WPS\",\"readability\",\"grade_level\",'i','we','pronoun','det','syllables_per_word','syllables_per_sentence', \"% words POS possessive\",\"% words 'of'\", \"Contractions\",\"we_i_ratio\"),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n# Get the mean values for the first year in the dataset\nyear_means &lt;- tidy_df_Inaug %&gt;%\n  filter(year == 1789)"
  },
  {
    "objectID": "coding/correlation/Correlations.html#variable-description",
    "href": "coding/correlation/Correlations.html#variable-description",
    "title": "Simple Correlation examples for students",
    "section": "Variable Description",
    "text": "Variable Description\nFlesch-Kincaid Ease of Readability: higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. Calculated using spaCy in python.\nThe Flesch‚ÄìKincaid Grade Level Score: presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts.Calculated using spaCy in python.\nI-usage: First-person singular pronoun usage (% of total words). Calculated using LIWC.\nWe-usage: First-person plural pronoun usage (% of total words). Calculated using LIWC.\nPronoun-usage: Overall pronoun usage (% of total words). Calculated using LIWC.\nPossessive-usage:First-person singular pronoun usage (% of total words). Calculated using NLTK POS, PRP, and PRP$ parser\nOf-usage: Usage of the word ‚Äòof‚Äô (% of total words). Calculated using NLTK parser.\nContraction-usage: Usage of 85 most common contractions in English (% of total words). Calculated using custom LIWC dictionary.\nDeterminers-usage: Determiner usage (% of total words). Calculated using LIWC."
  },
  {
    "objectID": "coding/correlation/Correlations.html#dates",
    "href": "coding/correlation/Correlations.html#dates",
    "title": "Simple Correlation examples for students",
    "section": "Dates",
    "text": "Dates\n\ninaug %&gt;% \n  select(year) %&gt;% \n  range()\n\n[1] 1789 2021"
  },
  {
    "objectID": "coding/correlation/Correlations.html#raw-count-of-speeches",
    "href": "coding/correlation/Correlations.html#raw-count-of-speeches",
    "title": "Simple Correlation examples for students",
    "section": "Raw count of Speeches",
    "text": "Raw count of Speeches\n\ninaug %&gt;%\n  select(Filename) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  reactable::reactable(striped = TRUE)"
  },
  {
    "objectID": "coding/correlation/Correlations.html#speeches-per-year",
    "href": "coding/correlation/Correlations.html#speeches-per-year",
    "title": "Simple Correlation examples for students",
    "section": "Speeches per year",
    "text": "Speeches per year\n\narticles_year &lt;- inaug %&gt;%\n  select(Filename,year) %&gt;%\n  unique() %&gt;%\n  group_by(year) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  reactable::reactable(striped = TRUE)\n articles_year"
  },
  {
    "objectID": "coding/Regression/Regression.html",
    "href": "coding/Regression/Regression.html",
    "title": "Regression",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Regression\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD.\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, broom, kableExtra, reactable, datarium, car,corrplot, install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion).\n\n\n\nThis is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())\n\n\n\n\nUsing stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n table_model = function(model_data,reference = \"Intercept\") {\n   model_data %&gt;% \n     tidy() %&gt;% \n     rename(\"SE\" = std.error,\n            \"t\" = statistic,\n            \"p\" = p.value) %&gt;%\n     kable() %&gt;% \n     kableExtra::kable_styling()\n   \n }\n\n\n\n\nSince we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the marketing dataset which is built into the R package datarium and we are going to call it df."
  },
  {
    "objectID": "coding/Regression/Regression.html#set-working-directory",
    "href": "coding/Regression/Regression.html#set-working-directory",
    "title": "Regression",
    "section": "",
    "text": "setwd(\"~/Desktop/Coding-Boot-Camp/Regression\") #change to your own WD. you can do that by modifying the file path or go session (on the upper bar) --&gt; set working directory)\n\nChange to your own working directory (WD) to save things like plots. You can do that by modifying the file path or go session (on the upper bar) ‚Äì&gt; set working directory). Working directories are important in R because they tell the computer where to look to grab information and save things like results. This can vary by project, script, etc. so it‚Äôs important to consistently have the appropriate WD. If you are unsure what your current WD is, you can use the getwd command in the console (usually the lower left hand pane) to get your WD."
  },
  {
    "objectID": "coding/Regression/Regression.html#load-packages",
    "href": "coding/Regression/Regression.html#load-packages",
    "title": "Regression",
    "section": "",
    "text": "if (!require(\"pacman\")) install.packages(\"pacman\") #run this if you don't have pacman \nlibrary(pacman)\npacman::p_load(tidyverse, ggpubr, broom, kableExtra, reactable, datarium, car,corrplot, install = T) \n#use pacman to load packages quickly \n\nFor this script, and here forward, We use pacman to load in all of our packages rather than using the iterative if (!require(\"PACKAGE\")) install.packages(\"PACKAGE\") set-up. There‚Äôs still some merit to using that if loading in packages in a certain order creates issues (e.g.,tidyverse and brms in a certain fashion)."
  },
  {
    "objectID": "coding/Regression/Regression.html#get-our-plot-aesthetics-set-up",
    "href": "coding/Regression/Regression.html#get-our-plot-aesthetics-set-up",
    "title": "Regression",
    "section": "",
    "text": "This is a super quick and easy way to style our plots without introduce a vile amount of code lines to each chunk!\n\npalette_map = c(\"#3B9AB2\", \"#EBCC2A\", \"#F21A00\")\npalette_condition = c(\"#ee9b00\", \"#bb3e03\", \"#005f73\")\n\nplot_aes = theme_classic() + # \n  theme(legend.position = \"top\",\n        legend.text = element_text(size = 12),\n        text = element_text(size = 16, family = \"Futura Medium\"),\n        axis.text = element_text(color = \"black\"),\n        axis.line = element_line(colour = \"black\"),\n        axis.ticks.y = element_blank())"
  },
  {
    "objectID": "coding/Regression/Regression.html#build-relevant-functions",
    "href": "coding/Regression/Regression.html#build-relevant-functions",
    "title": "Regression",
    "section": "",
    "text": "Using stuff like summary functions allows for us to present results in a clean, organized manner. For example, we can trim superfluous information from model output when sharing with collaborators among other things.\n\n table_model = function(model_data,reference = \"Intercept\") {\n   model_data %&gt;% \n     tidy() %&gt;% \n     rename(\"SE\" = std.error,\n            \"t\" = statistic,\n            \"p\" = p.value) %&gt;%\n     kable() %&gt;% \n     kableExtra::kable_styling()\n   \n }"
  },
  {
    "objectID": "coding/Regression/Regression.html#load-data",
    "href": "coding/Regression/Regression.html#load-data",
    "title": "Regression",
    "section": "",
    "text": "Since we are using an existing dataset in R, we don‚Äôt need to do anything fancy here. However, when normally load in data you can use a few different approaches. In most reproducible scripts you‚Äôll see people use nomenclature similar to df, data, dataframe, etc. to denote a dataframe. If you are working with multiple datasets, it‚Äôs advisable to call stuff by a intuitive name that allows you to know what the data actually is. For example, if I am working with two different corpora (e.g., Atlantic and NYT Best-Sellers) I will probably call the Atlantic dataframe atlantic and the NYT Best-sellers NYT for simplicity and so I don‚Äôt accidentally write over files.\nFor example, if your WD is already set and the data exists within said directory you can use: df &lt;- read_csv(MY_CSV.csv)\nIf the data is on something like Github you can use: df &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Language_Lab_Repro/main/Atlantic_Cleaned_all_vars.csv') #read in the data.\nIf you are working in one directory and need to call something for another directory you can do something like: Atlantic_FK &lt;- read_csv(\"~/Desktop/working-with-lyle/Atlantic/Atlantic_flesch_kinkaid_scores.csv\")\nThere are also other packages/functions that allow you to read in files with different extensions such as haven::read_sav() to read in a file from SPSS or rjson:: fromJSON(file=\"data.json\")to read in a json file. If you want to learn more about how to reading in different files you can take a peek at this site.\nFor the first half, we are going to be using the marketing dataset which is built into the R package datarium and we are going to call it df."
  },
  {
    "objectID": "coding/Regression/Regression.html#the-basics",
    "href": "coding/Regression/Regression.html#the-basics",
    "title": "Regression",
    "section": "The basics",
    "text": "The basics\nThe mathematical formula of the linear regression can be written as y = b0 + b1*x + e, where:\n\nb0 and b1 are known as the regression beta coefficients or parameters:\n\nb0 is the intercept (sometimes referred to as the constant) of the regression line; that is the predicted value when x = 0.\nb1 is the slope of the regression line.\n\ne is the error term (also known as the residual errors ‚Äì the distance between the observed data and the expected aka the line of best fit), the part of y that can be explained by the regression model\n\nThe figure below illustrates the linear regression model, where:\n\nthe best-fit regression line is in blue\nthe intercept (b0) and the slope (b1) are shown in legend\nthe error terms (e) are represented by vertical red lines extending from the regression line\n\n\n# Create some example data\nx &lt;- rnorm(100, 0, 100)\ny &lt;- rnorm(100, 0, 30)\n\n# Fit a linear regression model\nfit &lt;- lm(y ~ x)\n\n# Plot the data points\nplot(x, y, main = \"Linear Regression Example\")\n\n# Add the regression line in blue\nabline(fit, col = \"blue\")\n\n# Add the intercept and slope in green\nlegend(\"topleft\", \n       legend = paste0(\"b0 = \", round(coef(fit)[1], 2), \", b1 = \", round(coef(fit)[2], 2)), \n       col = \"blue\", lty = 1)\n\n# Add the error terms in red\ny_hat &lt;- predict(fit)\nsegments(x, y, x, y_hat, col = \"red\")\n\n\n\n\n\n\n\n\nFrom the scatter plot above, it can be seen that not all the data points fall exactly on the fitted regression line. Some of the points are above the blue curve and some are below it; overall, the residual errors (e) have approximately mean zero.\nThe sum of the squares of the residual errors are called the Residual Sum of Squares or RSS.\nThe average variation of points around the fitted regression line is called the Residual Standard Error (RSE; often just referred to as the standard error [SE]). This is one the metrics used to evaluate the overall quality of the fitted regression model. The lower the RSE, the better it is.\nSince the mean error term is zero, the outcome variable y can be approximately estimated as follow:\ny ~ b0 + b1*x\nMathematically, the beta coefficients (b0 and b1) are determined so that the RSS is as minimal as possible. This method of determining the beta coefficients is technically called least squares regression or ordinary least squares (OLS) regression.\nOnce, the beta coefficients are calculated, a t-test is performed to check whether or not these coefficients are significantly different from zero. A non-zero beta coefficients means that there is a significant relationship between the predictors (x) and the outcome variable (y)."
  },
  {
    "objectID": "coding/Regression/Regression.html#load-in-data",
    "href": "coding/Regression/Regression.html#load-in-data",
    "title": "Regression",
    "section": "Load in data",
    "text": "Load in data\n\n# Load the package\ndata(\"marketing\", package = \"datarium\")\nhead(marketing, 4)\n\n  youtube facebook newspaper sales\n1  276.12    45.36     83.04 26.52\n2   53.40    47.16     54.12 12.48\n3   20.64    55.08     83.16 11.16\n4  181.80    49.56     70.20 22.20\n\n\nFor this dataset we are interested in predicting sales on the basis of advertising budget spent on youtube."
  },
  {
    "objectID": "coding/Regression/Regression.html#data-visualization",
    "href": "coding/Regression/Regression.html#data-visualization",
    "title": "Regression",
    "section": "Data visualization",
    "text": "Data visualization\nNow that we have our research question mapped out we can visually represent our data doing the following:\n\nCreate a scatter plot displaying the sales units versus youtube advertising budget.\nAdd a smoothed line\n\n\nggplot(marketing, aes(x = youtube, y = sales)) +\n  geom_point() +\n  stat_smooth() +\n  plot_aes\n\n\n\n\n\n\n\n\nThe graph above suggests a linearly increasing relationship between the sales and the youtube variables. This is a good thing, because, one important assumption of the linear regression is that the relationship between the outcome and predictor variables is linear and additive.\nIt‚Äôs also possible to compute the correlation coefficient between the two variables using the R function cor():\n\ncor(marketing$sales, marketing$youtube)\n\n[1] 0.7822\n\n\nThe correlation coefficient measures the level of the association between two variables x and y. Its value ranges between -1 (perfect negative correlation: when x increases, y decreases) and +1 (perfect positive correlation: when x increases, y increases).\nA value closer to 0 suggests a weak relationship between the variables. A low correlation (-0.2 &lt; x &lt; 0.2) probably suggests that much of variation of the outcome variable (y) is not explained by the predictor (x). In such case, we should probably look for better predictor variables. Also want to note that the ‚Äòsize‚Äô of correlations also depends on the field you‚Äôre working in and the nature of the data. For example, a correlation of (-0.2 &lt; x &lt; 0.2) for working with language data may be considered sizable if there are a ton of observations :). Additionally, if your correlation coefficient is too large, then you start to run into issues with multicollinearlity (or measuring the same thing), but more on that later!\nIn our example, the correlation coefficient is large enough, so we can continue by building a linear model of y as a function of x."
  },
  {
    "objectID": "coding/Regression/Regression.html#interpretation",
    "href": "coding/Regression/Regression.html#interpretation",
    "title": "Regression",
    "section": "Interpretation",
    "text": "Interpretation\nFrom the output above:\n\nthe estimated regression line equation can be written as follow: sales = 8.44 + 0.048*youtube\nthe intercept (b0) is 8.44. It can be interpreted as the predicted sales unit for a zero youtube advertising budget. Recall that, we are operating in units of thousand dollars. This means that, for a youtube advertising budget equal zero, we can expect a sale of 8.44 *1000 = 8440 dollars.\nthe regression beta coefficient for the variable youtube (b1), also known as the slope, is 0.048. This means that, for a youtube advertising budget equal to 1000 dollars, we can expect an increase of 48 units (0.048 * 1000) in sales. That is, sales = 8.44 + 0.048*1000 = 56.44 units. As we are operating in units of thousand dollars, this represents a sale of 56440 dollars."
  },
  {
    "objectID": "coding/Regression/Regression.html#regression-line",
    "href": "coding/Regression/Regression.html#regression-line",
    "title": "Regression",
    "section": "Regression Line",
    "text": "Regression Line\nTo add the regression line onto the scatter plot, you can use the function stat_smooth() [ggplot2]. By default, the fitted line is presented with confidence interval around it. The confidence bands reflect the uncertainty about the line. If you don‚Äôt want to display it, specify the option se = FALSE in the function stat_smooth().\n\nggplot(marketing, aes(youtube, sales)) +\n  geom_point() +\n  stat_smooth(method = lm) + \n  plot_aes\n\n\n\n\n\n\n\n\nYou can also paste your model coefficients onto the graph too! This makes it super easy to share our results\n\nplot &lt;- ggplot(marketing, aes(youtube, sales)) +\n  geom_point() +\n  stat_smooth(method = lm) + \n  plot_aes\n\nmodel &lt;- lm(sales ~ youtube, data = marketing)\nmodel_pvalue &lt;- summary(model)$coefficients[2, 4]\n\ny_max &lt;- max(marketing$sales)\n\nplot + \n  geom_text(aes(x=0, y=y_max, label=paste(\"Youtube: \", format(coef(model)[2], digits=2), \" p = \", format(model_pvalue, digits=2))),\n            color=\"dodgerblue3\", size=5 ,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#model-summary",
    "href": "coding/Regression/Regression.html#model-summary",
    "title": "Regression",
    "section": "Model Summary",
    "text": "Model Summary\nWe can start exploring our model by displaying the statistical summary of the model using the R function summary()\nThe summary outputs shows 6 components, including:\n\nCall. Shows the function call used to compute the regression model.\nResiduals. Provide a quick view of the distribution of the residuals, which by definition have a mean zero. Therefore, the median should not be far from zero, and the minimum and maximum should be roughly equal in absolute value.\nCoefficients. Shows the regression beta coefficients and their statistical significance. Predictor variables, that are significantly associated to the outcome variable, are marked by stars.\nResidual standard error (RSE), R-squared (R2) and the F-statistic are metrics that are used to check how well the model fits to our data. The most relevant here is R-squared, which tells us how much variance our model explains or how much information it can provide to our prediction!\n\n\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ youtube, data = marketing)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.06  -2.35  -0.23   2.48   8.65 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.43911    0.54941    15.4   &lt;2e-16 ***\nyoutube      0.04754    0.00269    17.7   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.91 on 198 degrees of freedom\nMultiple R-squared:  0.612, Adjusted R-squared:  0.61 \nF-statistic:  312 on 1 and 198 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "coding/Regression/Regression.html#coefficient-significance",
    "href": "coding/Regression/Regression.html#coefficient-significance",
    "title": "Regression",
    "section": "Coefficient significance",
    "text": "Coefficient significance\nThe coefficients table, in the model statistical summary, shows:\n\nthe estimates of the beta coefficients\nthe standard errors (SE), which defines the accuracy of beta coefficients. For a given beta coefficient, the SE reflects how the coefficient varies under repeated sampling. It can be used to compute the confidence intervals and the t-statistic.\nthe t-statistic and the associated p-value, which defines the statistical significance of the beta coefficients."
  },
  {
    "objectID": "coding/Regression/Regression.html#t-statistic-and-p-values",
    "href": "coding/Regression/Regression.html#t-statistic-and-p-values",
    "title": "Regression",
    "section": "t-statistic and p-values:",
    "text": "t-statistic and p-values:\nFor a given predictor, the t-statistic (and its associated p-value) tests whether or not there is a statistically significant relationship between a given predictor and the outcome variable, that is whether or not the beta coefficient of the predictor is significantly different from zero.\nThe statistical hypotheses are as follow:\n\nNull hypothesis (H0): the coefficients are equal to zero (i.e., no relationship between x and y)\nAlternative Hypothesis (Ha): the coefficients are not equal to zero (i.e., there is some relationship between x and y)\n\nMathematically, for a given beta coefficient (b), the t-test is computed as t = (b - 0)/SE(b), where SE(b) is the standard error of the coefficient b. The t-statistic measures the number of standard deviations that b is away from 0. Thus a large t-statistic will produce a small p-value.\nThe higher the t-statistic (and the lower the p-value), the more significant the predictor. The symbols to the right visually specifies the level of significance. The line below the table shows the definition of these symbols; one star means 0.01 &lt; p &lt; 0.05. The more the stars beside the variable‚Äôs p-value, the more significant the variable.\nA statistically significant coefficient indicates that there is an association between the predictor (x) and the outcome (y) variable.\nIn our example, both the p-values for the intercept and the predictor variable are highly significant, so we can reject the null hypothesis and accept the alternative hypothesis, which means that there is a significant association between the predictor and the outcome variables.\nThe t-statistic is a very useful guide for whether or not to include a predictor in a model. High t-statistics (which go with low p-values near 0) indicate that a predictor should be retained in a model, while very low t-statistics indicate a predictor could be dropped (P. Bruce and Bruce 2017)."
  },
  {
    "objectID": "coding/Regression/Regression.html#standard-errors-and-confidence-intervals",
    "href": "coding/Regression/Regression.html#standard-errors-and-confidence-intervals",
    "title": "Regression",
    "section": "Standard errors and confidence intervals",
    "text": "Standard errors and confidence intervals\nThe standard error measures the variability/accuracy of the beta coefficients. It can be used to compute the confidence intervals of the coefficients. Frequentest approach confidence intervals should be interpreted as:\n\nConfidence Level: The first thing to understand is the confidence level associated with the interval. For example, if you have a 95% confidence interval, this means that if you were to repeat the sampling and estimation process many times, 95% of those intervals would contain the true population parameter.\nInterval Range: The interval itself is a range of values, usually expressed as ‚Äúestimate ¬± margin of error.‚Äù For example, if the estimate is 10 and the margin of error is 2, the interval would be 8 to 12. This means that we are 95% confident that the true parameter value falls within this range.\nUncertainty: It‚Äôs important to understand that the interval provides a range of plausible values, but it does not tell us the exact value of the parameter. There is always some uncertainty associated with the estimate.\nSample Size: The width of the interval depends on the sample size and the variability of the data. A larger sample size generally leads to a narrower interval, while more variability leads to a wider interval.\nRelevance: Finally, it‚Äôs important to consider the context of the problem and whether the interval is relevant for making decisions. If the interval is too wide to provide useful information, it may be necessary to collect more data or use a different statistical method. Additionally, it‚Äôs important to consider any assumptions or limitations of the statistical model used to construct the interval.\n\nFor example, the 95% confidence interval for the coefficient b1 is defined as b1 +/- 2*SE(b1), where:\n\nthe lower limits of b1 = b1 - 2SE(b1) = 0.047 - 20.00269 = 0.042\nthe upper limits of b1 = b1 + 2SE(b1) = 0.047 + 20.00269 = 0.052\n\nThat is, there is approximately a 95% chance that the interval [0.042, 0.052] will contain the true value of b1. Similarly the 95% confidence interval for b0 can be computed as b0 +/- 2*SE(b0).\nTo get these information, simply type confint(model):\n\nconfint(model)\n\n              2.5 %  97.5 %\n(Intercept) 7.35566 9.52256\nyoutube     0.04223 0.05284"
  },
  {
    "objectID": "coding/Regression/Regression.html#residual-standard-error",
    "href": "coding/Regression/Regression.html#residual-standard-error",
    "title": "Regression",
    "section": "Residual Standard Error",
    "text": "Residual Standard Error\nThe RSE (also known as the model sigma) is the residual variation, representing the average variation of the observations points around the fitted regression line. This is the standard deviation of residual errors.\nRSE provides an absolute measure of patterns in the data that can‚Äôt be explained by the model. When comparing two models, the model with the small RSE is a good indication that this model fits the best the data.\nDividing the RSE by the average value of the outcome variable will give you the prediction error rate, which should be as small as possible.\nIn our example, RSE = 3.91, meaning that the observed sales values deviate from the true regression line by approximately 3.9 units in average.\nWhether or not an RSE of 3.9 units is an acceptable prediction error is subjective and depends on the problem context. However, we can calculate the percentage error. In our data set, the mean value of sales is 16.827, and so the percentage error is 3.9/16.827 = 23%.\n\nGet the prediciton error\n\nsigma(model)*100/mean(marketing$sales)\n\n[1] 23.24"
  },
  {
    "objectID": "coding/Regression/Regression.html#r-squared-and-adjusted-r-squared",
    "href": "coding/Regression/Regression.html#r-squared-and-adjusted-r-squared",
    "title": "Regression",
    "section": "R-squared and Adjusted R-squared:",
    "text": "R-squared and Adjusted R-squared:\nThe R-squared (R2) ranges from 0 to 1 and represents the proportion of information (i.e.¬†variation) in the data that can be explained by the model. The adjusted R-squared adjusts for the degrees of freedom.\nThe R2 measures, how well the model fits the data. For a simple linear regression, R2 is the square of the Pearson correlation coefficient.\nA high value of R2 is a good indication. However, as the value of R2 tends to increase when more predictors are added in the model, such as in multiple linear regression model, you should mainly consider the adjusted R-squared, which is a penalized R2 for a higher number of predictors.\n\nAn (adjusted) R2 that is close to 1 indicates that a large proportion of the variability in the outcome has been explained by the regression model.\nA number near 0 indicates that the regression model did not explain much of the variability in the outcome."
  },
  {
    "objectID": "coding/Regression/Regression.html#f-statistic",
    "href": "coding/Regression/Regression.html#f-statistic",
    "title": "Regression",
    "section": "F-Statistic",
    "text": "F-Statistic\nThe F-statistic gives the overall significance of the model. It assess whether at least one predictor variable has a non-zero coefficient.\nIn a simple linear regression, this test is not really interesting since it just duplicates the information in given by the t-test, available in the coefficient table. In fact, the F test is identical to the square of the t test: 312.1 = (17.67)^2. This is true in any model with 1 degree of freedom.\nThe F-statistic becomes more important once we start using multiple predictors as in multiple linear regression.\nA large F-statistic will corresponds to a statistically significant p-value (p &lt; 0.05). In our example, the F-statistic equal 312.14 producing a p-value of 1.46e-42, which is highly significant."
  },
  {
    "objectID": "coding/Regression/Regression.html#load-in-the-data",
    "href": "coding/Regression/Regression.html#load-in-the-data",
    "title": "Regression",
    "section": "Load in the data",
    "text": "Load in the data\nAgain, we‚Äôll use the marketing data set [datarium package], which contains the impact of the amount of money spent on three advertising medias (youtube, facebook and newspaper) on sales.\n\ndata(\"marketing\", package = \"datarium\")\nhead(marketing, 4)\n\n  youtube facebook newspaper sales\n1  276.12    45.36     83.04 26.52\n2   53.40    47.16     54.12 12.48\n3   20.64    55.08     83.16 11.16\n4  181.80    49.56     70.20 22.20"
  },
  {
    "objectID": "coding/Regression/Regression.html#build-our-model",
    "href": "coding/Regression/Regression.html#build-our-model",
    "title": "Regression",
    "section": "Build our model",
    "text": "Build our model\nWe want to build a model for estimating sales based on the advertising budget invested in youtube, facebook and newspaper, as follow:\nsales = b0 + b1*youtube + b2*facebook + b3*newspaper\n\nmodel &lt;- lm(sales ~ youtube + facebook + newspaper, data = marketing)\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook + newspaper, data = marketing)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-10.59  -1.07   0.29   1.43   3.40 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.52667    0.37429    9.42   &lt;2e-16 ***\nyoutube      0.04576    0.00139   32.81   &lt;2e-16 ***\nfacebook     0.18853    0.00861   21.89   &lt;2e-16 ***\nnewspaper   -0.00104    0.00587   -0.18     0.86    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.02 on 196 degrees of freedom\nMultiple R-squared:  0.897, Adjusted R-squared:  0.896 \nF-statistic:  570 on 3 and 196 DF,  p-value: &lt;2e-16"
  },
  {
    "objectID": "coding/Regression/Regression.html#interpret",
    "href": "coding/Regression/Regression.html#interpret",
    "title": "Regression",
    "section": "Interpret",
    "text": "Interpret\nIn our example, it can be seen that p-value of the F-statistic is &lt; 2.2e-16, which is highly significant. This means that, at least, one of the predictor variables is significantly related to the outcome variable.\nTo see which predictor variables are significant, you can examine the coefficients table, which shows the estimate of regression beta coefficients and the associated t-statistic p-values:\n\ntable_model(model)\n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n3.5267\n0.3743\n9.4223\n0.0000\n\n\nyoutube\n0.0458\n0.0014\n32.8086\n0.0000\n\n\nfacebook\n0.1885\n0.0086\n21.8935\n0.0000\n\n\nnewspaper\n-0.0010\n0.0059\n-0.1767\n0.8599\n\n\n\n\n\n\n\nFor a given the predictor, the t-statistic evaluates whether or not there is significant association between the predictor and the outcome variable, that is whether the beta coefficient of the predictor is significantly different from zero.\nIt can be seen that, changing in youtube and facebook advertising budget are significantly associated to changes in sales while changes in newspaper budget is not significantly associated with sales.\nFor a given predictor variable, the coefficient (b) can be interpreted as the average effect on y of a one unit increase in predictor, holding all other predictors fixed.\nFor example, for a fixed amount of youtube and newspaper advertising budget, spending an additional 1 000 dollars on facebook advertising leads to an increase in sales by approximately 0.1885*1000 = 189 sale units, on average.\nThe youtube coefficient suggests that for every 1 000 dollars increase in youtube advertising budget, holding all other predictors constant, we can expect an increase of 0.045*1000 = 45 sales units, on average.\nWe found that newspaper is not significant in the multiple regression model. This means that, for a fixed amount of youtube and newspaper advertising budget, changes in the newspaper advertising budget will not significantly affect sales units.\nAs the newspaper variable is not significant, it is possible to remove it from the model:\n\nmodel  &lt;- lm(sales ~ youtube + facebook, data = marketing)\nsummary(model)\n\n\nCall:\nlm(formula = sales ~ youtube + facebook, data = marketing)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.557  -1.050   0.291   1.405   3.399 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.50532    0.35339    9.92   &lt;2e-16 ***\nyoutube      0.04575    0.00139   32.91   &lt;2e-16 ***\nfacebook     0.18799    0.00804   23.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.02 on 197 degrees of freedom\nMultiple R-squared:  0.897, Adjusted R-squared:  0.896 \nF-statistic:  860 on 2 and 197 DF,  p-value: &lt;2e-16\n\n\nFinally, our model equation can be written as follow: sales = 3.5 + 0.045*youtube + 0.187*facebook.\nThe confidence interval of the model coefficient can be extracted as follow:\n\nconfint(model)\n\n              2.5 % 97.5 %\n(Intercept) 2.80841 4.2022\nyoutube     0.04301 0.0485\nfacebook    0.17214 0.2038"
  },
  {
    "objectID": "coding/Regression/Regression.html#visualize-the-regression-model",
    "href": "coding/Regression/Regression.html#visualize-the-regression-model",
    "title": "Regression",
    "section": "Visualize the regression model",
    "text": "Visualize the regression model\n\n# Create a data frame with the actual values and predicted values\ndf &lt;- data.frame(sales = marketing$sales, predicted_sales = predict(model))\n\n# Create a scatterplot with the line of best fit\nplot &lt;- \n  ggplot(marketing, aes(x = youtube + facebook, y = sales)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", formula = y ~ x, se = T) +\n  labs(x = \"YouTube + Facebook\", y = \"Sales\") +\n  ggtitle(\"Actual vs. Predicted Sales\") +\n  plot_aes\n\nyoutube_pvalue &lt;- summary(model)$coefficients[2, 4]\nfbook_pvalue &lt;- summary(model)$coefficients[3, 4]\n\ny_max &lt;- max(marketing$sales)\n\nplot + \n  geom_text(aes(x=0, y=y_max, label=paste(\"Youtube: \", format(coef(model)[2], digits=2), \" p = \", format(model_pvalue, digits=2))),\n            color=\"red\", size=5 ,hjust = 0) +\n  geom_text(aes(x=0, y=y_max-5, label=paste(\"Facebook: \", format(coef(model)[3], digits=2), \" p = \", format(model_pvalue, digits=2))),\n            color=\"dodgerblue2\", size=5 ,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#load-the-data",
    "href": "coding/Regression/Regression.html#load-the-data",
    "title": "Regression",
    "section": "Load the data",
    "text": "Load the data\n\ninaug &lt;- read_csv('https://raw.githubusercontent.com/scm1210/Summer-Coding/main/data/Inaug_ALL_VARS.csv') #read in the data\n\ninaug &lt;- inaug %&gt;% mutate(we_i_ratio = we/i) \n\ntidy_df_Inaug&lt;- inaug %&gt;%\n group_by(year) %&gt;% ###grouping by the year \n   summarise_at(vars(\"WPS\",\"readability\",\"grade_level\",'i','we','pronoun','det','syllables_per_word','syllables_per_sentence', \"% words POS possessive\",\"% words 'of'\", \"Contractions\",\"we_i_ratio\"),  funs(mean, std.error),) #pulling the means and SEs for our variables of interest\n# Get the mean values for the first year in the dataset\nyear_means &lt;- tidy_df_Inaug %&gt;%\n  filter(year == 1789) \n\n\ntidy_df_Inaug$i_centered &lt;- tidy_df_Inaug$i_mean - year_means$i_mean\ntidy_df_Inaug$WPS_centered &lt;- tidy_df_Inaug$WPS_mean - year_means$WPS_mean\ntidy_df_Inaug$we_centered &lt;-tidy_df_Inaug$we_mean - year_means$we_mean\ntidy_df_Inaug$pronoun_centered &lt;-tidy_df_Inaug$pronoun_mean - year_means$pronoun_mean\ntidy_df_Inaug$pos_centered &lt;-tidy_df_Inaug$`% words POS possessive_mean` - year_means$`% words POS possessive_mean`\ntidy_df_Inaug$of_centered &lt;-tidy_df_Inaug$`% words 'of'_mean`- year_means$`% words 'of'_mean`\ntidy_df_Inaug$con_centered &lt;-tidy_df_Inaug$Contractions_mean - year_means$Contractions_mean\ntidy_df_Inaug$det_centered &lt;-tidy_df_Inaug$det_mean - year_means$det_mean"
  },
  {
    "objectID": "coding/Regression/Regression.html#variable-description",
    "href": "coding/Regression/Regression.html#variable-description",
    "title": "Regression",
    "section": "Variable Description",
    "text": "Variable Description\nFlesch-Kincaid Ease of Readability: higher scores indicate material that is easier to read; lower numbers mark passages that are more difficult to read. Calculated using spaCy in python.\nThe Flesch‚ÄìKincaid Grade Level Score: presents a score as a U.S. grade level, making it easier for teachers, parents, librarians, and others to judge the readability level of various books and texts.Calculated using spaCy in python.\nI-usage: First-person singular pronoun usage (% of total words). Calculated using LIWC.\nWe-usage: First-person plural pronoun usage (% of total words). Calculated using LIWC.\nPronoun-usage: Overall pronoun usage (% of total words). Calculated using LIWC.\nPossessive-usage:First-person singular pronoun usage (% of total words). Calculated using NLTK POS, PRP, and PRP$ parser\nOf-usage: Usage of the word ‚Äòof‚Äô (% of total words). Calculated using NLTK parser.\nContraction-usage: Usage of 85 most common contractions in English (% of total words). Calculated using custom LIWC dictionary.\nDeterminers-usage: Determiner usage (% of total words). Calculated using LIWC."
  },
  {
    "objectID": "coding/Regression/Regression.html#years",
    "href": "coding/Regression/Regression.html#years",
    "title": "Regression",
    "section": "years",
    "text": "years\n\ninaug %&gt;% \n  select(year) %&gt;% \n  range()\n\n[1] 1789 2021"
  },
  {
    "objectID": "coding/Regression/Regression.html#raw-count-of-speeches",
    "href": "coding/Regression/Regression.html#raw-count-of-speeches",
    "title": "Regression",
    "section": "Raw count of Speeches",
    "text": "Raw count of Speeches\n\ninaug %&gt;%\n  select(Filename) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  reactable::reactable(striped = TRUE)"
  },
  {
    "objectID": "coding/Regression/Regression.html#speeches-per-year",
    "href": "coding/Regression/Regression.html#speeches-per-year",
    "title": "Regression",
    "section": "Speeches per year",
    "text": "Speeches per year\n\narticles_year &lt;- inaug %&gt;%\n  select(Filename,year) %&gt;%\n  unique() %&gt;%\n  group_by(year) %&gt;%\n  dplyr::summarize(n = n()) %&gt;%\n  reactable::reactable(striped = TRUE)\n articles_year"
  },
  {
    "objectID": "coding/Regression/Regression.html#inaugural-addresses",
    "href": "coding/Regression/Regression.html#inaugural-addresses",
    "title": "Regression",
    "section": "Inaugural Addresses",
    "text": "Inaugural Addresses\n\nI-usage\n\ni_centered &lt;- lm(i_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(i_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n11.9676\n3.718\n3.219\n0.0021\n\n\nOriginal Publication year\n-0.0077\n0.002\n-3.926\n0.0002\n\n\n\n\n\n\n\n\n\nWe-usage\n\nwe_centered &lt;- lm(we_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(we_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-49.4826\n4.3154\n-11.47\n0\n\n\nOriginal Publication year\n0.0278\n0.0023\n12.29\n0\n\n\n\n\n\n\n\n\n\nPronouns\n\npronoun_centered &lt;- lm(pronoun_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(pronoun_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-30.1075\n6.7470\n-4.462\n0e+00\n\n\nOriginal Publication year\n0.0155\n0.0035\n4.369\n1e-04\n\n\n\n\n\n\n\n\n\nDeterminers\n\ndet_centered &lt;- lm(det_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(det_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n13.8294\n4.4158\n3.132\n0.0027\n\n\nOriginal Publication year\n-0.0078\n0.0023\n-3.369\n0.0014\n\n\n\n\n\n\n\n\n\nContractions\n\ncon_centered &lt;- lm(con_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(con_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-0.6582\n0.2503\n-2.629\n0.0110\n\n\nOriginal Publication year\n0.0004\n0.0001\n2.689\n0.0094\n\n\n\n\n\n\n\n\n\nPossesives\n\npos_centered &lt;- lm(pos_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(pos_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n-3.2954\n0.4958\n-6.647\n0\n\n\nOriginal Publication year\n0.0018\n0.0003\n6.995\n0\n\n\n\n\n\n\n\n\n\nOf-usage\n\nof_centered &lt;- lm(of_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(of_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n20.7280\n3.5511\n5.837\n0\n\n\nOriginal Publication year\n-0.0109\n0.0019\n-5.849\n0\n\n\n\n\n\n\n\n\n\nWords per Sentence\n\nWPS_centered &lt;- lm(WPS_centered ~ year, data = tidy_df_Inaug)\n\ntable_model(WPS_centered) \n\n\n\n\nterm\nestimate\nSE\nt\np\n\n\n\n\n(Intercept)\n223.8545\n23.7224\n9.436\n0\n\n\nOriginal Publication year\n-0.1347\n0.0124\n-10.827\n0"
  },
  {
    "objectID": "coding/Regression/Regression.html#i",
    "href": "coding/Regression/Regression.html#i",
    "title": "Regression",
    "section": "I",
    "text": "I\n\ninaug_i &lt;- lm(i_centered ~ year, data = tidy_df_Inaug)\ninaug_i_pvalue &lt;- summary(inaug_i)$coefficients[2, 4]\n\ni_max &lt;- max(tidy_df_Inaug$i_mean)\n\ni + \n  geom_text(aes(x=1789, y=i_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_i)[2], digits=2), \" p = \", format(inaug_i_pvalue, digits=2))),\n            color=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#we",
    "href": "coding/Regression/Regression.html#we",
    "title": "Regression",
    "section": "We",
    "text": "We\n\ninaug_we &lt;- lm(we_centered ~ year, data = tidy_df_Inaug)\ninaug_we_pvalue &lt;- summary(inaug_we)$coefficients[2, 4]\n\nwe_max &lt;- max(tidy_df_Inaug$we_mean)\n\nwe + \n  geom_text(aes(x=1789, y=we_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_we)[2], digits=2), \" p = \", format(inaug_we_pvalue, digits=2))),\n            color=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#pronouns-1",
    "href": "coding/Regression/Regression.html#pronouns-1",
    "title": "Regression",
    "section": "Pronouns",
    "text": "Pronouns\n\ninaug_pronouns &lt;- lm(pronoun_centered ~ year, data = tidy_df_Inaug)\ninaug_pronouns_pvalue &lt;- summary(inaug_pronouns)$coefficients[2, 4]\n\npronouns_max &lt;- max(tidy_df_Inaug$pronoun_mean)\n\npronouns +\n  geom_text(aes(x=1789, y=pronouns_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_pronouns)[2], digits=2), \" p = \", format(inaug_pronouns_pvalue, digits=2))),\n            color=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#wps",
    "href": "coding/Regression/Regression.html#wps",
    "title": "Regression",
    "section": "WPS",
    "text": "WPS\n\ninaug_WPS &lt;- lm(WPS_centered ~ year, data = tidy_df_Inaug)\ninaug_WPS_pvalue &lt;- summary(inaug_WPS)$coefficients[2, 4]\n\nwps_max &lt;- max(tidy_df_Inaug$WPS_mean)\n\nWPS +\ngeom_text(aes(x=1789, y=wps_max, label=paste(\"Inaugural Speeches: \", format(coef(inaug_WPS)[2], digits=2), \" p = \", format(inaug_WPS_pvalue, digits=2))),\ncolor=\"green4\", size=3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#determiners-1",
    "href": "coding/Regression/Regression.html#determiners-1",
    "title": "Regression",
    "section": "Determiners",
    "text": "Determiners\n\ninaug_determiners &lt;- lm(det_centered ~ year, data = tidy_df_Inaug)\ninaug_determiners_pvalue &lt;- summary(inaug_determiners)$coefficients[2, 4]\n\ndet_max &lt;- max(tidy_df_Inaug$det_mean)\n\ndeterminers +\ngeom_text(aes(x = 1789, y = det_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_determiners)[2], digits = 2), \" p = \", format(inaug_determiners_pvalue, digits = 2))), color = \"green4\", size = 3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#possessives",
    "href": "coding/Regression/Regression.html#possessives",
    "title": "Regression",
    "section": "Possessives",
    "text": "Possessives\n\ninaug_possessives &lt;- lm(pos_centered ~ year, data = tidy_df_Inaug)\ninaug_possessives_pvalue &lt;- summary(inaug_possessives)$coefficients[2, 4]\n\npossessives_max &lt;- max(tidy_df_Inaug$`% words POS possessive_mean`)\n\n\npossessives +\ngeom_text(aes(x = 1789, y = possessives_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_possessives)[2], digits = 2), \" p = \", format(inaug_possessives_pvalue, digits = 2))), color = \"green4\", size = 3, hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#of-usage-1",
    "href": "coding/Regression/Regression.html#of-usage-1",
    "title": "Regression",
    "section": "Of-usage",
    "text": "Of-usage\n\ninaug_of &lt;- lm(of_centered ~ year, data = tidy_df_Inaug)\ninaug_of_pvalue &lt;- summary(inaug_of)$coefficients[2, 4]\n\nof_max &lt;- max(tidy_df_Inaug$`% words 'of'_mean`)\n\nof +\ngeom_text(aes(x = 1789, y = of_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_of)[2], digits = 2), \" p = \", format(inaug_of_pvalue, digits = 2))), color = \"green4\", size = 3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#contractions-1",
    "href": "coding/Regression/Regression.html#contractions-1",
    "title": "Regression",
    "section": "Contractions",
    "text": "Contractions\n\ninaug_contractions &lt;- lm(con_centered ~ year, data = tidy_df_Inaug)\ninaug_contractions_pvalue &lt;- summary(inaug_contractions)$coefficients[2, 4]\n\ncont_max &lt;- max(tidy_df_Inaug$Contractions_mean)\n\ncontractions  +\ngeom_text(aes(x = 1789, y = cont_max, label = paste(\"Inaugural Speeches: \", format(coef(inaug_contractions)[2], digits = 2), \" p = \", format(inaug_contractions_pvalue, digits = 2))), color = \"green4\", size = 3,hjust = 0)"
  },
  {
    "objectID": "coding/Regression/Regression.html#all-graphs",
    "href": "coding/Regression/Regression.html#all-graphs",
    "title": "Regression",
    "section": "All Graphs",
    "text": "All Graphs\n\ntidy_smooth_graphs &lt;- ggpubr::ggarrange(i,we,pronouns, WPS,determiners, possessives, of, contractions,\n                                        ncol=4, nrow=2, common.legend = TRUE, legend = \"top\")\nannotate_figure(tidy_smooth_graphs,\n                top = text_grob(\"LIWC Variables\",  color = \"black\", face = \"bold\", size = 20),\n                 bottom = text_grob(\"Horizontal line represents Fisher Corpus\",  color = \"black\", size = 14))"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#standard-demographics",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#standard-demographics",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Standard Demographics",
    "text": "Standard Demographics\nAssessed at initial timepoint\n\nAge & ResidenceGenderRaceSES\n\n\n\nAge: ___\nState of residence: ___"
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#primary-mental-health-measures",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#primary-mental-health-measures",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Primary Mental Health Measures",
    "text": "Primary Mental Health Measures\n\nGAD-7 (anxiety; measured at both time points)\nPHQ-8 (depression; measured at both time points)\n\nIf highly correlated, will collapse."
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#description",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#description",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Description",
    "text": "Description\n\nTwo-session online longitudinal study.\n\nBaseline: open-text responses about mood, motivation, sleep (‚âà25 min) + GAD-7 + PHQ-8.\nFollow-up (3 weeks later): repeat assessments (‚âà10 min) and collect information on posts from social media."
  },
  {
    "objectID": "posts/LBA-Race-Kickoff/Presentation.html#flow",
    "href": "posts/LBA-Race-Kickoff/Presentation.html#flow",
    "title": "Data-Driven Approaches to Building Equitable Language-Based Mental Health Assessments",
    "section": "Flow",
    "text": "Flow"
  }
]